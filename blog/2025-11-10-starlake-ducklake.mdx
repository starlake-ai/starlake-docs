---
title: "Starlake + DuckLake: Start Small, Scale Big"
date: 2025-11-10
description: "Explore how Starlake and DuckLake together form the foundation of the Post-Modern Data Stack, enabling quality-first ingestion, SQL-only transformations, local-first development, and open, scalable data management."
tags: ["Starlake", "DuckLake", "Post-Modern Data Stack", "Data Engineering", "Open Source"]
---

## Starlake + DuckLake: Start Small, Scale Big

The Post-Modern Data Stack is not about more tools, it's about less friction.
It's about building end-to-end data systems that are declarative, open, and composable, 
without the complexity and lock-in that defined the Modern Data era.

That's where Starlake and DuckLake fit naturally together.
Both embrace simplicity as a strength: Starlake unifies ingestion, transformation, and orchestration through declarative YAML, 
while DuckLake brings a lightweight, SQL-backed lake format with ACID transactions, schema evolution, and time travel, all on open Parquet files.

Together, they form the foundation of the Post-Modern Data Stack: one where you can start small, locally, 
and scale effortlessly to the cloud without changing your model or your mindset.

The era of the "Modern Data Stack" (MDS) brought cloud agility, but with it came fragmentation, hidden complexity, vendor lock-in 
and brittle pipelines. 
As we argued in our "Post Modern Data Stack" piece, it's time to pivot to something more declarative, unified and developer-friendly.  ￼


## Enter DuckLake: A Format Built for the Post-Modern Stack

DuckLake introduces a next-generation lake format built on Parquet, 
but with the coordination layer living in a real SQL database (PostgreSQL, MySQL, SQLite, or even DuckDB).
This small design choice changes everything.

* Multi-user collaboration, natively supported
Unlike local-only formats or single-process metadata layers, DuckLake’s SQL-based catalog allows multiple users, data engineers, analysts, 
applications to read and write concurrently with transactional guarantees.

* ACID transactions and snapshots
DuckLake delivers full transactional integrity across datasets, with snapshot isolation, time travel, and schema evolution,
the kind of reliability historically reserved for data warehouses.

* Open and composable
Because it’s based on open standards (SQL + Parquet), 
you can query your data with your favorite engine and plug it into any orchestration or visualization layer.
No proprietary runtime, no hidden metadata formats.

* Local-to-cloud consistency
You can develop locally with a lightweight DuckDB catalog, then deploy the same datasets on a shared PostgreSQL-backed DuckLake in the cloud, 
no rewrites, no migrations, no friction.

## Start Small, Scale Big

Let's start building our data pipeline with Starlake and DuckLake.

First, install Starlake by following the instructions in the [Starlake documentation](https://starlake.ai/docs/installation).

Bootstrap a project using a sample command line:

```bash
cd /my/project/folder
starlake bootstrap
```

In the metadata section of your `application.sl.yml`, configure DuckLake as your catalog:

```yaml title="application.sl.yml for local development"
version: 1
application:
  connectionRef: {{ACTIVE_CONNECTION}}
  connections:
    ducklake_local:
      type: jdbc
      options:
        url: "jdbc:duckdb:"
        driver: "org.duckdb.DuckDBDriver"
        preActions: >
            INSTALL ducklake;
            LOAD ducklake;
            ATTACH IF NOT EXISTS 'ducklake:/local/path/metadata.ducklake' As my_ducklake
                (DATA_PATH '/local/path/');
            USE my_ducklake;

```

where the environment variable `ACTIVE_CONNECTION` is set to `ducklake_local` for local development.

That's it you're ready to use DuckLake as your lake format and start developing locally!

When you're ready to scale to the cloud, simply change your DuckLake catalog connection to point to a PostgreSQL or MySQL instance, 
and update the DATA_PATH to point to your cloud storage bucket (e.g., GCS, S3).
For example, to connect to a DuckLake catalog in PostgreSQL with data stored in GCS:

```yaml {15-37} title="application.sl.yml for local and cloud deployment"
version: 1
application:
  connectionRef: "{{ACTIVE_CONNECTION}}"
  connections:
    ducklake_local:
      type: jdbc
      options:
        url: "jdbc:duckdb:"
        driver: "org.duckdb.DuckDBDriver"
        preActions: >
            INSTALL ducklake;
            LOAD ducklake;
            ATTACH IF NOT EXISTS 'ducklake:/local/path/metadata.ducklake' As my_ducklake
                (DATA_PATH '/local/path/');
            USE my_ducklake;
    ducklake_cloud:
      type: jdbc
      options:
        url: "jdbc:postgresql://your_postgres_host/ducklake_catalog"
        driver: "org.postgresql.Driver"
        preActions: >   
            INSTALL POSTGRES;
            INSTALL ducklake;
            LOAD POSTGRES;
            LOAD ducklake;
            CREATE OR REPLACE SECRET (
                type gcs, 
                key_id '{{DUCKLAKE_HMAC_ACCESS_KEY_ID}}', 
                secret '{{DUCKLAKE_HMAC_SECRET_ACCESS_KEY}}' 
                SCOPE 'gs://ducklake_bucket/data_files/');
            ATTACH IF NOT EXISTS 'ducklake:postgres:
                    dbname=ducklake_catalog 
                    host=your_postgres_host 
                    port=5432 
                    user=dbuser 
                    password={{DUCKLAKE_PASSWORD}}' AS my_ducklake
                (DATA_PATH 'gs://ducklake_bucket/data_files/');

```


where the environment variable `ACTIVE_CONNECTION` is set to `ducklake_postgres` for cloud deployment.


With this setup, you can seamlessly transition from local development to cloud deployment without changing your data models or transformation logic.


## Why Starlake + DuckLake == The Right Pairing

Here's how the two reinforce each other in the Post-Modern stack:

|Need in Post-Modern Stack| How Starlake addresses it | How DuckLake enables it
|-------------------------|---------------------------|-------------------------|
|Quality-first ingestion	|Starlake enables validation/quality checks at ingestion.  ￼	|DuckLake's metadata enables lineage, versioning and auditing, supporting trusted ingestion.
|SQL-only, portable transformations	|Starlake keeps transformation logic as plain SQL, no templating.  ￼	|With DuckLake you're working on Parquet + catalog via SQL, ensuring transformations remain portable and engine-agnostic.
|Local dev, global deployment	|Starlake supports DuckDB local development, then deploys with no changes.  ￼	|DuckLake supports DuckDB locally and scales to larger catalogs/storage; the same data format persists.
|Git-style data branching	|Starlake includes snapshot/branching semantics for datasets.  ￼	|DuckLake snapshots/time-travel provide the required mechanism to treat data versions like code branches.
|Orchestration-agnostic pipelines	|Starlake analyzes SQL lineage, generates DAGs for any orchestrator.  ￼	|DuckLake provides unified metadata so orchestration tools can reliably reference dataset versions, dependencies and snapshots.
|Semantic modelling agnostic	|Starlake can output semantic layer models for multiple BI platforms.  ￼	|DuckLake ensures the underlying dataset format remains open and portable so semantic models aren't locked to one tool.


## Conclusion
The combination of Starlake and DuckLake represents a decisive shift toward the Post-Modern Data Stack,
one where openness, simplicity, and scalability coexist. Instead of assembling a tangle of incompatible tools, 
data teams can now build pipelines that are declarative, SQL-driven, and environment-agnostic from day one.

With Starlake, you define your data flow once: ingestion, transformation, validation, orchestration, all in YAML and SQL. 
With DuckLake, you store and query your data in an open, transactional lake format that scales seamlessly from a local DuckDB setup 
to a cloud-backed PostgreSQL catalog. 
The result is a development experience that feels as simple as working on your laptop, 
yet scales to enterprise-grade reliability and performance.

In recent performance tests, DuckLake processed 600 million TPC-H records in under 1 second for most queries,
proof that you don’t need heavyweight infrastructure to achieve warehouse-class performance.

The future of data engineering is declarative, composable, and open.
With Starlake + DuckLake, you can truly start small and scale big, without ever compromising on speed, quality, or control.


