# Load

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

In this section you will learn how to load and transform data using the samples files created when [bootstrapping a new project](bootstrap) and the [schemas
inferred](infer-schema) from these files.

## Load files
Loading is a two step process: The optional `import` step and the `load` step.

### Import step
In this first step, Starlake will look at the _directory_ attribute value in the YAML files and look at the file that matches the expected patterns defined in the table definition files.
In our example, the directories are `{{incoming_path}}/sales` and `{{incoming_path}}/hr`.

the `import` command moves the files that satisfy one table pattern from the _incoming_ folder to the `datasets/pending` folder.
Files that do not satisfy any pattern won't be loaded and are moved to the `datasets/unresolved` directory.


<Tabs groupId="platforms">
<TabItem value="linux_macos" label="Linux/MacOS">

```sh
$ cd $HOME/userguide
$ starlake import
```

</TabItem>
<TabItem value="windows" label="Windows">

```powershell
c:\users\me\userguide> starlake import
```

</TabItem>
<TabItem value="docker" label="Docker">

```shell
$ docker run                                                  \
    -e SL_ROOT=/app/userguide                                \
    -v $HOME/userguide:/app/userguide -it starlake import
```

</TabItem>
</Tabs>

The sample data files has now been moved to the `userguide/datasets/pending/sales` directory.

:::note
This step is optional and does not need to be run if your files directly arrive in the `datasets/pending/sales` folder.

Also note that the pending directory  (`${SL_ROOT}/datasets/pending` by default) location may be [redefined](../configuration/filesystem).

:::

### Load step

In this second step, each line in the file present in the `datasets/pending` folder is checked against the schema described in the YAML file
and its result is stored in the warehouse.


<Tabs groupId="platforms">
<TabItem value="linux_macos" label="Linux/MacOS">

```sh
$ cd $HOME/userguide
$ starlake load
```

</TabItem>
<TabItem value="windows" label="Windows">

```powershell
c:\users\me\starlake> starlake load
```

</TabItem>
<TabItem value="docker" label="Docker">

```shell
$ docker run                                                      \
    -e SL_ROOT=/app/userguide                                    \
    -v $HOME/myproject:/app/userguide -it starlake load
```

</TabItem>
</Tabs>

This will load the data files and since we chose the sparkLocal connection, they will be stored them as parquet files into the following folders:
- `datasets/accepted` for valid records
- `datasets/rejected` for invalid records
- `datasets/unresolved` for unrecognized files


:::caution

Starlake validate the data against the table's schema of the first pattern that match with the file name.
Hence, you must be careful regarding the pattern you set.
Make sure that there is no overlap.

:::


### Check the result

You can check the result by running the following python script to read the parquet file from the project directory:

```python
import pandas as pd
filepath = 'datasets/accepted/sales/customers/'
df = pd.read_parquet(filepath)
df.head
```

## Incremental updates
Sometimes, we want to alter existing data by adding new records or updating existing ones. This is called an incremental update or upsert.

This is useful when we have a large amount of data and we want to avoid reloading the entire data set every time we want to update our data warehouse.

In this section, we will see how to do incremental updates.


We've seen that we may overwrite or append the data in a table when we load it, but what if for some existing records,
we want to update the data in a table ?

This is done using the `merge` attribute in the table configuration file. In this example, we will use the
`merge` attribute to upsert records in the `sales.customers`.

The `merge.timestamp` attribute is used to recognize the most recent record concerning a customer identified by the columns listed in `merge.key` . If not specified, new records (the records being loaded) are considered
to be the most recent ones. In our case, the `signup` date will used as the merge timestamp for the records identified by their column `id`.


This is the content of the `metadata/load/customers.sl.yml` file once the `merge` attribute is added (highlighted lines):

```yaml title="metadata/load/sales/customers.sl.yml" {3-5}
---
table:
  merge:
    timestamp: signup
    key: [id]
  pattern: "customers.*.psv" # This property is a regular expression that will be used to match the file name.
                             # Please replace it by the adequate file pattern eq. customers-.*.psv if required
  attributes:         # Description of the fields to recognize
    - name: "id"        # attribute name and column name in the destination table if no rename attribute is defined
      type: "string"    # expected type
      array: false      # is it an array (false by default, ignored in DSV files) ?
      required: false   # Is this field required in the source (false by default, change it accordingly) ?
      privacy: "NONE"   # Should we encrypt this field before loading to the warehouse (No encryption by default )?
      ignore: false     # Should this field be excluded (false by default) ?
    - name: "signup"    # second attribute
      type: "timestamp" # recognized type by analyzing input.
    - name: "contact"
      type: "string"
      # ...
    - name: "birthdate"
      type: "date"      # recognized as semantic type date.
      # ...
    - name: "firstname"
      type: "string"
      # ...
    - name: "lastname"
      type: "string"
      # ...
    - name: "country"
      type: "string"
      # ...               # and so on ...
  metadata:
    mode: "FILE"
    format: "DSV"         # detected format
    encoding: "UTF-8"
    multiline: false
    array: false
    withHeader: true
    separator: "|"        # detected separator
    quote: "\""
    escape: "\\"
    write: "APPEND"

```


## Targeting another datawarehouse
We just loaded our text file as a parquet file. This is a very common format for data scientists and analysts.
Through minimum extra configuration, we are able to run Starlake on top of any warehouse and have these datasets available as tables.

The examples below describe the extra configuration required in the `metadata/application.sl.yml` configuration file
to load the data into  bigquery, databricks, redshift, snowflake or postgresql.



<Tabs groupId="datawarehouse">
<TabItem value="bigquery" label="bigquery">

```yaml
application:
  connectionRef: "bigquery"
  loader: native
  connections:
    bigquery:
      type: "bigquery"
      options:
        location: "us-central1" # europe-west1 or EU or US or ..
        authType: "APPLICATION_DEFAULT"
        authScopes: "https://www.googleapis.com/auth/cloud-platform" # comma separated list of scopes
        #authType: SERVICE_ACCOUNT_JSON_KEYFILE
        #jsonKeyfile: "/Users/me/.gcloud/keys/starlake-me.json"
        #authType: "ACCESS_TOKEN"
        #gcpAccessToken: "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"
```  

</TabItem>
<TabItem value="databricks" label="databricks">

```yaml
application:
  connectionRef: "databricks"
  loader: spark # note the spark loader here
  connections:
    sparkLocal:
      type: "databricks"
```

</TabItem>
<TabItem value="postgresql" label="postgresql">

```yaml
application:
  connectionRef: "postgresql"
  loader: spark # note the spark loader here
  connections:
    postgresql:
      type: jdbc
      options:
        url: "jdbc:postgresql://{{POSTGRES_HOST}}:{{POSTGRES_PORT}}/{{POSTGRES_DATABASE}}"
        driver: "org.postgresql.Driver"
        user: "{{DATABASE_USER}}"
        password: "{{DATABASE_PASSWORD}}"
        quoteIdentifiers: false
```  

</TabItem>
<TabItem value="redshift" label="Redshift">

```yaml
application:
  connectionRef: "redshift"
  loader: spark # note the spark loader here
  connections:
    redshift:
      options:
        url: "jdbc:redshift://account.region.redshift.amazonaws.com:5439/database",
        driver: com.amazon.redshift.Driver
        password: "{{REDSHIFT_PASSWORD}}"
        tempdir: "s3a://bucketName/data",
        tempdir_region: "eu-central-1" # required only if running from outside AWS (your laptop ...)
        aws_iam_role: "arn:aws:iam::aws_count_id:role/role_name"
```

Make sure to set the following environment variables:

```shell

export AWS_ACCESS_KEY_ID= # your AWS access key
export AWS_SECRET_ACCESS_KEY= # your AWS secret key
export AWS_SESSION_TOKEN= # your AWS session token
export AWS_DEFAULT_REGION= # your AWS region

```

</TabItem>
<TabItem value="snowflake" label="snowflake">

```yaml
application:
  connectionRef: "snowflake"
  loader: native
  connections:
    snowflake:
      type: jdbc
      options:
        url: "jdbc:snowflake://{{SNOWFLAKE_ACCOUNT}}.snowflakecomputing.com/"
        driver: "net.snowflake.client.jdbc.SnowflakeDriver"
        user: "{{SNOWFLAKE_USER}}"
        password: "{{SNOWFLAKE_PASSWORD}}"
        warehouse: "{{SNOWFLAKE_WAREHOUSE}}"
        db: "{{SNOWFLAKE_DB}}"
        keep_column_case: "off"
        preActions: "alter session set TIMESTAMP_TYPE_MAPPING = 'TIMESTAMP_LTZ';ALTER SESSION SET QUOTED_IDENTIFIERS_IGNORE_CASE = true"
```

</TabItem>
<TabItem value="spark-bigquery" label="spark bigquery">

```yaml
application:
  connectionRef: "spark-bigquery"
  loader: spark # note the spark loader here
  connections:
    spark-bigquery:
      type: "bigquery"
      options:
        location: "EU" # EU or US or ..
        authType: "APPLICATION_DEFAULT"
        authScopes: "https://www.googleapis.com/auth/cloud-platform" # comma separated list of scopes
        writeMethod: "direct" # direct or indirect (indirect is required for certain features see https://github.com/GoogleCloudDataproc/spark-bigquery-connector)
        #gcsBucket: "starlake-app" # Temporary GCS Bucket where intermediary files will be stored. Required in indirect mode only
        #authType: SERVICE_ACCOUNT_JSON_KEYFILE
        #jsonKeyfile: "/Users/me/.gcloud/keys/starlake-me.json"
        #authType: "ACCESS_TOKEN"
        #gcpAccessToken: "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"
```

</TabItem>

<TabItem value="spark-redshift" label="Spark Redshift">

```yaml
application:
  connectionRef: "redshift"
  loader: spark # note the spark loader here
  connections:
    redshift:
      sparkFormat: "io.github.spark_redshift_community.spark.redshift" # if running on top of Spark or else  "redshift" if running on top of Databricks
      options:
        url: "jdbc:redshift://account.region.redshift.amazonaws.com:5439/database",
        driver: com.amazon.redshift.Driver
        password: "{{REDSHIFT_PASSWORD}}"
        tempdir: "s3a://bucketName/data",
        tempdir_region: "eu-central-1" # required only if running from outside AWS (your laptop ...)
        aws_iam_role: "arn:aws:iam::aws_count_id:role/role_name"
```

Make sure to set the following environment variables:

```shell

export AWS_ACCESS_KEY_ID= # your AWS access key
export AWS_SECRET_ACCESS_KEY= # your AWS secret key
export AWS_SESSION_TOKEN= # your AWS session token
export AWS_DEFAULT_REGION= # your AWS region

```

</TabItem>


<TabItem value="spark-snowflake" label="spark snowflake">

```yaml
application:
  connectionRef: "spark-snowflake"
  loader: spark # note the spark loader here
  connections:
      spark-snowflake:
        type: jdbc
        sparkFormat: snowflake
        options:
          sfUrl: "{{SNOWFLAKE_ACCOUNT}}.snowflakecomputing.com" # make sure you do not prefix by jdbc:snowflake://. This is done by the snowflaek driver
          #sfDriver: "net.snowflake.client.jdbc.SnowflakeDriver"
          sfUser: "{{SNOWFLAKE_USER}}"
          sfPassword: "{{SNOWFLAKE_PASSWORD}}"
          sfWarehouse: "{{SNOWFLAKE_WAREHOUSE}}"
          sfDatabase: "{{SNOWFLAKE_DB}}"
          keep_column_case: "off"
          autopushdown: on
          preActions: "alter session set TIMESTAMP_TYPE_MAPPING = 'TIMESTAMP_LTZ';ALTER SESSION SET QUOTED_IDENTIFIERS_IGNORE_CASE = true"

```

</TabItem>
</Tabs>

Using Spark instead of the BigQuery Load API may slow down the ingestion process but it has among others the following advantages:
- It allows to load data from any source supported by Spark including Fixed Width Files, XML files, JSON Arrays files ...
- It allows to load data into any destination supported by Spark including Snowflake, Amazon Redshift ...
- It allows to apply any transformation supported by Spark
- It allows to report any number of errors instead of 5 errors max with the BigQuery Load API (This is a BigQuery API Limitation)


To load the data into BigQuery, simply put back the samples data files now archived in the datasets folder back
to the sample-data/hr and sample-data/sales folders and run the _import_ and _load_ commands again.

:::note

Spark is not required to load data into any of the target datawarehouse, nor is it required to run transformations on the data.

Even when using Spark, you do not need to instantiate a cluster.  Spark becomes useful for advanced data validation at load time.

:::
