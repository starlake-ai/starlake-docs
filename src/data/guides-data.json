{
  "guides": [
    {
      "id": "starlake-snowflake-native-app-install",
      "title": "Install Starlake Native App from Snowflake Marketplace",
      "description": "Learn how to install Starlake as a Snowflake Native App from the Snowflake Marketplace.",
      "categories": [
        "Snowflake"
      ],
      "icon": "/img/icons/snowflake.svg",
      "tags": [
        "Snowflake"
      ],
      "level": "Beginner",
      "content": "## Starlake on Snowflake Native App \n\nIn this quickstart guide, you'll learn how to install Starlake as a Snowflake Native App from the Snowflake Marketplace. \n\n\n## Locate Starlake in Snowflake Marketplace\n\n__Prerequisites__\n\n- A Snowflake account\n- Basic knowledge of Snowflake services\n\nLet's get started!\n\n1. Log in to Snowflake Snowsight\n2. Navigate to the \"Data Products / Marketplace\" section\n3. Search for \"Starlake\" in the Marketplace using the search bar\n4. Click on the \"Starlake\" listing\n5. Click \"Install\" to add Starlake to your Snowflake account\n\n## Install Starlake\n\n1. After clicking \"Install\", you will be prompted to select a warehouse name and size for Starlake.\n2. Leave the default (starlake_compute_wh and X-SMALL) or choose a name for your warehouse (e.g., \"STL_WH\") and select the desired size (e.g., \"X-Small\").\n3. Click \"Continue\" to proceed with the installation.\n4. Within a few seconds, Starlake will be installed\n5. Once the installation is complete, you need to wait for the app URL to be generated (requires 2 to 3 minutes). Keep hitting the \"Refresh URL\" button on the page until the URL appears.\n\n\n## Access Starlake\n\n1. Once the app URL is generated, click on it to access Starlake.\n2. You may be prompted to log in with the root user credentials (admin@localhost.localdomain).\n3. After logging in, you can start using Starlake for your data integration and analytics needs.\n4. The first time you access Starlake, you will be able to set the default root password for the root user (admin@localhost.localdomain).\n5. Click on your email in top bottom-left bar and select \"My Profile\". \n6. In the \"Update Password\" tab enter the old password (\"admin\" by default) and enter your new password.\n\n\n## Enroll users\n\nUsers are automatically enrolled through the \"Sign Up\" link on the Starlake login screen.\n\nYou can however whitelist the users that are allowed to enroll.\n\nIn the bottom left bar, click on your email and select \"Admin\" then on the \"Platform whitelist\" tab enter one valid user email address per line. To allow all users from a specific domain, enter the domain name only (e.g., `example.com`)\n\nNote that only users with a valid Snowflake account can access the app.",
      "tabs": [
        {
          "id": 1,
          "label": "Starlake on Snowflake Native App ",
          "content": "In this quickstart guide, you'll learn how to install Starlake as a Snowflake Native App from the Snowflake Marketplace."
        },
        {
          "id": 2,
          "label": "Locate Starlake in Snowflake Marketplace",
          "content": "__Prerequisites__\n\n- A Snowflake account\n- Basic knowledge of Snowflake services\n\nLet's get started!\n\n1. Log in to Snowflake Snowsight\n2. Navigate to the \"Data Products / Marketplace\" section\n3. Search for \"Starlake\" in the Marketplace using the search bar\n4. Click on the \"Starlake\" listing\n5. Click \"Install\" to add Starlake to your Snowflake account"
        },
        {
          "id": 3,
          "label": "Install Starlake",
          "content": "1. After clicking \"Install\", you will be prompted to select a warehouse name and size for Starlake.\n2. Leave the default (starlake_compute_wh and X-SMALL) or choose a name for your warehouse (e.g., \"STL_WH\") and select the desired size (e.g., \"X-Small\").\n3. Click \"Continue\" to proceed with the installation.\n4. Within a few seconds, Starlake will be installed\n5. Once the installation is complete, you need to wait for the app URL to be generated (requires 2 to 3 minutes). Keep hitting the \"Refresh URL\" button on the page until the URL appears."
        },
        {
          "id": 4,
          "label": "Access Starlake",
          "content": "1. Once the app URL is generated, click on it to access Starlake.\n2. You may be prompted to log in with the root user credentials (admin@localhost.localdomain).\n3. After logging in, you can start using Starlake for your data integration and analytics needs.\n4. The first time you access Starlake, you will be able to set the default root password for the root user (admin@localhost.localdomain).\n5. Click on your email in top bottom-left bar and select \"My Profile\". \n6. In the \"Update Password\" tab enter the old password (\"admin\" by default) and enter your new password."
        },
        {
          "id": 5,
          "label": "Enroll users",
          "content": "Users are automatically enrolled through the \"Sign Up\" link on the Starlake login screen.\n\nYou can however whitelist the users that are allowed to enroll.\n\nIn the bottom left bar, click on your email and select \"Admin\" then on the \"Platform whitelist\" tab enter one valid user email address per line. To allow all users from a specific domain, enter the domain name only (e.g., `example.com`)\n\nNote that only users with a valid Snowflake account can access the app."
        }
      ]
    },
    {
      "id": "project-setup-on-snowflake",
      "title": "Project Setup on Snowflake",
      "description": "Learn how to connect Starlake with your Snowflake data sources.",
      "categories": [
        "Snowflake"
      ],
      "icon": "/img/icons/snowflake.svg",
      "tags": [
        "Snowflake"
      ],
      "level": "Beginner",
      "content": "## Create a New Project\n\n\n__Prerequisites__\n\n- A Snowflake account\n- Basic knowledge of Snowflake services\n- Starlake installed and running\n\n![Step 1](/img/guides/project-setup-on-snowflake/step1.png \"Step 1\")\n\n1. Login to Starlake. If you don't have an account, you'll need to create one.\n2. Click on the \"New Project\" button. You'll be prompted to enter a name and description for your project.\n3. Hit the \"Next\" button to configure your database connection.\n\n## Configure Snowflake Connection\n\n1. Under \"Snowflake Configuration\", enter your Snowflake account details:\n![Step 2]( /img/guides/project-setup-on-snowflake/step2.1.png \"Step 2\")\n   - Account Name in the form <org_name>.<account_name> (e.g., qbuqrxc-or28007)\n   - Username: <your_username> as listed in Snowsight in the \"Admin / Users & roles\" section\n   - Password: you may here use a programmatic access token for authentication see [here](https://docs.snowflake.com/en/user-guide/programmatic-access-tokens#generating-a-programmatic-access-token))\n   - Warehouse: COMPUTE_WH by default\n   - Database: <your_database>\n   - Default schema (PUBLIC by default)\n  Any extra options required by your organization may be added by clicking the \"Advanced Options\" button.\n2. Test the connection to ensure Starlake can access your Snowflake account.\n![Step 2]( /img/guides/project-setup-on-snowflake/step2.2.png \"Step 2\")\n3. Hit the \"Next\" button to select your orchestrator.\n\n\n## Select Orchestrator (Optional)\n\n![Step 3]( /img/guides/project-setup-on-snowflake/step3.png \"Step 3\")\n\n1. Enable orchestration.\n2. Choose your preferred orchestrator from the list provided (Snowflake Task by default).\n3. Hit the \"Next\" button to proceed to the final step. Configure your Git integration settings as needed.\n\n\n## Configure Git Integration (Optional)\n\nBy default, Starlake uses an internal Git repository where you can commit your changes but not pushe them to an external Git repository.\nNote: You can always download later your project files and Git history to a local folder from inside Starlake.\n\nTo configure Git integration:\n\n![Step 4]( /img/guides/project-setup-on-snowflake/step4.png \"Step 4\")\n\n1. Deselect \"Use internal Git repository\" if you want to use an external Git provider.\n2. Select your Git provider (e.g., GitHub, GitLab, Bitbucket).\n3. Enter your Git repository details:\n   - Repository URL: <your_repository_url>\n   - Authentication Method: \"Personal Access Token\" is currently supported.\n   - Personal Access Token: <your_personal_access_token> (see [here](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens) for more details).\n4. Test your connection to ensure Starlake can access your Git repository.\n5. Hit the \"Next\" button to configure AI integration.\n\n\n## Configure AI Integration (Optional)\n\n![Step 5]( /img/guides/project-setup-on-snowflake/step5.png \"Step 5\")\n\n1. Enable AI integration if desired.\n2. Choose your preferred AI provider from the list provided (e.g., OpenAI).\n3. Enter your AI provider details:\n   - API Key: <your_api_key> (see [here](https://platform.openai.com/api-keys))\n4. Hit the \"Next\" button to proceed to the final step. Review your project settings and click \"Finish\" to complete the setup.\n\n## Review and Finish\n\n![Step 6]( /img/guides/project-setup-on-snowflake/step6.png \"Step 6\")\n\n1. Review all your project settings.\n2. Click \"Finish\" to complete the setup.\n\n## Video Recap\n\n<div>\n<video width=\"100%\" height=\"100%\" controls>\n  <source src=\"/img/guides/project-setup-on-snowflake/project-setup-on-snowflake.mp4\" type=\"video/mp4\">\n  Your browser does not support the video tag.\n</video>\n</div>",
      "tabs": [
        {
          "id": 1,
          "label": "Create a New Project",
          "content": "__Prerequisites__\n\n- A Snowflake account\n- Basic knowledge of Snowflake services\n- Starlake installed and running\n\n![Step 1](/img/guides/project-setup-on-snowflake/step1.png \"Step 1\")\n\n1. Login to Starlake. If you don't have an account, you'll need to create one.\n2. Click on the \"New Project\" button. You'll be prompted to enter a name and description for your project.\n3. Hit the \"Next\" button to configure your database connection."
        },
        {
          "id": 2,
          "label": "Configure Snowflake Connection",
          "content": "1. Under \"Snowflake Configuration\", enter your Snowflake account details:\n![Step 2]( /img/guides/project-setup-on-snowflake/step2.1.png \"Step 2\")\n   - Account Identifier in the form <org_name>-<account_name> (e.g., qbuqrxc-or28007)\n   - Username: <your_username> as listed in Snowsight in the \"Admin / Users & roles\" section\n   - Password: you may here use a programmatic access token for authentication see [here](https://docs.snowflake.com/en/user-guide/programmatic-access-tokens#generating-a-programmatic-access-token))\n   - Warehouse: COMPUTE_WH by default\n   - Database: <your_database>\n   - Default schema (PUBLIC by default)\n  Any extra options required by your organization may be added by clicking the \"Advanced Options\" button.\n2. Test the connection to ensure Starlake can access your Snowflake account.\n![Step 2]( /img/guides/project-setup-on-snowflake/step2.2.png \"Step 2\")\n3. Hit the \"Next\" button to select your orchestrator."
        },
        {
          "id": 3,
          "label": "Select Orchestrator (Optional)",
          "content": "![Step 3]( /img/guides/project-setup-on-snowflake/step3.png \"Step 3\")\n\n1. Enable orchestration.\n2. Choose your preferred orchestrator from the list provided (Snowflake Task by default).\n3. Hit the \"Next\" button to proceed to the final step. Configure your Git integration settings as needed."
        },
        {
          "id": 4,
          "label": "Configure Git Integration (Optional)",
          "content": "By default, Starlake uses an internal Git repository where you can commit your changes but not pushe them to an external Git repository.\nNote: You can always download later your project files and Git history to a local folder from inside Starlake.\n\nTo configure Git integration:\n\n![Step 4]( /img/guides/project-setup-on-snowflake/step4.png \"Step 4\")\n\n1. Deselect \"Use internal Git repository\" if you want to use an external Git provider.\n2. Select your Git provider (e.g., GitHub, GitLab, Bitbucket).\n3. Enter your Git repository details:\n   - Repository URL: <your_repository_url>\n   - Authentication Method: \"Personal Access Token\" is currently supported.\n   - Personal Access Token: <your_personal_access_token> (see [here](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens) for more details).\n4. Test your connection to ensure Starlake can access your Git repository.\n5. Hit the \"Next\" button to configure AI integration."
        },
        {
          "id": 5,
          "label": "Configure AI Integration (Optional)",
          "content": "![Step 5]( /img/guides/project-setup-on-snowflake/step5.png \"Step 5\")\n\n1. Enable AI integration if desired.\n2. Choose your preferred AI provider from the list provided (e.g., OpenAI).\n3. Enter your AI provider details:\n   - API Key: <your_api_key> (see [here](https://platform.openai.com/api-keys))\n4. Hit the \"Next\" button to proceed to the final step. Review your project settings and click \"Finish\" to complete the setup."
        },
        {
          "id": 6,
          "label": "Review and Finish",
          "content": "![Step 6]( /img/guides/project-setup-on-snowflake/step6.png \"Step 6\")\n\n1. Review all your project settings.\n2. Click \"Finish\" to complete the setup."
        },
        {
          "id": 7,
          "label": "Video Recap",
          "content": "<div>\n<video width=\"100%\" height=\"100%\" controls>\n  <source src=\"/img/guides/project-setup-on-snowflake/project-setup-on-snowflake.mp4\" type=\"video/mp4\">\n  Your browser does not support the video tag.\n</video>\n</div>"
        }
      ]
    },
    {
      "id": "starlake-load-csv-files",
      "title": "Load CSV Files into your Database",
      "description": "Learn how to load CSV files into your database.",
      "categories": [
        "Starlake",
        "Load"
      ],
      "icon": "/img/icons/starlake.png",
      "tags": [
        "Starlake",
        "Load"
      ],
      "level": "Beginner",
      "content": "## Create a domain (Optional)\n\n\nA domain is a logical grouping of your data assets in Starlake. It maps to a schema in your database also known as a dataset in BigQuery. If you don't have a domain yet, you can create one or use an existing one if you already created it in Starlake.\n\n1. In your Starlake project, navigate to the \"Load\" section.\n2. Click on \"Add Domain\" and enter a domain name. If this domain already exists in the database, it will be mapped to the existing schema. We'll name our domain, \"starbake\". \n3. You may optionally add a description your domain. This is useful for documentation purposes and for AI assistance. This description will be saved in the database as a comment.\n4. Click \"Create\" to finalize the domain creation.\n\n![](/img/guides/load-csv-files/step1.png)\n\nWe are now ready to load our CSV files into the schema \"starbake\" of our database.\n\n\n## Infer your table name from the CSV file\n\nPrepare a sample CSV file with a few rows of data that represent the structure you want to import and do not forget to include headers. Make sure the use use the correct data types for each column and the same delimiter as your CSV file.\n\n1. Click on \"Load\" and select \"CSV / JSON / XML\".\n2. Choose the CSV file you want to load.\n![](/img/guides/load-csv-files/step2.png)\nA preview of the file will be displayed.\nStarlake will also infer the table name from the filename. Update the table name if necessary.\nIf A.I. is enabled, you may request it to suggest a description for the table by clicking on the \"A.I.\" button inside the \"Description\" field.\n1. If you do not want to load the preview data, you can disable the \"Load Data\" option.\n2. Hit the \"Next\" button.\n\n## Infer table schema from the CSV file\n\nStarlake will automatically infer the table schema from the CSV file. You can review and modify the inferred schema if necessary.\n\n![](/img/guides/load-csv-files/step3.1.png)\n\nTo customize the inferred schema, you can edit the data types, and invoke the A.I. assistant to suggest descriptions as needed. This allows you to ensure that the schema matches your requirements before loading the data.\nThis schema will be created when executing your load if it does not already exist.\n\nYou can also define the primary key for the table by selecting one or more columns in the \"Primary Key\" section. This is important for optimizing query performance and improving A.I. suggestions when building transformations.\n\nHit the \"Finish\" button to complete the process.\n\nIf you kept the \"Load Data\" option enabled, Starlake will start loading the data from the CSV file into the specified table and display the results.\n\n![](/img/guides/load-csv-files/step3.2.png)\n\n\n## Going Further\n\nOnce you inferred the table schema and loaded the data, you can start building transformations on top of it. Starlake provides a powerful SQL editor and a visual transformation builder to help you create complex data pipelines with ease.\n\nYou can also:\n\n- define write strategies for your data loads, such as \"append\" or \"overwrite\", \"slow changing dimensions\" ..., to control how new data is integrated into your tables.\n- schedule regular data loads and define freshness criteria for your data to ensure it is up-to-date and relevant.\n- create expectations for your data to ensure quality and consistency.\n- Add new fields through SQL transformations during the load process.\n- Encrypt sensitive data to protect it at rest during the load process.\n- Add access controls to your data to ensure only authorized users can view or modify it.",
      "tabs": [
        {
          "id": 1,
          "label": "Create a domain (Optional)",
          "content": "A domain is a logical grouping of your data assets in Starlake. It maps to a schema in your database also known as a dataset in BigQuery. If you don't have a domain yet, you can create one or use an existing one if you already created it in Starlake.\n\n1. In your Starlake project, navigate to the \"Load\" section.\n2. Click on \"Add Domain\" and enter a domain name. If this domain already exists in the database, it will be mapped to the existing schema. We'll name our domain, \"starbake\". \n3. You may optionally add a description your domain. This is useful for documentation purposes and for AI assistance. This description will be saved in the database as a comment.\n4. Click \"Create\" to finalize the domain creation.\n\n![](/img/guides/load-csv-files/step1.png)\n\nWe are now ready to load our CSV files into the schema \"starbake\" of our database."
        },
        {
          "id": 2,
          "label": "Infer your table name from the CSV file",
          "content": "Prepare a sample CSV file with a few rows of data that represent the structure you want to import and do not forget to include headers. Make sure the use use the correct data types for each column and the same delimiter as your CSV file.\n\n1. Click on \"Load\" and select \"CSV / JSON / XML\".\n2. Choose the CSV file you want to load.\n![](/img/guides/load-csv-files/step2.png)\nA preview of the file will be displayed.\nStarlake will also infer the table name from the filename. Update the table name if necessary.\nIf A.I. is enabled, you may request it to suggest a description for the table by clicking on the \"A.I.\" button inside the \"Description\" field.\n1. If you do not want to load the preview data, you can disable the \"Load Data\" option.\n2. Hit the \"Next\" button."
        },
        {
          "id": 3,
          "label": "Infer table schema from the CSV file",
          "content": "Starlake will automatically infer the table schema from the CSV file. You can review and modify the inferred schema if necessary.\n\n![](/img/guides/load-csv-files/step3.1.png)\n\nTo customize the inferred schema, you can edit the data types, and invoke the A.I. assistant to suggest descriptions as needed. This allows you to ensure that the schema matches your requirements before loading the data.\nThis schema will be created when executing your load if it does not already exist.\n\nYou can also define the primary key for the table by selecting one or more columns in the \"Primary Key\" section. This is important for optimizing query performance and improving A.I. suggestions when building transformations.\n\nHit the \"Finish\" button to complete the process.\n\nIf you kept the \"Load Data\" option enabled, Starlake will start loading the data from the CSV file into the specified table and display the results.\n\n![](/img/guides/load-csv-files/step3.2.png)"
        },
        {
          "id": 4,
          "label": "Going Further",
          "content": "Once you inferred the table schema and loaded the data, you can start building transformations on top of it. Starlake provides a powerful SQL editor and a visual transformation builder to help you create complex data pipelines with ease.\n\nYou can also:\n\n- define write strategies for your data loads, such as \"append\" or \"overwrite\", \"slow changing dimensions\" ..., to control how new data is integrated into your tables.\n- schedule regular data loads and define freshness criteria for your data to ensure it is up-to-date and relevant.\n- create expectations for your data to ensure quality and consistency.\n- Add new fields through SQL transformations during the load process.\n- Encrypt sensitive data to protect it at rest during the load process.\n- Add access controls to your data to ensure only authorized users can view or modify it."
        }
      ]
    },
    {
      "id": "starlake-load-json-files",
      "title": "Load JSON Files into your Database",
      "description": "Learn how to load JSON files into your database.",
      "categories": [
        "Starlake",
        "Load"
      ],
      "icon": "/img/icons/starlake.png",
      "tags": [
        "Starlake",
        "Load"
      ],
      "level": "Beginner",
      "content": "## Create a domain (Optional)\n\n\nA domain is a logical grouping of your data assets in Starlake. It maps to a schema in your database also known as a dataset in BigQuery. If you don't have a domain yet, you can create one or use an existing one if you already created it in Starlake.\n\nThis schema will be created when executing your load if it does not already exist.\n\n1. In your Starlake project, navigate to the \"Load\" section.\n2. Click on \"Add Domain\" and enter a domain name. If this domain already exists in the database, it will be mapped to the existing schema. We'll name our domain, \"starbake\". \n3. You may optionally add a description your domain. This is useful for documentation purposes and for AI assistance. This description will be saved in the database as a comment.\n4. Click \"Create\" to finalize the domain creation.\n\n![](/img/guides/load-json-files/step1.png)\n\nWe are now ready to load our JSON files into the schema \"starbake\" of our database.\n\n\n## Infer your table name from the JSON file\n\nPrepare a sample JSON file with a few rows of data that represent the structure you want to import. This may be an array of objects or newline-delimited JSON file (JSONL format).\n\n1. Click on \"Load\" and select \"CSV / JSON / XML\".\n2. Choose the JSON file you want to load.\n![](/img/guides/load-json-files/step2.png)\nA preview of the file will be displayed.\nStarlake will also infer the table name from the filename. Update the table name if necessary.\nIf A.I. is enabled, you may request it to suggest a description for the table by clicking on the \"A.I.\" button inside the \"Description\" field.\n\nWhen the JSON file contains nested and/or repeated structures, Starlake will automatically flatten them into separate columns. You can review and modify the inferred schema if necessary.\nThis is however only supported on BigQuery and Databricks.\n\nOn Snowflake, only single level JSON files are flattened to multiple columns. If you do not want to flatten the JSON file, you can choose to load it as a single column by enabling the \"Load as Single Column\" option.\n\n1. If you do not want to load the preview data, you can disable the \"Load Data\" option.\n2. Hit the \"Next\" button.\n\n## Infer table schema from the JSON file\n\nStarlake will automatically infer the table schema from the JSON file. You can review and modify the inferred schema if necessary.\n\n![](/img/guides/load-json-files/step3.1.png)\n\nTo customize the inferred schema, you can edit the data types, and invoke the A.I. assistant to suggest descriptions as needed. This allows you to ensure that the schema matches your requirements before loading the data.\n\nYou can also define the primary key for the table by selecting one or more columns in the \"Primary Key\" section. This is important for optimizing query performance and improving A.I. suggestions when building transformations.\n\nHit the \"Finish\" button to complete the process.\n\nIf you kept the \"Load Data\" option enabled, Starlake will start loading the data from the JSON file into the specified table and display the results.\n\n![](/img/guides/load-json-files/step3.2.png)\n\n\n## Going Further\n\nOnce you inferred the table schema and loaded the data, you can start building transformations on top of it. Starlake provides a powerful SQL editor and a visual transformation builder to help you create complex data pipelines with ease.\n\nYou can also:\n\n- define write strategies for your data loads, such as \"append\" or \"overwrite\", \"slow changing dimensions\" ..., to control how new data is integrated into your tables.\n- schedule regular data loads and define freshness criteria for your data to ensure it is up-to-date and relevant.\n- create expectations for your data to ensure quality and consistency.\n- Add new fields through SQL transformations during the load process.\n- Encrypt sensitive data to protect it at rest during the load process.\n- Add access controls to your data to ensure only authorized users can view or modify it.",
      "tabs": [
        {
          "id": 1,
          "label": "Create a domain (Optional)",
          "content": "A domain is a logical grouping of your data assets in Starlake. It maps to a schema in your database also known as a dataset in BigQuery. If you don't have a domain yet, you can create one or use an existing one if you already created it in Starlake.\n\nThis schema will be created when executing your load if it does not already exist.\n\n1. In your Starlake project, navigate to the \"Load\" section.\n2. Click on \"Add Domain\" and enter a domain name. If this domain already exists in the database, it will be mapped to the existing schema. We'll name our domain, \"starbake\". \n3. You may optionally add a description your domain. This is useful for documentation purposes and for AI assistance. This description will be saved in the database as a comment.\n4. Click \"Create\" to finalize the domain creation.\n\n![](/img/guides/load-json-files/step1.png)\n\nWe are now ready to load our JSON files into the schema \"starbake\" of our database."
        },
        {
          "id": 2,
          "label": "Infer your table name from the JSON file",
          "content": "Prepare a sample JSON file with a few rows of data that represent the structure you want to import. This may be an array of objects or newline-delimited JSON file (JSONL format).\n\n1. Click on \"Load\" and select \"CSV / JSON / XML\".\n2. Choose the JSON file you want to load.\n![](/img/guides/load-json-files/step2.png)\nA preview of the file will be displayed.\nStarlake will also infer the table name from the filename. Update the table name if necessary.\nIf A.I. is enabled, you may request it to suggest a description for the table by clicking on the \"A.I.\" button inside the \"Description\" field.\n\nWhen the JSON file contains nested and/or repeated structures, Starlake will automatically flatten them into separate columns. You can review and modify the inferred schema if necessary.\nThis is however only supported on BigQuery and Databricks.\n\nOn Snowflake, only single level JSON files are flattened to multiple columns. If you do not want to flatten the JSON file, you can choose to load it as a single column by enabling the \"Load as Single Column\" option.\n\n1. If you do not want to load the preview data, you can disable the \"Load Data\" option.\n2. Hit the \"Next\" button."
        },
        {
          "id": 3,
          "label": "Infer table schema from the JSON file",
          "content": "Starlake will automatically infer the table schema from the JSON file. You can review and modify the inferred schema if necessary.\n\n![](/img/guides/load-json-files/step3.1.png)\n\nTo customize the inferred schema, you can edit the data types, and invoke the A.I. assistant to suggest descriptions as needed. This allows you to ensure that the schema matches your requirements before loading the data.\n\nYou can also define the primary key for the table by selecting one or more columns in the \"Primary Key\" section. This is important for optimizing query performance and improving A.I. suggestions when building transformations.\n\nHit the \"Finish\" button to complete the process.\n\nIf you kept the \"Load Data\" option enabled, Starlake will start loading the data from the JSON file into the specified table and display the results.\n\n![](/img/guides/load-json-files/step3.2.png)"
        },
        {
          "id": 4,
          "label": "Going Further",
          "content": "Once you inferred the table schema and loaded the data, you can start building transformations on top of it. Starlake provides a powerful SQL editor and a visual transformation builder to help you create complex data pipelines with ease.\n\nYou can also:\n\n- define write strategies for your data loads, such as \"append\" or \"overwrite\", \"slow changing dimensions\" ..., to control how new data is integrated into your tables.\n- schedule regular data loads and define freshness criteria for your data to ensure it is up-to-date and relevant.\n- create expectations for your data to ensure quality and consistency.\n- Add new fields through SQL transformations during the load process.\n- Encrypt sensitive data to protect it at rest during the load process.\n- Add access controls to your data to ensure only authorized users can view or modify it."
        }
      ]
    },
    {
      "id": "starlake-transform-data",
      "title": "Transform data to insights",
      "description": "Learn how to transform data into insights.",
      "categories": [
        "Starlake",
        "Transform"
      ],
      "icon": "/img/icons/starlake.png",
      "tags": [
        "Starlake",
        "Transform"
      ],
      "level": "Beginner",
      "content": "## Create a domain (Optional)\n\n\nA domain is a logical grouping of your data assets in Starlake. It maps to a schema in your database also known as a dataset in BigQuery. If you don't have a domain yet, you can create one or use an existing one if you already created it in Starlake.\n\nThis schema will be created when executing your transform if it does not already exist.\n\n1. In your Starlake project, navigate to the \"Transform\" section.\n2. Click on \"Add Domain\" and enter a domain name. If this domain already exists in the database, it will be mapped to the existing schema. We'll name our domain, \"starbake_analytics\". \n3. You may optionally add a description your domain. This is useful for documentation purposes and for AI assistance. This description will be saved in the database as a comment.\n4. Click \"Create\" to finalize the domain creation.\n\n![](/img/guides/transform-data/step1.png)\n\nWe are now ready to write our first insight in the schema \"starbake_analytics\" of our database.\n\n## Create your first insight\n\n1. Select the domain \"starbake_analytics\".\n2. Click the \"Add Insight\" button.\n3. Enter a name for your insight.\n4. Click \"Create\" to finalize the insight creation.\n5. In the \"Code editor\", write your SQL query to transform the data and generate insights.\n6. Click \"Run\" to execute your query and view the results.\n7. Review the results and make any necessary adjustments to your query.\n\n![](/img/guides/transform-data/step2.png)\n\n\n## Materialize your insights\n\nOnce you are satisfied with your SQL query and the results it produces, you can materialize your insights by creating a new table in your database.\n\nIn the \"Write Strategy\" section, select one of the following options:\n- APPEND: Data will be appended to the target table\n- OVERWRITE: Data will replace the existing data in the target table if it exists\n- SCD2 (Slow Changing Dimension) : Data will be merged into the target table based on a key column and timestamps columns to keep historical data\n- ...\n\n![](/img/guides/transform-data/step3.1.png)\n\n1. In the \"Code Editor\", click the \"Run\" drop-down and \"Materialize\" option.\n2. Starlake will execute your query and store the results in the table named after your transform.\n\n## View your query lineage\n\nThe bottom panel will also show you the query lineage as it will be handled by the orchestrator\n\n![](/img/guides/transform-data/step3.2.png)",
      "tabs": [
        {
          "id": 1,
          "label": "Create a domain (Optional)",
          "content": "A domain is a logical grouping of your data assets in Starlake. It maps to a schema in your database also known as a dataset in BigQuery. If you don't have a domain yet, you can create one or use an existing one if you already created it in Starlake.\n\nThis schema will be created when executing your transform if it does not already exist.\n\n1. In your Starlake project, navigate to the \"Transform\" section.\n2. Click on \"Add Domain\" and enter a domain name. If this domain already exists in the database, it will be mapped to the existing schema. We'll name our domain, \"starbake_analytics\". \n3. You may optionally add a description your domain. This is useful for documentation purposes and for AI assistance. This description will be saved in the database as a comment.\n4. Click \"Create\" to finalize the domain creation.\n\n![](/img/guides/transform-data/step1.png)\n\nWe are now ready to write our first insight in the schema \"starbake_analytics\" of our database."
        },
        {
          "id": 2,
          "label": "Create your first insight",
          "content": "1. Select the domain \"starbake_analytics\".\n2. Click the \"Add Insight\" button.\n3. Enter a name for your insight.\n4. Click \"Create\" to finalize the insight creation.\n5. In the \"Code editor\", write your SQL query to transform the data and generate insights.\n6. Click \"Run\" to execute your query and view the results.\n7. Review the results and make any necessary adjustments to your query.\n\n![](/img/guides/transform-data/step2.png)"
        },
        {
          "id": 3,
          "label": "Materialize your insights",
          "content": "Once you are satisfied with your SQL query and the results it produces, you can materialize your insights by creating a new table in your database.\n\nIn the \"Write Strategy\" section, select one of the following options:\n- APPEND: Data will be appended to the target table\n- OVERWRITE: Data will replace the existing data in the target table if it exists\n- SCD2 (Slow Changing Dimension) : Data will be merged into the target table based on a key column and timestamps columns to keep historical data\n- ...\n\n![](/img/guides/transform-data/step3.1.png)\n\n1. In the \"Code Editor\", click the \"Run\" drop-down and \"Materialize\" option.\n2. Starlake will execute your query and store the results in the table named after your transform."
        },
        {
          "id": 4,
          "label": "View your query lineage",
          "content": "The bottom panel will also show you the query lineage as it will be handled by the orchestrator\n\n![](/img/guides/transform-data/step3.2.png)"
        }
      ]
    },
    {
      "id": "starlake-orchestrate-snowflake-tasks",
      "title": "Orchestrate in Snowflake Tasks",
      "description": "Learn how to orchestrate tasks in Snowflake.",
      "categories": [
        "Starlake",
        "Orchestrate",
        "Snowflake"
      ],
      "icon": "/img/icons/starlake.png",
      "tags": [
        "Starlake",
        "Orchestrate",
        "Snowflake"
      ],
      "level": "Beginner",
      "content": "## Set schedules\n\nYou can set schedules for your orchestrations using cron expressions. This allows you to automate the execution of your data pipelines at specified intervals.\n\nSimply head to the Orchestrate/Schedules menu and add a new schedule with your desired cron expression for each table or task you want to run periodically.\n\n![](/img/guides/orchestrate-in-snowflake-tasks/step1.png)\n\n\nNote that you do not need to set a schedule for tasks that are dependencies of other tasks, since they will be triggered automatically when their parent tasks are executed.\n\n\n## Configure External Stage (Optional)\n\nIf you are loading Data from Cloud Storage (like S3, GCS, or ADLS), you can configure Snowflake Tasks to orchestrate the loading process. This is particularly useful for automating data ingestion workflows. Snowflake Tasks need to know the external stage where your data is stored. \n\nTo create an external stage in Snowflake, you can use the following SQL command:\n\n```sql\nCREATE OR REPLACE STAGE my_external_stage\nURL='s3://my-bucket/path/to/data/'\nSTORAGE_INTEGRATION = my_s3_integration;\n```\n\nExternal stage configuration in Snowflake documentation: \n- for Google Cloud Storage: https://docs.snowflake.com/en/user-guide/data-load-gcs-config\n- for Amazon S3: https://docs.snowflake.com/en/user-guide/data-load-s3-config\n- for Azure Container: https://docs.snowflake.com/en/user-guide/data-load-azure-config\n\nOnce you have created the external stage, you can configure Snowflake Tasks in Starlake by specifying the stage name in the Orchestrate/Config/snowflake-load-sql Tasks menu. This will allow Starlake to create and manage Snowflake Tasks that load data from the specified external stage into your Snowflake tables.\n\n![](/img/guides/orchestrate-in-snowflake-tasks/step3.png)\n\n\n## Dry Run Snowflake Tasks\n\nFirst let's generate our Snowflake Tasks code. Navigate to Orchestrate/Dags and click \"generate\" to create the Snowpark code that will handle your loads and transformations.\n\nBefore deploying your Snowflake Tasks, it's a good practice to perform a dry run. This allows you to validate the configuration and ensure that everything is set up correctly without actually executing the tasks. \n\nTo perform a dry run, click on the \"Dry Run\" button. This will simulate the execution of your Snowflake Tasks and provide feedback on any potential issues or errors in the configuration.\n\nIf the dry run is successful and you are satisfied with the configuration, you can proceed to deploy the Snowflake Tasks by clicking on the \"Deploy\" button. This will create the tasks in your Snowflake environment, ready to be executed according to the schedules you have set.\n\n![](/img/guides/orchestrate-in-snowflake-tasks/step4.png)\n\n\n## Deploy Snowflake Tasks\n\nOnce you have successfully completed the dry run and are satisfied with the configuration, you can proceed to deploy the Snowflake Tasks.\n\nTo deploy the tasks, simply click on the \"Deploy\" button. This action will create the tasks in your Snowflake environment, making them ready for execution based on the schedules you have defined.\n\nYou can check on the status of your deployed tasks in the Snowflake UI. Navigate to the \"Tasks\" section to view the list of tasks, their current status, and any execution history.\n\nThe tasks will have been created in the schema you specified at project creation time.\n\n\n![](/img/guides/orchestrate-in-snowflake-tasks/step5.png)",
      "tabs": [
        {
          "id": 1,
          "label": "Set schedules",
          "content": "You can set schedules for your orchestrations using cron expressions. This allows you to automate the execution of your data pipelines at specified intervals.\n\nSimply head to the Orchestrate/Schedules menu and add a new schedule with your desired cron expression for each table or task you want to run periodically.\n\n![](/img/guides/orchestrate-in-snowflake-tasks/step1.png)\n\n\nNote that you do not need to set a schedule for tasks that are dependencies of other tasks, since they will be triggered automatically when their parent tasks are executed."
        },
        {
          "id": 2,
          "label": "Configure External Stage (Optional)",
          "content": "If you are loading Data from Cloud Storage (like S3, GCS, or ADLS), you can configure Snowflake Tasks to orchestrate the loading process. This is particularly useful for automating data ingestion workflows. Snowflake Tasks need to know the external stage where your data is stored. \n\nTo create an external stage in Snowflake, you can use the following SQL command:\n\n```sql\nCREATE OR REPLACE STAGE my_external_stage\nURL='s3://my-bucket/path/to/data/'\nSTORAGE_INTEGRATION = my_s3_integration;\n```\n\nExternal stage configuration in Snowflake documentation: \n- for Google Cloud Storage: https://docs.snowflake.com/en/user-guide/data-load-gcs-config\n- for Amazon S3: https://docs.snowflake.com/en/user-guide/data-load-s3-config\n- for Azure Container: https://docs.snowflake.com/en/user-guide/data-load-azure-config\n\nOnce you have created the external stage, you can configure Snowflake Tasks in Starlake by specifying the stage name in the Orchestrate/Config/snowflake-load-sql Tasks menu. This will allow Starlake to create and manage Snowflake Tasks that load data from the specified external stage into your Snowflake tables.\n\n![](/img/guides/orchestrate-in-snowflake-tasks/step3.png)"
        },
        {
          "id": 3,
          "label": "Dry Run Snowflake Tasks",
          "content": "First let's generate our Snowflake Tasks code. Navigate to Orchestrate/Dags and click \"generate\" to create the Snowpark code that will handle your loads and transformations.\n\nBefore deploying your Snowflake Tasks, it's a good practice to perform a dry run. This allows you to validate the configuration and ensure that everything is set up correctly without actually executing the tasks. \n\nTo perform a dry run, click on the \"Dry Run\" button. This will simulate the execution of your Snowflake Tasks and provide feedback on any potential issues or errors in the configuration.\n\nIf the dry run is successful and you are satisfied with the configuration, you can proceed to deploy the Snowflake Tasks by clicking on the \"Deploy\" button. This will create the tasks in your Snowflake environment, ready to be executed according to the schedules you have set.\n\n![](/img/guides/orchestrate-in-snowflake-tasks/step4.png)"
        },
        {
          "id": 4,
          "label": "Deploy Snowflake Tasks",
          "content": "Once you have successfully completed the dry run and are satisfied with the configuration, you can proceed to deploy the Snowflake Tasks.\n\nTo deploy the tasks, simply click on the \"Deploy\" button. This action will create the tasks in your Snowflake environment, making them ready for execution based on the schedules you have defined.\n\nYou can check on the status of your deployed tasks in the Snowflake UI. Navigate to the \"Tasks\" section to view the list of tasks, their current status, and any execution history.\n\nThe tasks will have been created in the schema you specified at project creation time.\n\n\n![](/img/guides/orchestrate-in-snowflake-tasks/step5.png)"
        }
      ]
    }
  ],
  "categories": [
    "Load",
    "Orchestrate",
    "Snowflake",
    "Starlake",
    "Transform"
  ]
}