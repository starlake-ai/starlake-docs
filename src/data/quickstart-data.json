{
  "quickstarts": [
    {
      "id": "aws",
      "title": "Quickstart for Starlake and AWS",
      "description": "Get started with Starlake and AWS data services in minutes",
      "categories": [
        "AWS"
      ],
      "icon": "/img/icons/aws.svg",
      "tags": [
        "AWS"
      ],
      "level": "Beginner",
      "content": "<div style={{maxWidth: '900px'}}>\n\n## Introduction\n\nIn this quickstart guide, you'll learn how to use Starlake with AWS data services. It will show you how to: \n\n- Set up AWS credentials\n- Configure Starlake for AWS\n- Create your first data pipeline\n- Deploy and monitor your pipelines\n\n### Prerequisites\n\n- An AWS account\n- Basic knowledge of AWS services\n- Starlake CLI installed\n\nLet's get started!\n\n## Set up AWS credentials\n\nBefore you can use Starlake with AWS, you need to configure your AWS credentials.\n\n### Step 1: Create an IAM User\n\n1. Log in to your AWS Management Console\n2. Navigate to IAM service\n3. Click \"Users\" and then \"Add user\"\n4. Enter a username (e.g., \"starlake-user\")\n5. Select \"Programmatic access\"\n\n### Step 2: Attach Permissions\n\nAttach the following policies to your user:\n- AmazonS3FullAccess\n- AmazonRedshiftFullAccess\n- AmazonGlueFullAccess\n\n### Step 3: Get Access Keys\n\n1. After creating the user, click \"Create access key\"\n2. Save the Access Key ID and Secret Access Key\n3. Configure these in your Starlake environment\n\n## Configure Starlake\n\nNow let's configure Starlake to work with your AWS environment.\n\n### Environment Variables\n\nSet the following environment variables:\n\n```bash\nexport AWS_ACCESS_KEY_ID=your_access_key\nexport AWS_SECRET_ACCESS_KEY=your_secret_key\nexport AWS_DEFAULT_REGION=us-east-1\n```\n\n### Starlake Configuration\n\nCreate a `starlake.conf` file:\n\n```yaml\nconnections:\n  aws:\n    type: \"aws\"\n    region: \"us-east-1\"\n    bucket: \"your-data-bucket\"\n```\n\n## Create your first pipeline\n\nLet's create a simple data pipeline using Starlake and AWS.\n\n### Step 1: Define the Extract\n\nCreate `extract/aws-sales.yml`:\n\n```yaml\nextract:\n  connectionRef: \"aws\"\n  tables:\n    - name: \"sales_data\"\n      schema: \"public\"\n      incremental: true\n      timestamp: \"created_at\"\n      partitionColumn: \"sale_date\"\n      fetchSize: 1000\n      where: \"sale_date >= '2024-01-01'\"\n\n# Optional: Add transformations during extract\ntransform:\n  - name: \"clean_sales_data\"\n    sql: |\n      SELECT \n        order_id,\n        customer_id,\n        product_id,\n        amount,\n        sale_date,\n        created_at\n      FROM sales_data\n      WHERE amount > 0\n```\n\n### Step 2: Define the Load\n\nCreate `load/sales.yml`:\n\n```yaml\ntable:\n  pattern: \"sales_data.*.parquet\"\n  metadata:\n    mode: \"FILE\"\n    format: \"PARQUET\"\n    encoding: \"UTF-8\"\n    withHeader: true\n  writeStrategy:\n    type: \"UPSERT_BY_KEY_AND_TIMESTAMP\"\n    timestamp: \"created_at\"\n    key: [\"order_id\"]\n  attributes:\n    - name: \"order_id\"\n      type: \"string\"\n      required: true\n      privacy: \"NONE\"\n    - name: \"customer_id\"\n      type: \"string\"\n      required: true\n      privacy: \"HIDE\"\n    - name: \"product_id\"\n      type: \"string\"\n      required: true\n    - name: \"amount\"\n      type: \"float\"\n      required: true\n    - name: \"sale_date\"\n      type: \"timestamp\"\n      required: true\n    - name: \"created_at\"\n      type: \"timestamp\"\n      required: true\n```\n\n### Step 3: Create Sample Data\n\nCreate a sample CSV file `data/sales_sample.csv`:\n\n```csv\norder_id,customer_id,product_id,amount,sale_date,created_at\nORD001,CUST001,PROD001,150.50,2024-01-15,2024-01-15 10:30:00\nORD002,CUST002,PROD002,299.99,2024-01-16,2024-01-16 14:20:00\nORD003,CUST001,PROD003,75.25,2024-01-17,2024-01-17 09:15:00\n```\n\n## Deploy and monitor\n\nNow let's deploy your pipeline and monitor its execution.\n\n### Deploy the Pipeline\n\nRun the following commands to execute your pipeline:\n\n```bash\n# Extract data from source\nstarlake extract --config extract/aws-sales.yml\n\n# Load data into AWS\nstarlake load --config load/sales.yml\n\n# Or run both in sequence\nstarlake run --extract-config extract/aws-sales.yml --load-config load/sales.yml\n```\n\n### Monitor Execution\n\n#### 1. Check Starlake Logs\n\n```bash\n# View recent logs\ntail -f logs/starlake.log\n\n# Check execution status\nstarlake status --job-id <job_id>\n```\n\n#### 2. Monitor AWS Services\n\n- Check S3 bucket for uploaded files\n- Monitor CloudWatch logs\n- Review AWS Glue job status\n\n#### 3. Verify Data\n\n```bash\n# Check loaded data\naws s3 ls s3://your-data-bucket/sales_data/\n\n# Sample data\naws s3 cp s3://your-data-bucket/sales_data/part-00000.parquet - | head -10\n```\n\n### Next Steps\n\n- Set up scheduling with cron or Airflow\n- Configure alerts and notifications\n- Scale your pipelines as needed\n- Implement data quality checks\n\n## Next steps\n\nCongratulations! You've successfully set up Starlake with AWS.\n\n### What's Next?\n\n#### Advanced Configuration\n\n- **Data Quality**: Implement data validation rules\n- **Incremental Loading**: Set up efficient incremental updates\n- **Partitioning**: Optimize performance with table partitioning\n- **Clustering**: Improve query performance with clustering keys\n\n#### Scheduling and Orchestration\n\n```bash\n# Set up cron job for daily execution\ncrontab -e\n\n# Add this line for daily execution at 2 AM\n0 2 * * * /path/to/starlake run --extract-config extract/aws-sales.yml --load-config load/sales.yml\n```\n\n#### Monitoring and Alerting\n\n- Set up AWS CloudWatch alerts\n- Configure Starlake notifications\n- Monitor data quality metrics\n- Track pipeline performance\n\n#### Security Enhancements\n\n- Implement IAM roles and policies\n- Set up S3 bucket policies\n- Use AWS KMS for encryption\n- Configure VPC and security groups\n\n### Resources\n\n- [Starlake Documentation](https://docs.starlake.ai)\n- [AWS Documentation](https://docs.aws.amazon.com)\n- [AWS Best Practices](https://aws.amazon.com/architecture/best-practices/)\n- [Community Support](https://github.com/starlake-ai/starlake)\n\n### Need Help?\n\nIf you encounter any issues:\n1. Check the troubleshooting guide\n2. Review the logs for error messages\n3. Verify AWS permissions\n4. Reach out to the community\n\n### Example Advanced Configuration\n\n```yaml\n# Advanced AWS configuration\nconnections:\n  aws:\n    type: \"aws\"\n    region: \"us-east-1\"\n    bucket: \"your-data-bucket\"\n    options:\n      encryption: \"AES256\"\n      lifecycle_policy: true\n      versioning: true\n```\n\n</div>",
      "tabs": [
        {
          "id": 1,
          "label": "Introduction",
          "content": "In this quickstart guide, you'll learn how to use Starlake with AWS data services. It will show you how to: \n\n- Set up AWS credentials\n- Configure Starlake for AWS\n- Create your first data pipeline\n- Deploy and monitor your pipelines\n\n### Prerequisites\n\n- An AWS account\n- Basic knowledge of AWS services\n- Starlake CLI installed\n\nLet's get started!"
        },
        {
          "id": 2,
          "label": "Set up AWS credentials",
          "content": "Before you can use Starlake with AWS, you need to configure your AWS credentials.\n\n### Step 1: Create an IAM User\n\n1. Log in to your AWS Management Console\n2. Navigate to IAM service\n3. Click \"Users\" and then \"Add user\"\n4. Enter a username (e.g., \"starlake-user\")\n5. Select \"Programmatic access\"\n\n### Step 2: Attach Permissions\n\nAttach the following policies to your user:\n- AmazonS3FullAccess\n- AmazonRedshiftFullAccess\n- AmazonGlueFullAccess\n\n### Step 3: Get Access Keys\n\n1. After creating the user, click \"Create access key\"\n2. Save the Access Key ID and Secret Access Key\n3. Configure these in your Starlake environment"
        },
        {
          "id": 3,
          "label": "Configure Starlake",
          "content": "Now let's configure Starlake to work with your AWS environment.\n\n### Environment Variables\n\nSet the following environment variables:\n\n```bash\nexport AWS_ACCESS_KEY_ID=your_access_key\nexport AWS_SECRET_ACCESS_KEY=your_secret_key\nexport AWS_DEFAULT_REGION=us-east-1\n```\n\n### Starlake Configuration\n\nCreate a `starlake.conf` file:\n\n```yaml\nconnections:\n  aws:\n    type: \"aws\"\n    region: \"us-east-1\"\n    bucket: \"your-data-bucket\"\n```"
        },
        {
          "id": 4,
          "label": "Create your first pipeline",
          "content": "Let's create a simple data pipeline using Starlake and AWS.\n\n### Step 1: Define the Extract\n\nCreate `extract/aws-sales.yml`:\n\n```yaml\nextract:\n  connectionRef: \"aws\"\n  tables:\n    - name: \"sales_data\"\n      schema: \"public\"\n      incremental: true\n      timestamp: \"created_at\"\n      partitionColumn: \"sale_date\"\n      fetchSize: 1000\n      where: \"sale_date >= '2024-01-01'\"\n\n# Optional: Add transformations during extract\ntransform:\n  - name: \"clean_sales_data\"\n    sql: |\n      SELECT \n        order_id,\n        customer_id,\n        product_id,\n        amount,\n        sale_date,\n        created_at\n      FROM sales_data\n      WHERE amount > 0\n```\n\n### Step 2: Define the Load\n\nCreate `load/sales.yml`:\n\n```yaml\ntable:\n  pattern: \"sales_data.*.parquet\"\n  metadata:\n    mode: \"FILE\"\n    format: \"PARQUET\"\n    encoding: \"UTF-8\"\n    withHeader: true\n  writeStrategy:\n    type: \"UPSERT_BY_KEY_AND_TIMESTAMP\"\n    timestamp: \"created_at\"\n    key: [\"order_id\"]\n  attributes:\n    - name: \"order_id\"\n      type: \"string\"\n      required: true\n      privacy: \"NONE\"\n    - name: \"customer_id\"\n      type: \"string\"\n      required: true\n      privacy: \"HIDE\"\n    - name: \"product_id\"\n      type: \"string\"\n      required: true\n    - name: \"amount\"\n      type: \"float\"\n      required: true\n    - name: \"sale_date\"\n      type: \"timestamp\"\n      required: true\n    - name: \"created_at\"\n      type: \"timestamp\"\n      required: true\n```\n\n### Step 3: Create Sample Data\n\nCreate a sample CSV file `data/sales_sample.csv`:\n\n```csv\norder_id,customer_id,product_id,amount,sale_date,created_at\nORD001,CUST001,PROD001,150.50,2024-01-15,2024-01-15 10:30:00\nORD002,CUST002,PROD002,299.99,2024-01-16,2024-01-16 14:20:00\nORD003,CUST001,PROD003,75.25,2024-01-17,2024-01-17 09:15:00\n```"
        },
        {
          "id": 5,
          "label": "Deploy and monitor",
          "content": "Now let's deploy your pipeline and monitor its execution.\n\n### Deploy the Pipeline\n\nRun the following commands to execute your pipeline:\n\n```bash\n# Extract data from source\nstarlake extract --config extract/aws-sales.yml\n\n# Load data into AWS\nstarlake load --config load/sales.yml\n\n# Or run both in sequence\nstarlake run --extract-config extract/aws-sales.yml --load-config load/sales.yml\n```\n\n### Monitor Execution\n\n#### 1. Check Starlake Logs\n\n```bash\n# View recent logs\ntail -f logs/starlake.log\n\n# Check execution status\nstarlake status --job-id <job_id>\n```\n\n#### 2. Monitor AWS Services\n\n- Check S3 bucket for uploaded files\n- Monitor CloudWatch logs\n- Review AWS Glue job status\n\n#### 3. Verify Data\n\n```bash\n# Check loaded data\naws s3 ls s3://your-data-bucket/sales_data/\n\n# Sample data\naws s3 cp s3://your-data-bucket/sales_data/part-00000.parquet - | head -10\n```\n\n### Next Steps\n\n- Set up scheduling with cron or Airflow\n- Configure alerts and notifications\n- Scale your pipelines as needed\n- Implement data quality checks"
        },
        {
          "id": 6,
          "label": "Next steps",
          "content": "Congratulations! You've successfully set up Starlake with AWS.\n\n### What's Next?\n\n#### Advanced Configuration\n\n- **Data Quality**: Implement data validation rules\n- **Incremental Loading**: Set up efficient incremental updates\n- **Partitioning**: Optimize performance with table partitioning\n- **Clustering**: Improve query performance with clustering keys\n\n#### Scheduling and Orchestration\n\n```bash\n# Set up cron job for daily execution\ncrontab -e\n\n# Add this line for daily execution at 2 AM\n0 2 * * * /path/to/starlake run --extract-config extract/aws-sales.yml --load-config load/sales.yml\n```\n\n#### Monitoring and Alerting\n\n- Set up AWS CloudWatch alerts\n- Configure Starlake notifications\n- Monitor data quality metrics\n- Track pipeline performance\n\n#### Security Enhancements\n\n- Implement IAM roles and policies\n- Set up S3 bucket policies\n- Use AWS KMS for encryption\n- Configure VPC and security groups\n\n### Resources\n\n- [Starlake Documentation](https://docs.starlake.ai)\n- [AWS Documentation](https://docs.aws.amazon.com)\n- [AWS Best Practices](https://aws.amazon.com/architecture/best-practices/)\n- [Community Support](https://github.com/starlake-ai/starlake)\n\n### Need Help?\n\nIf you encounter any issues:\n1. Check the troubleshooting guide\n2. Review the logs for error messages\n3. Verify AWS permissions\n4. Reach out to the community\n\n### Example Advanced Configuration\n\n```yaml\n# Advanced AWS configuration\nconnections:\n  aws:\n    type: \"aws\"\n    region: \"us-east-1\"\n    bucket: \"your-data-bucket\"\n    options:\n      encryption: \"AES256\"\n      lifecycle_policy: true\n      versioning: true\n```\n\n</div>"
        }
      ]
    },
    {
      "id": "azure",
      "title": "Quickstart for Starlake and Azure",
      "description": "Get started with Starlake and Azure data services in minutes",
      "categories": [
        "Azure"
      ],
      "icon": "/img/icons/azure.svg",
      "tags": [
        "Azure"
      ],
      "level": "Beginner",
      "content": "<div style={{maxWidth: '900px'}}>\n\n## Introduction\n\nIn this quickstart guide, you'll learn how to use Starlake with Azure data services. It will show you how to: \n\n- Set up Azure credentials and permissions\n- Configure Starlake for Azure\n- Create your first data pipeline\n- Deploy and monitor your pipelines\n\n### Prerequisites\n\n- An Azure account (trial or paid)\n- Basic knowledge of Azure services\n- Starlake CLI installed\n- Access to create storage accounts and databases\n\nLet's get started!\n\n## Set up Azure credentials\n\nBefore you can use Starlake with Azure, you need to configure your Azure credentials and permissions.\n\n### Step 1: Create an Azure Service Principal\n\n1. Log in to your Azure portal\n2. Navigate to **Azure Active Directory** > **App registrations**\n3. Click **\"New registration\"**\n4. Enter a name (e.g., `starlake-app`)\n5. Select **\"Accounts in this organizational directory only\"**\n6. Click **\"Register\"**\n\n### Step 2: Get Application Credentials\n\n1. Note the **Application (client) ID** and **Directory (tenant) ID**\n2. Go to **Certificates & secrets**\n3. Click **\"New client secret\"**\n4. Add a description and select expiration\n5. Copy the **Value** (this is your client secret)\n\n### Step 3: Assign Permissions\n\n1. Go to **API permissions**\n2. Click **\"Add a permission\"**\n3. Select **\"Azure Storage\"** and **\"Delegated permissions\"**\n4. Select **\"user_impersonation\"**\n5. Click **\"Grant admin consent\"**\n\n## Configure Starlake\n\nNow let's configure Starlake to work with your Azure environment.\n\n### Environment Variables\n\nSet the following environment variables in your shell:\n\n```bash\nexport AZURE_TENANT_ID=your_tenant_id\nexport AZURE_CLIENT_ID=your_client_id\nexport AZURE_CLIENT_SECRET=your_client_secret\nexport AZURE_SUBSCRIPTION_ID=your_subscription_id\nexport AZURE_STORAGE_ACCOUNT=your_storage_account\nexport AZURE_STORAGE_KEY=your_storage_key\n```\n\n### Starlake Configuration\n\nCreate a `starlake.conf` file in your project root:\n\n```yaml\nconnections:\n  azure:\n    type: \"azure\"\n    tenant_id: \"${AZURE_TENANT_ID}\"\n    client_id: \"${AZURE_CLIENT_ID}\"\n    client_secret: \"${AZURE_CLIENT_SECRET}\"\n    subscription_id: \"${AZURE_SUBSCRIPTION_ID}\"\n    storage_account: \"${AZURE_STORAGE_ACCOUNT}\"\n    storage_key: \"${AZURE_STORAGE_KEY}\"\n    options:\n      container: \"starlake-data\"\n      endpoint: \"https://${AZURE_STORAGE_ACCOUNT}.blob.core.windows.net\"\n\n# Global settings\nsettings:\n  default-connection: \"azure\"\n  default-format: \"PARQUET\"\n  default-mode: \"FILE\"\n```\n\n### Test Connection\n\nTest your connection with:\n\n```bash\nstarlake test-connection --connection azure\n```\n\n## Create your first pipeline\n\nLet's create a simple data pipeline using Starlake and Azure.\n\n### Step 1: Define the Extract\n\nCreate `extract/azure-sales.yml`:\n\n```yaml\nextract:\n  connectionRef: \"azure\"\n  tables:\n    - name: \"sales_data\"\n      schema: \"public\"\n      incremental: true\n      timestamp: \"created_at\"\n      partitionColumn: \"sale_date\"\n      fetchSize: 1000\n      where: \"sale_date >= '2024-01-01'\"\n\n# Optional: Add transformations during extract\ntransform:\n  - name: \"clean_sales_data\"\n    sql: |\n      SELECT \n        order_id,\n        customer_id,\n        product_id,\n        amount,\n        sale_date,\n        created_at\n      FROM sales_data\n      WHERE amount > 0\n```\n\n### Step 2: Define the Load\n\nCreate `load/sales.yml`:\n\n```yaml\ntable:\n  pattern: \"sales_data.*.parquet\"\n  metadata:\n    mode: \"FILE\"\n    format: \"PARQUET\"\n    encoding: \"UTF-8\"\n    withHeader: true\n  writeStrategy:\n    type: \"UPSERT_BY_KEY_AND_TIMESTAMP\"\n    timestamp: \"created_at\"\n    key: [\"order_id\"]\n  attributes:\n    - name: \"order_id\"\n      type: \"string\"\n      required: true\n      privacy: \"NONE\"\n    - name: \"customer_id\"\n      type: \"string\"\n      required: true\n      privacy: \"HIDE\"\n    - name: \"product_id\"\n      type: \"string\"\n      required: true\n    - name: \"amount\"\n      type: \"float\"\n      required: true\n    - name: \"sale_date\"\n      type: \"timestamp\"\n      required: true\n    - name: \"created_at\"\n      type: \"timestamp\"\n      required: true\n```\n\n### Step 3: Create Sample Data\n\nCreate a sample CSV file `data/sales_sample.csv`:\n\n```csv\norder_id,customer_id,product_id,amount,sale_date,created_at\nORD001,CUST001,PROD001,150.50,2024-01-15,2024-01-15 10:30:00\nORD002,CUST002,PROD002,299.99,2024-01-16,2024-01-16 14:20:00\nORD003,CUST001,PROD003,75.25,2024-01-17,2024-01-17 09:15:00\n```\n\n## Deploy and monitor\n\nNow let's deploy your pipeline and monitor its execution.\n\n### Deploy the Pipeline\n\nRun the following commands to execute your pipeline:\n\n```bash\n# Extract data from source\nstarlake extract --config extract/azure-sales.yml\n\n# Load data into Azure\nstarlake load --config load/sales.yml\n\n# Or run both in sequence\nstarlake run --extract-config extract/azure-sales.yml --load-config load/sales.yml\n```\n\n### Monitor Execution\n\n#### 1. Check Starlake Logs\n\n```bash\n# View recent logs\ntail -f logs/starlake.log\n\n# Check execution status\nstarlake status --job-id <job_id>\n```\n\n#### 2. Monitor Azure Services\n\n- Check Azure Storage for uploaded files\n- Monitor Azure Monitor logs\n- Review Azure Data Factory job status\n\n#### 3. Verify Data in Azure\n\n```bash\n# Check loaded data\naz storage blob list --container-name starlake-data --account-name your_storage_account\n\n# Sample data\naz storage blob download --container-name starlake-data --name sales_data/part-00000.parquet --file - --account-name your_storage_account\n```\n\n### Next Steps\n\n- Set up scheduling with Azure Data Factory\n- Configure alerts and notifications\n- Scale your pipelines as needed\n- Implement data quality checks\n\n## Next steps\n\nCongratulations! You've successfully set up Starlake with Azure.\n\n### What's Next?\n\n#### Advanced Configuration\n\n- **Data Quality**: Implement data validation rules\n- **Incremental Loading**: Set up efficient incremental updates\n- **Partitioning**: Optimize performance with table partitioning\n- **Clustering**: Improve query performance with clustering keys\n\n#### Scheduling and Orchestration\n\n```bash\n# Set up Azure Data Factory pipeline\naz datafactory pipeline create --resource-group your-rg --factory-name your-factory --name starlake-pipeline --pipeline @pipeline.json\n```\n\n#### Monitoring and Alerting\n\n- Set up Azure Monitor alerts\n- Configure Starlake notifications\n- Monitor data quality metrics\n- Track pipeline performance\n\n#### Security Enhancements\n\n- Implement Azure AD authentication\n- Set up storage account policies\n- Use Azure Key Vault for secrets\n- Configure network security groups\n\n### Resources\n\n- [Starlake Documentation](https://docs.starlake.ai)\n- [Azure Documentation](https://docs.microsoft.com/azure)\n- [Azure Best Practices](https://docs.microsoft.com/azure/architecture/best-practices/)\n- [Community Support](https://github.com/starlake-ai/starlake)\n\n### Need Help?\n\nIf you encounter any issues:\n1. Check the troubleshooting guide\n2. Review the logs for error messages\n3. Verify Azure permissions\n4. Reach out to the community\n\n### Example Advanced Configuration\n\n```yaml\n# Advanced Azure configuration\nconnections:\n  azure:\n    type: \"azure\"\n    tenant_id: \"${AZURE_TENANT_ID}\"\n    client_id: \"${AZURE_CLIENT_ID}\"\n    client_secret: \"${AZURE_CLIENT_SECRET}\"\n    subscription_id: \"${AZURE_SUBSCRIPTION_ID}\"\n    storage_account: \"${AZURE_STORAGE_ACCOUNT}\"\n    storage_key: \"${AZURE_STORAGE_KEY}\"\n    options:\n      container: \"starlake-data\"\n      endpoint: \"https://${AZURE_STORAGE_ACCOUNT}.blob.core.windows.net\"\n      encryption: \"AES256\"\n      lifecycle_policy: true\n      versioning: true\n```\n\n</div>",
      "tabs": [
        {
          "id": 1,
          "label": "Introduction",
          "content": "In this quickstart guide, you'll learn how to use Starlake with Azure data services. It will show you how to: \n\n- Set up Azure credentials and permissions\n- Configure Starlake for Azure\n- Create your first data pipeline\n- Deploy and monitor your pipelines\n\n### Prerequisites\n\n- An Azure account (trial or paid)\n- Basic knowledge of Azure services\n- Starlake CLI installed\n- Access to create storage accounts and databases\n\nLet's get started!"
        },
        {
          "id": 2,
          "label": "Set up Azure credentials",
          "content": "Before you can use Starlake with Azure, you need to configure your Azure credentials and permissions.\n\n### Step 1: Create an Azure Service Principal\n\n1. Log in to your Azure portal\n2. Navigate to **Azure Active Directory** > **App registrations**\n3. Click **\"New registration\"**\n4. Enter a name (e.g., `starlake-app`)\n5. Select **\"Accounts in this organizational directory only\"**\n6. Click **\"Register\"**\n\n### Step 2: Get Application Credentials\n\n1. Note the **Application (client) ID** and **Directory (tenant) ID**\n2. Go to **Certificates & secrets**\n3. Click **\"New client secret\"**\n4. Add a description and select expiration\n5. Copy the **Value** (this is your client secret)\n\n### Step 3: Assign Permissions\n\n1. Go to **API permissions**\n2. Click **\"Add a permission\"**\n3. Select **\"Azure Storage\"** and **\"Delegated permissions\"**\n4. Select **\"user_impersonation\"**\n5. Click **\"Grant admin consent\"**"
        },
        {
          "id": 3,
          "label": "Configure Starlake",
          "content": "Now let's configure Starlake to work with your Azure environment.\n\n### Environment Variables\n\nSet the following environment variables in your shell:\n\n```bash\nexport AZURE_TENANT_ID=your_tenant_id\nexport AZURE_CLIENT_ID=your_client_id\nexport AZURE_CLIENT_SECRET=your_client_secret\nexport AZURE_SUBSCRIPTION_ID=your_subscription_id\nexport AZURE_STORAGE_ACCOUNT=your_storage_account\nexport AZURE_STORAGE_KEY=your_storage_key\n```\n\n### Starlake Configuration\n\nCreate a `starlake.conf` file in your project root:\n\n```yaml\nconnections:\n  azure:\n    type: \"azure\"\n    tenant_id: \"${AZURE_TENANT_ID}\"\n    client_id: \"${AZURE_CLIENT_ID}\"\n    client_secret: \"${AZURE_CLIENT_SECRET}\"\n    subscription_id: \"${AZURE_SUBSCRIPTION_ID}\"\n    storage_account: \"${AZURE_STORAGE_ACCOUNT}\"\n    storage_key: \"${AZURE_STORAGE_KEY}\"\n    options:\n      container: \"starlake-data\"\n      endpoint: \"https://${AZURE_STORAGE_ACCOUNT}.blob.core.windows.net\"\n\n# Global settings\nsettings:\n  default-connection: \"azure\"\n  default-format: \"PARQUET\"\n  default-mode: \"FILE\"\n```\n\n### Test Connection\n\nTest your connection with:\n\n```bash\nstarlake test-connection --connection azure\n```"
        },
        {
          "id": 4,
          "label": "Create your first pipeline",
          "content": "Let's create a simple data pipeline using Starlake and Azure.\n\n### Step 1: Define the Extract\n\nCreate `extract/azure-sales.yml`:\n\n```yaml\nextract:\n  connectionRef: \"azure\"\n  tables:\n    - name: \"sales_data\"\n      schema: \"public\"\n      incremental: true\n      timestamp: \"created_at\"\n      partitionColumn: \"sale_date\"\n      fetchSize: 1000\n      where: \"sale_date >= '2024-01-01'\"\n\n# Optional: Add transformations during extract\ntransform:\n  - name: \"clean_sales_data\"\n    sql: |\n      SELECT \n        order_id,\n        customer_id,\n        product_id,\n        amount,\n        sale_date,\n        created_at\n      FROM sales_data\n      WHERE amount > 0\n```\n\n### Step 2: Define the Load\n\nCreate `load/sales.yml`:\n\n```yaml\ntable:\n  pattern: \"sales_data.*.parquet\"\n  metadata:\n    mode: \"FILE\"\n    format: \"PARQUET\"\n    encoding: \"UTF-8\"\n    withHeader: true\n  writeStrategy:\n    type: \"UPSERT_BY_KEY_AND_TIMESTAMP\"\n    timestamp: \"created_at\"\n    key: [\"order_id\"]\n  attributes:\n    - name: \"order_id\"\n      type: \"string\"\n      required: true\n      privacy: \"NONE\"\n    - name: \"customer_id\"\n      type: \"string\"\n      required: true\n      privacy: \"HIDE\"\n    - name: \"product_id\"\n      type: \"string\"\n      required: true\n    - name: \"amount\"\n      type: \"float\"\n      required: true\n    - name: \"sale_date\"\n      type: \"timestamp\"\n      required: true\n    - name: \"created_at\"\n      type: \"timestamp\"\n      required: true\n```\n\n### Step 3: Create Sample Data\n\nCreate a sample CSV file `data/sales_sample.csv`:\n\n```csv\norder_id,customer_id,product_id,amount,sale_date,created_at\nORD001,CUST001,PROD001,150.50,2024-01-15,2024-01-15 10:30:00\nORD002,CUST002,PROD002,299.99,2024-01-16,2024-01-16 14:20:00\nORD003,CUST001,PROD003,75.25,2024-01-17,2024-01-17 09:15:00\n```"
        },
        {
          "id": 5,
          "label": "Deploy and monitor",
          "content": "Now let's deploy your pipeline and monitor its execution.\n\n### Deploy the Pipeline\n\nRun the following commands to execute your pipeline:\n\n```bash\n# Extract data from source\nstarlake extract --config extract/azure-sales.yml\n\n# Load data into Azure\nstarlake load --config load/sales.yml\n\n# Or run both in sequence\nstarlake run --extract-config extract/azure-sales.yml --load-config load/sales.yml\n```\n\n### Monitor Execution\n\n#### 1. Check Starlake Logs\n\n```bash\n# View recent logs\ntail -f logs/starlake.log\n\n# Check execution status\nstarlake status --job-id <job_id>\n```\n\n#### 2. Monitor Azure Services\n\n- Check Azure Storage for uploaded files\n- Monitor Azure Monitor logs\n- Review Azure Data Factory job status\n\n#### 3. Verify Data in Azure\n\n```bash\n# Check loaded data\naz storage blob list --container-name starlake-data --account-name your_storage_account\n\n# Sample data\naz storage blob download --container-name starlake-data --name sales_data/part-00000.parquet --file - --account-name your_storage_account\n```\n\n### Next Steps\n\n- Set up scheduling with Azure Data Factory\n- Configure alerts and notifications\n- Scale your pipelines as needed\n- Implement data quality checks"
        },
        {
          "id": 6,
          "label": "Next steps",
          "content": "Congratulations! You've successfully set up Starlake with Azure.\n\n### What's Next?\n\n#### Advanced Configuration\n\n- **Data Quality**: Implement data validation rules\n- **Incremental Loading**: Set up efficient incremental updates\n- **Partitioning**: Optimize performance with table partitioning\n- **Clustering**: Improve query performance with clustering keys\n\n#### Scheduling and Orchestration\n\n```bash\n# Set up Azure Data Factory pipeline\naz datafactory pipeline create --resource-group your-rg --factory-name your-factory --name starlake-pipeline --pipeline @pipeline.json\n```\n\n#### Monitoring and Alerting\n\n- Set up Azure Monitor alerts\n- Configure Starlake notifications\n- Monitor data quality metrics\n- Track pipeline performance\n\n#### Security Enhancements\n\n- Implement Azure AD authentication\n- Set up storage account policies\n- Use Azure Key Vault for secrets\n- Configure network security groups\n\n### Resources\n\n- [Starlake Documentation](https://docs.starlake.ai)\n- [Azure Documentation](https://docs.microsoft.com/azure)\n- [Azure Best Practices](https://docs.microsoft.com/azure/architecture/best-practices/)\n- [Community Support](https://github.com/starlake-ai/starlake)\n\n### Need Help?\n\nIf you encounter any issues:\n1. Check the troubleshooting guide\n2. Review the logs for error messages\n3. Verify Azure permissions\n4. Reach out to the community\n\n### Example Advanced Configuration\n\n```yaml\n# Advanced Azure configuration\nconnections:\n  azure:\n    type: \"azure\"\n    tenant_id: \"${AZURE_TENANT_ID}\"\n    client_id: \"${AZURE_CLIENT_ID}\"\n    client_secret: \"${AZURE_CLIENT_SECRET}\"\n    subscription_id: \"${AZURE_SUBSCRIPTION_ID}\"\n    storage_account: \"${AZURE_STORAGE_ACCOUNT}\"\n    storage_key: \"${AZURE_STORAGE_KEY}\"\n    options:\n      container: \"starlake-data\"\n      endpoint: \"https://${AZURE_STORAGE_ACCOUNT}.blob.core.windows.net\"\n      encryption: \"AES256\"\n      lifecycle_policy: true\n      versioning: true\n```\n\n</div>"
        }
      ]
    },
    {
      "id": "bigquery",
      "title": "Quickstart for Starlake and BigQuery",
      "description": "Get started with Starlake and BigQuery in minutes",
      "categories": [
        "BigQuery"
      ],
      "icon": "/img/icons/bigquery.svg",
      "tags": [
        "BigQuery"
      ],
      "level": "Beginner",
      "content": "<div style={{maxWidth: '900px'}}>\n\n## Introduction\n\nIn this quickstart guide, you'll learn how to use Starlake with Google BigQuery. It will show you how to: \n\n- Set up BigQuery credentials and permissions\n- Configure Starlake for BigQuery\n- Create your first data pipeline\n- Deploy and monitor your pipelines\n\n### Prerequisites\n\n- A Google Cloud Platform account\n- Basic knowledge of BigQuery and GCP\n- Starlake CLI installed\n- Access to create datasets and tables\n\nLet's get started!\n\n## Set up BigQuery credentials\n\nBefore you can use Starlake with BigQuery, you need to configure your BigQuery credentials and permissions.\n\n### Step 1: Create a Google Cloud Project\n\n1. Go to the [Google Cloud Console](https://console.cloud.google.com)\n2. Click **\"Select a project\"** or **\"New Project\"**\n3. Enter a project name (e.g., `starlake-project`)\n4. Click **\"Create\"**\n\n### Step 2: Enable BigQuery API\n\n1. In the Google Cloud Console, go to **APIs & Services** > **Library**\n2. Search for **\"BigQuery API\"**\n3. Click on it and click **\"Enable\"**\n\n### Step 3: Create a Service Account\n\n1. Go to **IAM & Admin** > **Service Accounts**\n2. Click **\"Create Service Account\"**\n3. Enter a name (e.g., `starlake-service-account`)\n4. Add description: \"Service account for Starlake BigQuery integration\"\n5. Click **\"Create and Continue\"**\n\n### Step 4: Assign Permissions\n\n1. Add the following roles:\n   - **BigQuery Admin**\n   - **Storage Admin**\n   - **Service Account User**\n2. Click **\"Continue\"**\n3. Click **\"Done\"**\n\n### Step 5: Create and Download Key\n\n1. Click on your service account\n2. Go to **Keys** tab\n3. Click **\"Add Key\"** > **\"Create new key\"**\n4. Select **JSON** format\n5. Click **\"Create\"**\n6. Save the JSON file securely\n\n## Configure Starlake\n\nNow let's configure Starlake to work with your BigQuery environment.\n\n### Environment Variables\n\nSet the following environment variables in your shell:\n\n```bash\nexport GOOGLE_APPLICATION_CREDENTIALS=/path/to/your/service-account-key.json\nexport BIGQUERY_PROJECT_ID=your-project-id\nexport BIGQUERY_DATASET=starlake_dataset\nexport BIGQUERY_LOCATION=US\n```\n\n### Starlake Configuration\n\nCreate a `starlake.conf` file in your project root:\n\n```yaml\nconnections:\n  bigquery:\n    type: \"bigquery\"\n    project_id: \"${BIGQUERY_PROJECT_ID}\"\n    dataset: \"${BIGQUERY_DATASET}\"\n    location: \"${BIGQUERY_LOCATION}\"\n    credentials_file: \"${GOOGLE_APPLICATION_CREDENTIALS}\"\n    options:\n      useLegacySql: false\n      allowLargeResults: true\n\n# Global settings\nsettings:\n  default-connection: \"bigquery\"\n  default-format: \"PARQUET\"\n  default-mode: \"FILE\"\n```\n\n### Test Connection\n\nTest your connection with:\n\n```bash\nstarlake test-connection --connection bigquery\n```\n\n## Create your first pipeline\n\nLet's create a simple data pipeline using Starlake and BigQuery.\n\n### Step 1: Define the Extract\n\nCreate `extract/bigquery-sales.yml`:\n\n```yaml\nextract:\n  connectionRef: \"bigquery\"\n  tables:\n    - name: \"sales_data\"\n      schema: \"public\"\n      incremental: true\n      timestamp: \"created_at\"\n      partitionColumn: \"sale_date\"\n      fetchSize: 1000\n      where: \"sale_date >= '2024-01-01'\"\n\n# Optional: Add transformations during extract\ntransform:\n  - name: \"clean_sales_data\"\n    sql: |\n      SELECT \n        order_id,\n        customer_id,\n        product_id,\n        amount,\n        sale_date,\n        created_at\n      FROM sales_data\n      WHERE amount > 0\n```\n\n### Step 2: Define the Load\n\nCreate `load/sales.yml`:\n\n```yaml\ntable:\n  pattern: \"sales_data.*.parquet\"\n  metadata:\n    mode: \"FILE\"\n    format: \"PARQUET\"\n    encoding: \"UTF-8\"\n    withHeader: true\n  writeStrategy:\n    type: \"UPSERT_BY_KEY_AND_TIMESTAMP\"\n    timestamp: \"created_at\"\n    key: [\"order_id\"]\n  attributes:\n    - name: \"order_id\"\n      type: \"string\"\n      required: true\n      privacy: \"NONE\"\n    - name: \"customer_id\"\n      type: \"string\"\n      required: true\n      privacy: \"HIDE\"\n    - name: \"product_id\"\n      type: \"string\"\n      required: true\n    - name: \"amount\"\n      type: \"float\"\n      required: true\n    - name: \"sale_date\"\n      type: \"timestamp\"\n      required: true\n    - name: \"created_at\"\n      type: \"timestamp\"\n      required: true\n```\n\n### Step 3: Create Sample Data\n\nCreate a sample CSV file `data/sales_sample.csv`:\n\n```csv\norder_id,customer_id,product_id,amount,sale_date,created_at\nORD001,CUST001,PROD001,150.50,2024-01-15,2024-01-15 10:30:00\nORD002,CUST002,PROD002,299.99,2024-01-16,2024-01-16 14:20:00\nORD003,CUST001,PROD003,75.25,2024-01-17,2024-01-17 09:15:00\n```\n\n## Deploy and monitor\n\nNow let's deploy your pipeline and monitor its execution.\n\n### Deploy the Pipeline\n\nRun the following commands to execute your pipeline:\n\n```bash\n# Extract data from source\nstarlake extract --config extract/bigquery-sales.yml\n\n# Load data into BigQuery\nstarlake load --config load/sales.yml\n\n# Or run both in sequence\nstarlake run --extract-config extract/bigquery-sales.yml --load-config load/sales.yml\n```\n\n### Monitor Execution\n\n#### 1. Check Starlake Logs\n\n```bash\n# View recent logs\ntail -f logs/starlake.log\n\n# Check execution status\nstarlake status --job-id <job_id>\n```\n\n#### 2. Monitor BigQuery\n\nIn BigQuery console or using bq command:\n\n```sql\n-- Check loaded data\nSELECT COUNT(*) as total_rows \nFROM `your-project-id.starlake_dataset.sales_data`;\n\n-- Sample data\nSELECT * \nFROM `your-project-id.starlake_dataset.sales_data` \nLIMIT 10;\n\n-- Data quality check\nSELECT \n  COUNT(*) as total_orders,\n  SUM(amount) as total_amount,\n  MIN(sale_date) as earliest_sale,\n  MAX(sale_date) as latest_sale\nFROM `your-project-id.starlake_dataset.sales_data`;\n```\n\n#### 3. Monitor Job History\n\n```bash\n# List recent BigQuery jobs\nbq ls --jobs --max_results=10\n\n# Get job details\nbq show --job=true your-project-id:US.job_id\n```\n\n### Next Steps\n\n- Set up scheduling with Cloud Scheduler\n- Configure alerts and notifications\n- Scale your pipelines as needed\n- Implement data quality checks\n\n## Next steps\n\nCongratulations! You've successfully set up Starlake with BigQuery.\n\n### What's Next?\n\n#### Advanced Configuration\n\n- **Data Quality**: Implement data validation rules\n- **Incremental Loading**: Set up efficient incremental updates\n- **Partitioning**: Optimize performance with table partitioning\n- **Clustering**: Improve query performance with clustering keys\n\n#### Scheduling and Orchestration\n\n```bash\n# Set up Cloud Scheduler job\ngcloud scheduler jobs create http starlake-pipeline \\\n  --schedule=\"0 2 * * *\" \\\n  --uri=\"https://your-function-url.com/api/trigger\" \\\n  --http-method=POST \\\n  --message-body='{\"action\": \"run-pipeline\"}'\n```\n\n#### Monitoring and Alerting\n\n- Set up Cloud Monitoring alerts\n- Configure Starlake notifications\n- Monitor data quality metrics\n- Track pipeline performance\n\n#### Security Enhancements\n\n- Implement IAM roles and policies\n- Set up dataset-level permissions\n- Use customer-managed encryption keys\n- Configure VPC service controls\n\n### Resources\n\n- [Starlake Documentation](https://docs.starlake.ai)\n- [BigQuery Documentation](https://cloud.google.com/bigquery/docs)\n- [BigQuery Best Practices](https://cloud.google.com/bigquery/docs/best-practices)\n- [Community Support](https://github.com/starlake-ai/starlake)\n\n### Need Help?\n\nIf you encounter any issues:\n1. Check the troubleshooting guide\n2. Review the logs for error messages\n3. Verify BigQuery permissions\n4. Reach out to the community\n\n### Example Advanced Configuration\n\n```yaml\n# Advanced BigQuery configuration\nconnections:\n  bigquery:\n    type: \"bigquery\"\n    project_id: \"${BIGQUERY_PROJECT_ID}\"\n    dataset: \"${BIGQUERY_DATASET}\"\n    location: \"${BIGQUERY_LOCATION}\"\n    credentials_file: \"${GOOGLE_APPLICATION_CREDENTIALS}\"\n    options:\n      useLegacySql: false\n      allowLargeResults: true\n      maximumBytesBilled: \"1000000000\"\n      useQueryCache: true\n```\n\n</div>",
      "tabs": [
        {
          "id": 1,
          "label": "Introduction",
          "content": "In this quickstart guide, you'll learn how to use Starlake with Google BigQuery. It will show you how to: \n\n- Set up BigQuery credentials and permissions\n- Configure Starlake for BigQuery\n- Create your first data pipeline\n- Deploy and monitor your pipelines\n\n### Prerequisites\n\n- A Google Cloud Platform account\n- Basic knowledge of BigQuery and GCP\n- Starlake CLI installed\n- Access to create datasets and tables\n\nLet's get started!"
        },
        {
          "id": 2,
          "label": "Set up BigQuery credentials",
          "content": "Before you can use Starlake with BigQuery, you need to configure your BigQuery credentials and permissions.\n\n### Step 1: Create a Google Cloud Project\n\n1. Go to the [Google Cloud Console](https://console.cloud.google.com)\n2. Click **\"Select a project\"** or **\"New Project\"**\n3. Enter a project name (e.g., `starlake-project`)\n4. Click **\"Create\"**\n\n### Step 2: Enable BigQuery API\n\n1. In the Google Cloud Console, go to **APIs & Services** > **Library**\n2. Search for **\"BigQuery API\"**\n3. Click on it and click **\"Enable\"**\n\n### Step 3: Create a Service Account\n\n1. Go to **IAM & Admin** > **Service Accounts**\n2. Click **\"Create Service Account\"**\n3. Enter a name (e.g., `starlake-service-account`)\n4. Add description: \"Service account for Starlake BigQuery integration\"\n5. Click **\"Create and Continue\"**\n\n### Step 4: Assign Permissions\n\n1. Add the following roles:\n   - **BigQuery Admin**\n   - **Storage Admin**\n   - **Service Account User**\n2. Click **\"Continue\"**\n3. Click **\"Done\"**\n\n### Step 5: Create and Download Key\n\n1. Click on your service account\n2. Go to **Keys** tab\n3. Click **\"Add Key\"** > **\"Create new key\"**\n4. Select **JSON** format\n5. Click **\"Create\"**\n6. Save the JSON file securely"
        },
        {
          "id": 3,
          "label": "Configure Starlake",
          "content": "Now let's configure Starlake to work with your BigQuery environment.\n\n### Environment Variables\n\nSet the following environment variables in your shell:\n\n```bash\nexport GOOGLE_APPLICATION_CREDENTIALS=/path/to/your/service-account-key.json\nexport BIGQUERY_PROJECT_ID=your-project-id\nexport BIGQUERY_DATASET=starlake_dataset\nexport BIGQUERY_LOCATION=US\n```\n\n### Starlake Configuration\n\nCreate a `starlake.conf` file in your project root:\n\n```yaml\nconnections:\n  bigquery:\n    type: \"bigquery\"\n    project_id: \"${BIGQUERY_PROJECT_ID}\"\n    dataset: \"${BIGQUERY_DATASET}\"\n    location: \"${BIGQUERY_LOCATION}\"\n    credentials_file: \"${GOOGLE_APPLICATION_CREDENTIALS}\"\n    options:\n      useLegacySql: false\n      allowLargeResults: true\n\n# Global settings\nsettings:\n  default-connection: \"bigquery\"\n  default-format: \"PARQUET\"\n  default-mode: \"FILE\"\n```\n\n### Test Connection\n\nTest your connection with:\n\n```bash\nstarlake test-connection --connection bigquery\n```"
        },
        {
          "id": 4,
          "label": "Create your first pipeline",
          "content": "Let's create a simple data pipeline using Starlake and BigQuery.\n\n### Step 1: Define the Extract\n\nCreate `extract/bigquery-sales.yml`:\n\n```yaml\nextract:\n  connectionRef: \"bigquery\"\n  tables:\n    - name: \"sales_data\"\n      schema: \"public\"\n      incremental: true\n      timestamp: \"created_at\"\n      partitionColumn: \"sale_date\"\n      fetchSize: 1000\n      where: \"sale_date >= '2024-01-01'\"\n\n# Optional: Add transformations during extract\ntransform:\n  - name: \"clean_sales_data\"\n    sql: |\n      SELECT \n        order_id,\n        customer_id,\n        product_id,\n        amount,\n        sale_date,\n        created_at\n      FROM sales_data\n      WHERE amount > 0\n```\n\n### Step 2: Define the Load\n\nCreate `load/sales.yml`:\n\n```yaml\ntable:\n  pattern: \"sales_data.*.parquet\"\n  metadata:\n    mode: \"FILE\"\n    format: \"PARQUET\"\n    encoding: \"UTF-8\"\n    withHeader: true\n  writeStrategy:\n    type: \"UPSERT_BY_KEY_AND_TIMESTAMP\"\n    timestamp: \"created_at\"\n    key: [\"order_id\"]\n  attributes:\n    - name: \"order_id\"\n      type: \"string\"\n      required: true\n      privacy: \"NONE\"\n    - name: \"customer_id\"\n      type: \"string\"\n      required: true\n      privacy: \"HIDE\"\n    - name: \"product_id\"\n      type: \"string\"\n      required: true\n    - name: \"amount\"\n      type: \"float\"\n      required: true\n    - name: \"sale_date\"\n      type: \"timestamp\"\n      required: true\n    - name: \"created_at\"\n      type: \"timestamp\"\n      required: true\n```\n\n### Step 3: Create Sample Data\n\nCreate a sample CSV file `data/sales_sample.csv`:\n\n```csv\norder_id,customer_id,product_id,amount,sale_date,created_at\nORD001,CUST001,PROD001,150.50,2024-01-15,2024-01-15 10:30:00\nORD002,CUST002,PROD002,299.99,2024-01-16,2024-01-16 14:20:00\nORD003,CUST001,PROD003,75.25,2024-01-17,2024-01-17 09:15:00\n```"
        },
        {
          "id": 5,
          "label": "Deploy and monitor",
          "content": "Now let's deploy your pipeline and monitor its execution.\n\n### Deploy the Pipeline\n\nRun the following commands to execute your pipeline:\n\n```bash\n# Extract data from source\nstarlake extract --config extract/bigquery-sales.yml\n\n# Load data into BigQuery\nstarlake load --config load/sales.yml\n\n# Or run both in sequence\nstarlake run --extract-config extract/bigquery-sales.yml --load-config load/sales.yml\n```\n\n### Monitor Execution\n\n#### 1. Check Starlake Logs\n\n```bash\n# View recent logs\ntail -f logs/starlake.log\n\n# Check execution status\nstarlake status --job-id <job_id>\n```\n\n#### 2. Monitor BigQuery\n\nIn BigQuery console or using bq command:\n\n```sql\n-- Check loaded data\nSELECT COUNT(*) as total_rows \nFROM `your-project-id.starlake_dataset.sales_data`;\n\n-- Sample data\nSELECT * \nFROM `your-project-id.starlake_dataset.sales_data` \nLIMIT 10;\n\n-- Data quality check\nSELECT \n  COUNT(*) as total_orders,\n  SUM(amount) as total_amount,\n  MIN(sale_date) as earliest_sale,\n  MAX(sale_date) as latest_sale\nFROM `your-project-id.starlake_dataset.sales_data`;\n```\n\n#### 3. Monitor Job History\n\n```bash\n# List recent BigQuery jobs\nbq ls --jobs --max_results=10\n\n# Get job details\nbq show --job=true your-project-id:US.job_id\n```\n\n### Next Steps\n\n- Set up scheduling with Cloud Scheduler\n- Configure alerts and notifications\n- Scale your pipelines as needed\n- Implement data quality checks"
        },
        {
          "id": 6,
          "label": "Next steps",
          "content": "Congratulations! You've successfully set up Starlake with BigQuery.\n\n### What's Next?\n\n#### Advanced Configuration\n\n- **Data Quality**: Implement data validation rules\n- **Incremental Loading**: Set up efficient incremental updates\n- **Partitioning**: Optimize performance with table partitioning\n- **Clustering**: Improve query performance with clustering keys\n\n#### Scheduling and Orchestration\n\n```bash\n# Set up Cloud Scheduler job\ngcloud scheduler jobs create http starlake-pipeline \\\n  --schedule=\"0 2 * * *\" \\\n  --uri=\"https://your-function-url.com/api/trigger\" \\\n  --http-method=POST \\\n  --message-body='{\"action\": \"run-pipeline\"}'\n```\n\n#### Monitoring and Alerting\n\n- Set up Cloud Monitoring alerts\n- Configure Starlake notifications\n- Monitor data quality metrics\n- Track pipeline performance\n\n#### Security Enhancements\n\n- Implement IAM roles and policies\n- Set up dataset-level permissions\n- Use customer-managed encryption keys\n- Configure VPC service controls\n\n### Resources\n\n- [Starlake Documentation](https://docs.starlake.ai)\n- [BigQuery Documentation](https://cloud.google.com/bigquery/docs)\n- [BigQuery Best Practices](https://cloud.google.com/bigquery/docs/best-practices)\n- [Community Support](https://github.com/starlake-ai/starlake)\n\n### Need Help?\n\nIf you encounter any issues:\n1. Check the troubleshooting guide\n2. Review the logs for error messages\n3. Verify BigQuery permissions\n4. Reach out to the community\n\n### Example Advanced Configuration\n\n```yaml\n# Advanced BigQuery configuration\nconnections:\n  bigquery:\n    type: \"bigquery\"\n    project_id: \"${BIGQUERY_PROJECT_ID}\"\n    dataset: \"${BIGQUERY_DATASET}\"\n    location: \"${BIGQUERY_LOCATION}\"\n    credentials_file: \"${GOOGLE_APPLICATION_CREDENTIALS}\"\n    options:\n      useLegacySql: false\n      allowLargeResults: true\n      maximumBytesBilled: \"1000000000\"\n      useQueryCache: true\n```\n\n</div>"
        }
      ]
    },
    {
      "id": "databricks",
      "title": "Quickstart for Starlake and Databricks",
      "description": "Get started with Starlake and Databricks in minutes",
      "categories": [
        "Databricks"
      ],
      "icon": "/img/icons/databricks.svg",
      "tags": [
        "Databricks"
      ],
      "level": "Beginner",
      "content": "<div style={{maxWidth: '900px'}}>\n\n## Introduction\n\nIn this quickstart guide, you'll learn how to use Starlake with Databricks. It will show you how to: \n\n- Set up Databricks workspace and permissions\n- Configure Starlake for Databricks\n- Create your first data pipeline\n- Deploy and monitor your pipelines\n\n### Prerequisites\n\n- A Databricks account (trial or paid)\n- Basic knowledge of Databricks and Apache Spark\n- Starlake CLI installed\n- Access to create clusters and jobs\n\nLet's get started!\n\n## Set up Databricks credentials\n\nBefore you can use Starlake with Databricks, you need to configure your Databricks credentials and permissions.\n\n### Step 1: Create a Databricks Workspace\n\n1. Go to [Databricks](https://databricks.com) and sign up for a trial or log in\n2. Click **\"Create Workspace\"**\n3. Choose your cloud provider (AWS, Azure, or GCP)\n4. Follow the setup wizard to create your workspace\n\n### Step 2: Create a Personal Access Token\n\n1. In your Databricks workspace, click on your profile icon\n2. Select **\"User Settings\"**\n3. Go to **\"Access Tokens\"** tab\n4. Click **\"Generate New Token\"**\n5. Add a comment (e.g., \"Starlake integration\")\n6. Click **\"Generate\"**\n7. Copy and save the token securely\n\n### Step 3: Create a Cluster\n\n1. In your Databricks workspace, go to **\"Compute\"**\n2. Click **\"Create Cluster\"**\n3. Enter a cluster name (e.g., `starlake-cluster`)\n4. Choose **\"Single Node\"** for development\n5. Select **\"Databricks Runtime 13.3 LTS\"**\n6. Click **\"Create Cluster\"**\n\n### Step 4: Get Workspace Information\n\n1. Note your **Workspace URL** (e.g., `https://adb-1234567890123456.7.azuredatabricks.net`)\n2. Note your **Workspace ID** (from the URL)\n3. Save your **Personal Access Token**\n\n## Configure Starlake\n\nNow let's configure Starlake to work with your Databricks environment.\n\n### Environment Variables\n\nSet the following environment variables in your shell:\n\n```bash\nexport DATABRICKS_HOST=https://your-workspace-url\nexport DATABRICKS_TOKEN=your-personal-access-token\nexport DATABRICKS_CLUSTER_ID=your-cluster-id\nexport DATABRICKS_WORKSPACE_ID=your-workspace-id\n```\n\n### Starlake Configuration\n\nCreate a `starlake.conf` file in your project root:\n\n```yaml\nconnections:\n  databricks:\n    type: \"databricks\"\n    host: \"${DATABRICKS_HOST}\"\n    token: \"${DATABRICKS_TOKEN}\"\n    cluster_id: \"${DATABRICKS_CLUSTER_ID}\"\n    workspace_id: \"${DATABRICKS_WORKSPACE_ID}\"\n    options:\n      spark.sql.adaptive.enabled: true\n      spark.sql.adaptive.coalescePartitions.enabled: true\n\n# Global settings\nsettings:\n  default-connection: \"databricks\"\n  default-format: \"PARQUET\"\n  default-mode: \"FILE\"\n```\n\n### Test Connection\n\nTest your connection with:\n\n```bash\nstarlake test-connection --connection databricks\n```\n\n## Create your first pipeline\n\nLet's create a simple data pipeline using Starlake and Databricks.\n\n### Step 1: Define the Extract\n\nCreate `extract/databricks-sales.yml`:\n\n```yaml\nextract:\n  connectionRef: \"databricks\"\n  tables:\n    - name: \"sales_data\"\n      schema: \"public\"\n      incremental: true\n      timestamp: \"created_at\"\n      partitionColumn: \"sale_date\"\n      fetchSize: 1000\n      where: \"sale_date >= '2024-01-01'\"\n\n# Optional: Add transformations during extract\ntransform:\n  - name: \"clean_sales_data\"\n    sql: |\n      SELECT \n        order_id,\n        customer_id,\n        product_id,\n        amount,\n        sale_date,\n        created_at\n      FROM sales_data\n      WHERE amount > 0\n```\n\n### Step 2: Define the Load\n\nCreate `load/sales.yml`:\n\n```yaml\ntable:\n  pattern: \"sales_data.*.parquet\"\n  metadata:\n    mode: \"FILE\"\n    format: \"PARQUET\"\n    encoding: \"UTF-8\"\n    withHeader: true\n  writeStrategy:\n    type: \"UPSERT_BY_KEY_AND_TIMESTAMP\"\n    timestamp: \"created_at\"\n    key: [\"order_id\"]\n  attributes:\n    - name: \"order_id\"\n      type: \"string\"\n      required: true\n      privacy: \"NONE\"\n    - name: \"customer_id\"\n      type: \"string\"\n      required: true\n      privacy: \"HIDE\"\n    - name: \"product_id\"\n      type: \"string\"\n      required: true\n    - name: \"amount\"\n      type: \"float\"\n      required: true\n    - name: \"sale_date\"\n      type: \"timestamp\"\n      required: true\n    - name: \"created_at\"\n      type: \"timestamp\"\n      required: true\n```\n\n### Step 3: Create Sample Data\n\nCreate a sample CSV file `data/sales_sample.csv`:\n\n```csv\norder_id,customer_id,product_id,amount,sale_date,created_at\nORD001,CUST001,PROD001,150.50,2024-01-15,2024-01-15 10:30:00\nORD002,CUST002,PROD002,299.99,2024-01-16,2024-01-16 14:20:00\nORD003,CUST001,PROD003,75.25,2024-01-17,2024-01-17 09:15:00\n```\n\n## Deploy and monitor\n\nNow let's deploy your pipeline and monitor its execution.\n\n### Deploy the Pipeline\n\nRun the following commands to execute your pipeline:\n\n```bash\n# Extract data from source\nstarlake extract --config extract/databricks-sales.yml\n\n# Load data into Databricks\nstarlake load --config load/sales.yml\n\n# Or run both in sequence\nstarlake run --extract-config extract/databricks-sales.yml --load-config load/sales.yml\n```\n\n### Monitor Execution\n\n#### 1. Check Starlake Logs\n\n```bash\n# View recent logs\ntail -f logs/starlake.log\n\n# Check execution status\nstarlake status --job-id <job_id>\n```\n\n#### 2. Monitor Databricks\n\n- Check cluster status in Databricks UI\n- Monitor job runs in **\"Jobs\"** section\n- Review Spark UI for detailed execution metrics\n\n#### 3. Verify Data in Databricks\n\nIn a Databricks notebook:\n\n```sql\n-- Check loaded data\nSELECT COUNT(*) as total_rows FROM starlake_schema.sales_data;\n\n-- Sample data\nSELECT * FROM starlake_schema.sales_data LIMIT 10;\n\n-- Data quality check\nSELECT \n  COUNT(*) as total_orders,\n  SUM(amount) as total_amount,\n  MIN(sale_date) as earliest_sale,\n  MAX(sale_date) as latest_sale\nFROM starlake_schema.sales_data;\n```\n\n### Next Steps\n\n- Set up scheduling with Databricks Jobs\n- Configure alerts and notifications\n- Scale your pipelines as needed\n- Implement data quality checks\n\n## Next steps\n\nCongratulations! You've successfully set up Starlake with Databricks.\n\n### What's Next?\n\n#### Advanced Configuration\n\n- **Data Quality**: Implement data validation rules\n- **Incremental Loading**: Set up efficient incremental updates\n- **Partitioning**: Optimize performance with table partitioning\n- **Clustering**: Improve query performance with clustering keys\n\n#### Scheduling and Orchestration\n\n```bash\n# Create Databricks job\ndatabricks jobs create --json '{\n  \"name\": \"starlake-pipeline\",\n  \"existing_cluster_id\": \"your-cluster-id\",\n  \"notebook_task\": {\n    \"notebook_path\": \"/Shared/starlake-pipeline\"\n  },\n  \"schedule\": {\n    \"quartz_cron_expression\": \"0 0 2 * * ?\",\n    \"timezone_id\": \"UTC\"\n  }\n}'\n```\n\n#### Monitoring and Alerting\n\n- Set up Databricks alerts\n- Configure Starlake notifications\n- Monitor data quality metrics\n- Track pipeline performance\n\n#### Security Enhancements\n\n- Implement Unity Catalog\n- Set up access control lists\n- Use Databricks secrets\n- Configure network security\n\n### Resources\n\n- [Starlake Documentation](https://docs.starlake.ai)\n- [Databricks Documentation](https://docs.databricks.com)\n- [Databricks Best Practices](https://docs.databricks.com/best-practices/index.html)\n- [Community Support](https://github.com/starlake-ai/starlake)\n\n### Need Help?\n\nIf you encounter any issues:\n1. Check the troubleshooting guide\n2. Review the logs for error messages\n3. Verify Databricks permissions\n4. Reach out to the community\n\n### Example Advanced Configuration\n\n```yaml\n# Advanced Databricks configuration\nconnections:\n  databricks:\n    type: \"databricks\"\n    host: \"${DATABRICKS_HOST}\"\n    token: \"${DATABRICKS_TOKEN}\"\n    cluster_id: \"${DATABRICKS_CLUSTER_ID}\"\n    workspace_id: \"${DATABRICKS_WORKSPACE_ID}\"\n    options:\n      spark.sql.adaptive.enabled: true\n      spark.sql.adaptive.coalescePartitions.enabled: true\n      spark.sql.adaptive.skewJoin.enabled: true\n      spark.sql.adaptive.localShuffleReader.enabled: true\n```\n\n</div>",
      "tabs": [
        {
          "id": 1,
          "label": "Introduction",
          "content": "In this quickstart guide, you'll learn how to use Starlake with Databricks. It will show you how to: \n\n- Set up Databricks workspace and permissions\n- Configure Starlake for Databricks\n- Create your first data pipeline\n- Deploy and monitor your pipelines\n\n### Prerequisites\n\n- A Databricks account (trial or paid)\n- Basic knowledge of Databricks and Apache Spark\n- Starlake CLI installed\n- Access to create clusters and jobs\n\nLet's get started!"
        },
        {
          "id": 2,
          "label": "Set up Databricks credentials",
          "content": "Before you can use Starlake with Databricks, you need to configure your Databricks credentials and permissions.\n\n### Step 1: Create a Databricks Workspace\n\n1. Go to [Databricks](https://databricks.com) and sign up for a trial or log in\n2. Click **\"Create Workspace\"**\n3. Choose your cloud provider (AWS, Azure, or GCP)\n4. Follow the setup wizard to create your workspace\n\n### Step 2: Create a Personal Access Token\n\n1. In your Databricks workspace, click on your profile icon\n2. Select **\"User Settings\"**\n3. Go to **\"Access Tokens\"** tab\n4. Click **\"Generate New Token\"**\n5. Add a comment (e.g., \"Starlake integration\")\n6. Click **\"Generate\"**\n7. Copy and save the token securely\n\n### Step 3: Create a Cluster\n\n1. In your Databricks workspace, go to **\"Compute\"**\n2. Click **\"Create Cluster\"**\n3. Enter a cluster name (e.g., `starlake-cluster`)\n4. Choose **\"Single Node\"** for development\n5. Select **\"Databricks Runtime 13.3 LTS\"**\n6. Click **\"Create Cluster\"**\n\n### Step 4: Get Workspace Information\n\n1. Note your **Workspace URL** (e.g., `https://adb-1234567890123456.7.azuredatabricks.net`)\n2. Note your **Workspace ID** (from the URL)\n3. Save your **Personal Access Token**"
        },
        {
          "id": 3,
          "label": "Configure Starlake",
          "content": "Now let's configure Starlake to work with your Databricks environment.\n\n### Environment Variables\n\nSet the following environment variables in your shell:\n\n```bash\nexport DATABRICKS_HOST=https://your-workspace-url\nexport DATABRICKS_TOKEN=your-personal-access-token\nexport DATABRICKS_CLUSTER_ID=your-cluster-id\nexport DATABRICKS_WORKSPACE_ID=your-workspace-id\n```\n\n### Starlake Configuration\n\nCreate a `starlake.conf` file in your project root:\n\n```yaml\nconnections:\n  databricks:\n    type: \"databricks\"\n    host: \"${DATABRICKS_HOST}\"\n    token: \"${DATABRICKS_TOKEN}\"\n    cluster_id: \"${DATABRICKS_CLUSTER_ID}\"\n    workspace_id: \"${DATABRICKS_WORKSPACE_ID}\"\n    options:\n      spark.sql.adaptive.enabled: true\n      spark.sql.adaptive.coalescePartitions.enabled: true\n\n# Global settings\nsettings:\n  default-connection: \"databricks\"\n  default-format: \"PARQUET\"\n  default-mode: \"FILE\"\n```\n\n### Test Connection\n\nTest your connection with:\n\n```bash\nstarlake test-connection --connection databricks\n```"
        },
        {
          "id": 4,
          "label": "Create your first pipeline",
          "content": "Let's create a simple data pipeline using Starlake and Databricks.\n\n### Step 1: Define the Extract\n\nCreate `extract/databricks-sales.yml`:\n\n```yaml\nextract:\n  connectionRef: \"databricks\"\n  tables:\n    - name: \"sales_data\"\n      schema: \"public\"\n      incremental: true\n      timestamp: \"created_at\"\n      partitionColumn: \"sale_date\"\n      fetchSize: 1000\n      where: \"sale_date >= '2024-01-01'\"\n\n# Optional: Add transformations during extract\ntransform:\n  - name: \"clean_sales_data\"\n    sql: |\n      SELECT \n        order_id,\n        customer_id,\n        product_id,\n        amount,\n        sale_date,\n        created_at\n      FROM sales_data\n      WHERE amount > 0\n```\n\n### Step 2: Define the Load\n\nCreate `load/sales.yml`:\n\n```yaml\ntable:\n  pattern: \"sales_data.*.parquet\"\n  metadata:\n    mode: \"FILE\"\n    format: \"PARQUET\"\n    encoding: \"UTF-8\"\n    withHeader: true\n  writeStrategy:\n    type: \"UPSERT_BY_KEY_AND_TIMESTAMP\"\n    timestamp: \"created_at\"\n    key: [\"order_id\"]\n  attributes:\n    - name: \"order_id\"\n      type: \"string\"\n      required: true\n      privacy: \"NONE\"\n    - name: \"customer_id\"\n      type: \"string\"\n      required: true\n      privacy: \"HIDE\"\n    - name: \"product_id\"\n      type: \"string\"\n      required: true\n    - name: \"amount\"\n      type: \"float\"\n      required: true\n    - name: \"sale_date\"\n      type: \"timestamp\"\n      required: true\n    - name: \"created_at\"\n      type: \"timestamp\"\n      required: true\n```\n\n### Step 3: Create Sample Data\n\nCreate a sample CSV file `data/sales_sample.csv`:\n\n```csv\norder_id,customer_id,product_id,amount,sale_date,created_at\nORD001,CUST001,PROD001,150.50,2024-01-15,2024-01-15 10:30:00\nORD002,CUST002,PROD002,299.99,2024-01-16,2024-01-16 14:20:00\nORD003,CUST001,PROD003,75.25,2024-01-17,2024-01-17 09:15:00\n```"
        },
        {
          "id": 5,
          "label": "Deploy and monitor",
          "content": "Now let's deploy your pipeline and monitor its execution.\n\n### Deploy the Pipeline\n\nRun the following commands to execute your pipeline:\n\n```bash\n# Extract data from source\nstarlake extract --config extract/databricks-sales.yml\n\n# Load data into Databricks\nstarlake load --config load/sales.yml\n\n# Or run both in sequence\nstarlake run --extract-config extract/databricks-sales.yml --load-config load/sales.yml\n```\n\n### Monitor Execution\n\n#### 1. Check Starlake Logs\n\n```bash\n# View recent logs\ntail -f logs/starlake.log\n\n# Check execution status\nstarlake status --job-id <job_id>\n```\n\n#### 2. Monitor Databricks\n\n- Check cluster status in Databricks UI\n- Monitor job runs in **\"Jobs\"** section\n- Review Spark UI for detailed execution metrics\n\n#### 3. Verify Data in Databricks\n\nIn a Databricks notebook:\n\n```sql\n-- Check loaded data\nSELECT COUNT(*) as total_rows FROM starlake_schema.sales_data;\n\n-- Sample data\nSELECT * FROM starlake_schema.sales_data LIMIT 10;\n\n-- Data quality check\nSELECT \n  COUNT(*) as total_orders,\n  SUM(amount) as total_amount,\n  MIN(sale_date) as earliest_sale,\n  MAX(sale_date) as latest_sale\nFROM starlake_schema.sales_data;\n```\n\n### Next Steps\n\n- Set up scheduling with Databricks Jobs\n- Configure alerts and notifications\n- Scale your pipelines as needed\n- Implement data quality checks"
        },
        {
          "id": 6,
          "label": "Next steps",
          "content": "Congratulations! You've successfully set up Starlake with Databricks.\n\n### What's Next?\n\n#### Advanced Configuration\n\n- **Data Quality**: Implement data validation rules\n- **Incremental Loading**: Set up efficient incremental updates\n- **Partitioning**: Optimize performance with table partitioning\n- **Clustering**: Improve query performance with clustering keys\n\n#### Scheduling and Orchestration\n\n```bash\n# Create Databricks job\ndatabricks jobs create --json '{\n  \"name\": \"starlake-pipeline\",\n  \"existing_cluster_id\": \"your-cluster-id\",\n  \"notebook_task\": {\n    \"notebook_path\": \"/Shared/starlake-pipeline\"\n  },\n  \"schedule\": {\n    \"quartz_cron_expression\": \"0 0 2 * * ?\",\n    \"timezone_id\": \"UTC\"\n  }\n}'\n```\n\n#### Monitoring and Alerting\n\n- Set up Databricks alerts\n- Configure Starlake notifications\n- Monitor data quality metrics\n- Track pipeline performance\n\n#### Security Enhancements\n\n- Implement Unity Catalog\n- Set up access control lists\n- Use Databricks secrets\n- Configure network security\n\n### Resources\n\n- [Starlake Documentation](https://docs.starlake.ai)\n- [Databricks Documentation](https://docs.databricks.com)\n- [Databricks Best Practices](https://docs.databricks.com/best-practices/index.html)\n- [Community Support](https://github.com/starlake-ai/starlake)\n\n### Need Help?\n\nIf you encounter any issues:\n1. Check the troubleshooting guide\n2. Review the logs for error messages\n3. Verify Databricks permissions\n4. Reach out to the community\n\n### Example Advanced Configuration\n\n```yaml\n# Advanced Databricks configuration\nconnections:\n  databricks:\n    type: \"databricks\"\n    host: \"${DATABRICKS_HOST}\"\n    token: \"${DATABRICKS_TOKEN}\"\n    cluster_id: \"${DATABRICKS_CLUSTER_ID}\"\n    workspace_id: \"${DATABRICKS_WORKSPACE_ID}\"\n    options:\n      spark.sql.adaptive.enabled: true\n      spark.sql.adaptive.coalescePartitions.enabled: true\n      spark.sql.adaptive.skewJoin.enabled: true\n      spark.sql.adaptive.localShuffleReader.enabled: true\n```\n\n</div>"
        }
      ]
    },
    {
      "id": "gcp",
      "title": "Quickstart for Starlake and GCP",
      "description": "Get started with Starlake and Google Cloud Platform in minutes",
      "categories": [
        "GCP"
      ],
      "icon": "/img/icons/gcp.svg",
      "tags": [
        "GCP"
      ],
      "level": "Beginner",
      "content": "<div style={{maxWidth: '900px'}}>\n\n## Introduction\n\nIn this quickstart guide, you'll learn how to use Starlake with Google Cloud Platform (GCP). It will show you how to: \n\n- Set up GCP credentials and permissions\n- Configure Starlake for GCP services\n- Create your first data pipeline\n- Deploy and monitor your pipelines\n\n### Prerequisites\n\n- A Google Cloud Platform account\n- Basic knowledge of GCP services\n- Starlake CLI installed\n- Access to create projects and resources\n\nLet's get started!\n\n## Set up GCP credentials\n\nBefore you can use Starlake with GCP, you need to configure your GCP credentials and permissions.\n\n### Step 1: Create a Google Cloud Project\n\n1. Go to the [Google Cloud Console](https://console.cloud.google.com)\n2. Click **\"Select a project\"** or **\"New Project\"**\n3. Enter a project name (e.g., `starlake-project`)\n4. Click **\"Create\"**\n\n### Step 2: Enable Required APIs\n\n1. In the Google Cloud Console, go to **APIs & Services** > **Library**\n2. Search for and enable the following APIs:\n   - **BigQuery API**\n   - **Cloud Storage API**\n   - **Cloud Dataproc API** (if using Dataproc)\n   - **Cloud Functions API** (if using Cloud Functions)\n\n### Step 3: Create a Service Account\n\n1. Go to **IAM & Admin** > **Service Accounts**\n2. Click **\"Create Service Account\"**\n3. Enter a name (e.g., `starlake-service-account`)\n4. Add description: \"Service account for Starlake GCP integration\"\n5. Click **\"Create and Continue\"**\n\n### Step 4: Assign Permissions\n\n1. Add the following roles:\n   - **BigQuery Admin**\n   - **Storage Admin**\n   - **Dataproc Admin** (if using Dataproc)\n   - **Service Account User**\n2. Click **\"Continue\"**\n3. Click **\"Done\"**\n\n### Step 5: Create and Download Key\n\n1. Click on your service account\n2. Go to **Keys** tab\n3. Click **\"Add Key\"** > **\"Create new key\"**\n4. Select **JSON** format\n5. Click **\"Create\"**\n6. Save the JSON file securely\n\n## Configure Starlake\n\nNow let's configure Starlake to work with your GCP environment.\n\n### Environment Variables\n\nSet the following environment variables in your shell:\n\n```bash\nexport GOOGLE_APPLICATION_CREDENTIALS=/path/to/your/service-account-key.json\nexport GCP_PROJECT_ID=your-project-id\nexport GCP_REGION=us-central1\nexport GCP_BUCKET=your-data-bucket\n```\n\n### Starlake Configuration\n\nCreate a `starlake.conf` file in your project root:\n\n```yaml\nconnections:\n  gcp:\n    type: \"gcp\"\n    project_id: \"${GCP_PROJECT_ID}\"\n    region: \"${GCP_REGION}\"\n    bucket: \"${GCP_BUCKET}\"\n    credentials_file: \"${GOOGLE_APPLICATION_CREDENTIALS}\"\n    options:\n      storage_class: \"STANDARD\"\n      location: \"US\"\n\n# Global settings\nsettings:\n  default-connection: \"gcp\"\n  default-format: \"PARQUET\"\n  default-mode: \"FILE\"\n```\n\n### Test Connection\n\nTest your connection with:\n\n```bash\nstarlake test-connection --connection gcp\n```\n\n## Create your first pipeline\n\nLet's create a simple data pipeline using Starlake and GCP.\n\n### Step 1: Define the Extract\n\nCreate `extract/gcp-sales.yml`:\n\n```yaml\nextract:\n  connectionRef: \"gcp\"\n  tables:\n    - name: \"sales_data\"\n      schema: \"public\"\n      incremental: true\n      timestamp: \"created_at\"\n      partitionColumn: \"sale_date\"\n      fetchSize: 1000\n      where: \"sale_date >= '2024-01-01'\"\n\n# Optional: Add transformations during extract\ntransform:\n  - name: \"clean_sales_data\"\n    sql: |\n      SELECT \n        order_id,\n        customer_id,\n        product_id,\n        amount,\n        sale_date,\n        created_at\n      FROM sales_data\n      WHERE amount > 0\n```\n\n### Step 2: Define the Load\n\nCreate `load/sales.yml`:\n\n```yaml\ntable:\n  pattern: \"sales_data.*.parquet\"\n  metadata:\n    mode: \"FILE\"\n    format: \"PARQUET\"\n    encoding: \"UTF-8\"\n    withHeader: true\n  writeStrategy:\n    type: \"UPSERT_BY_KEY_AND_TIMESTAMP\"\n    timestamp: \"created_at\"\n    key: [\"order_id\"]\n  attributes:\n    - name: \"order_id\"\n      type: \"string\"\n      required: true\n      privacy: \"NONE\"\n    - name: \"customer_id\"\n      type: \"string\"\n      required: true\n      privacy: \"HIDE\"\n    - name: \"product_id\"\n      type: \"string\"\n      required: true\n    - name: \"amount\"\n      type: \"float\"\n      required: true\n    - name: \"sale_date\"\n      type: \"timestamp\"\n      required: true\n    - name: \"created_at\"\n      type: \"timestamp\"\n      required: true\n```\n\n### Step 3: Create Sample Data\n\nCreate a sample CSV file `data/sales_sample.csv`:\n\n```csv\norder_id,customer_id,product_id,amount,sale_date,created_at\nORD001,CUST001,PROD001,150.50,2024-01-15,2024-01-15 10:30:00\nORD002,CUST002,PROD002,299.99,2024-01-16,2024-01-16 14:20:00\nORD003,CUST001,PROD003,75.25,2024-01-17,2024-01-17 09:15:00\n```\n\n## Deploy and monitor\n\nNow let's deploy your pipeline and monitor its execution.\n\n### Deploy the Pipeline\n\nRun the following commands to execute your pipeline:\n\n```bash\n# Extract data from source\nstarlake extract --config extract/gcp-sales.yml\n\n# Load data into GCP\nstarlake load --config load/sales.yml\n\n# Or run both in sequence\nstarlake run --extract-config extract/gcp-sales.yml --load-config load/sales.yml\n```\n\n### Monitor Execution\n\n#### 1. Check Starlake Logs\n\n```bash\n# View recent logs\ntail -f logs/starlake.log\n\n# Check execution status\nstarlake status --job-id <job_id>\n```\n\n#### 2. Monitor GCP Services\n\n- Check Cloud Storage for uploaded files\n- Monitor Cloud Logging for execution logs\n- Review BigQuery for data processing\n\n#### 3. Verify Data in GCP\n\n```bash\n# Check loaded data in Cloud Storage\ngsutil ls gs://your-data-bucket/sales_data/\n\n# Sample data from BigQuery\nbq query --use_legacy_sql=false \"\nSELECT COUNT(*) as total_rows \nFROM \\`your-project-id.starlake_dataset.sales_data\\`\n\"\n```\n\n### Next Steps\n\n- Set up scheduling with Cloud Scheduler\n- Configure alerts and notifications\n- Scale your pipelines as needed\n- Implement data quality checks\n\n## Next steps\n\nCongratulations! You've successfully set up Starlake with GCP.\n\n### What's Next?\n\n#### Advanced Configuration\n\n- **Data Quality**: Implement data validation rules\n- **Incremental Loading**: Set up efficient incremental updates\n- **Partitioning**: Optimize performance with table partitioning\n- **Clustering**: Improve query performance with clustering keys\n\n#### Scheduling and Orchestration\n\n```bash\n# Set up Cloud Scheduler job\ngcloud scheduler jobs create http starlake-pipeline \\\n  --schedule=\"0 2 * * *\" \\\n  --uri=\"https://your-function-url.com/api/trigger\" \\\n  --http-method=POST \\\n  --message-body='{\"action\": \"run-pipeline\"}'\n```\n\n#### Monitoring and Alerting\n\n- Set up Cloud Monitoring alerts\n- Configure Starlake notifications\n- Monitor data quality metrics\n- Track pipeline performance\n\n#### Security Enhancements\n\n- Implement IAM roles and policies\n- Set up bucket-level permissions\n- Use customer-managed encryption keys\n- Configure VPC service controls\n\n### Resources\n\n- [Starlake Documentation](https://docs.starlake.ai)\n- [GCP Documentation](https://cloud.google.com/docs)\n- [GCP Best Practices](https://cloud.google.com/architecture/best-practices)\n- [Community Support](https://github.com/starlake-ai/starlake)\n\n### Need Help?\n\nIf you encounter any issues:\n1. Check the troubleshooting guide\n2. Review the logs for error messages\n3. Verify GCP permissions\n4. Reach out to the community\n\n### Example Advanced Configuration\n\n```yaml\n# Advanced GCP configuration\nconnections:\n  gcp:\n    type: \"gcp\"\n    project_id: \"${GCP_PROJECT_ID}\"\n    region: \"${GCP_REGION}\"\n    bucket: \"${GCP_BUCKET}\"\n    credentials_file: \"${GOOGLE_APPLICATION_CREDENTIALS}\"\n    options:\n      storage_class: \"STANDARD\"\n      location: \"US\"\n      encryption: \"AES256\"\n      lifecycle_policy: true\n      versioning: true\n```\n\n</div>",
      "tabs": [
        {
          "id": 1,
          "label": "Introduction",
          "content": "In this quickstart guide, you'll learn how to use Starlake with Google Cloud Platform (GCP). It will show you how to: \n\n- Set up GCP credentials and permissions\n- Configure Starlake for GCP services\n- Create your first data pipeline\n- Deploy and monitor your pipelines\n\n### Prerequisites\n\n- A Google Cloud Platform account\n- Basic knowledge of GCP services\n- Starlake CLI installed\n- Access to create projects and resources\n\nLet's get started!"
        },
        {
          "id": 2,
          "label": "Set up GCP credentials",
          "content": "Before you can use Starlake with GCP, you need to configure your GCP credentials and permissions.\n\n### Step 1: Create a Google Cloud Project\n\n1. Go to the [Google Cloud Console](https://console.cloud.google.com)\n2. Click **\"Select a project\"** or **\"New Project\"**\n3. Enter a project name (e.g., `starlake-project`)\n4. Click **\"Create\"**\n\n### Step 2: Enable Required APIs\n\n1. In the Google Cloud Console, go to **APIs & Services** > **Library**\n2. Search for and enable the following APIs:\n   - **BigQuery API**\n   - **Cloud Storage API**\n   - **Cloud Dataproc API** (if using Dataproc)\n   - **Cloud Functions API** (if using Cloud Functions)\n\n### Step 3: Create a Service Account\n\n1. Go to **IAM & Admin** > **Service Accounts**\n2. Click **\"Create Service Account\"**\n3. Enter a name (e.g., `starlake-service-account`)\n4. Add description: \"Service account for Starlake GCP integration\"\n5. Click **\"Create and Continue\"**\n\n### Step 4: Assign Permissions\n\n1. Add the following roles:\n   - **BigQuery Admin**\n   - **Storage Admin**\n   - **Dataproc Admin** (if using Dataproc)\n   - **Service Account User**\n2. Click **\"Continue\"**\n3. Click **\"Done\"**\n\n### Step 5: Create and Download Key\n\n1. Click on your service account\n2. Go to **Keys** tab\n3. Click **\"Add Key\"** > **\"Create new key\"**\n4. Select **JSON** format\n5. Click **\"Create\"**\n6. Save the JSON file securely"
        },
        {
          "id": 3,
          "label": "Configure Starlake",
          "content": "Now let's configure Starlake to work with your GCP environment.\n\n### Environment Variables\n\nSet the following environment variables in your shell:\n\n```bash\nexport GOOGLE_APPLICATION_CREDENTIALS=/path/to/your/service-account-key.json\nexport GCP_PROJECT_ID=your-project-id\nexport GCP_REGION=us-central1\nexport GCP_BUCKET=your-data-bucket\n```\n\n### Starlake Configuration\n\nCreate a `starlake.conf` file in your project root:\n\n```yaml\nconnections:\n  gcp:\n    type: \"gcp\"\n    project_id: \"${GCP_PROJECT_ID}\"\n    region: \"${GCP_REGION}\"\n    bucket: \"${GCP_BUCKET}\"\n    credentials_file: \"${GOOGLE_APPLICATION_CREDENTIALS}\"\n    options:\n      storage_class: \"STANDARD\"\n      location: \"US\"\n\n# Global settings\nsettings:\n  default-connection: \"gcp\"\n  default-format: \"PARQUET\"\n  default-mode: \"FILE\"\n```\n\n### Test Connection\n\nTest your connection with:\n\n```bash\nstarlake test-connection --connection gcp\n```"
        },
        {
          "id": 4,
          "label": "Create your first pipeline",
          "content": "Let's create a simple data pipeline using Starlake and GCP.\n\n### Step 1: Define the Extract\n\nCreate `extract/gcp-sales.yml`:\n\n```yaml\nextract:\n  connectionRef: \"gcp\"\n  tables:\n    - name: \"sales_data\"\n      schema: \"public\"\n      incremental: true\n      timestamp: \"created_at\"\n      partitionColumn: \"sale_date\"\n      fetchSize: 1000\n      where: \"sale_date >= '2024-01-01'\"\n\n# Optional: Add transformations during extract\ntransform:\n  - name: \"clean_sales_data\"\n    sql: |\n      SELECT \n        order_id,\n        customer_id,\n        product_id,\n        amount,\n        sale_date,\n        created_at\n      FROM sales_data\n      WHERE amount > 0\n```\n\n### Step 2: Define the Load\n\nCreate `load/sales.yml`:\n\n```yaml\ntable:\n  pattern: \"sales_data.*.parquet\"\n  metadata:\n    mode: \"FILE\"\n    format: \"PARQUET\"\n    encoding: \"UTF-8\"\n    withHeader: true\n  writeStrategy:\n    type: \"UPSERT_BY_KEY_AND_TIMESTAMP\"\n    timestamp: \"created_at\"\n    key: [\"order_id\"]\n  attributes:\n    - name: \"order_id\"\n      type: \"string\"\n      required: true\n      privacy: \"NONE\"\n    - name: \"customer_id\"\n      type: \"string\"\n      required: true\n      privacy: \"HIDE\"\n    - name: \"product_id\"\n      type: \"string\"\n      required: true\n    - name: \"amount\"\n      type: \"float\"\n      required: true\n    - name: \"sale_date\"\n      type: \"timestamp\"\n      required: true\n    - name: \"created_at\"\n      type: \"timestamp\"\n      required: true\n```\n\n### Step 3: Create Sample Data\n\nCreate a sample CSV file `data/sales_sample.csv`:\n\n```csv\norder_id,customer_id,product_id,amount,sale_date,created_at\nORD001,CUST001,PROD001,150.50,2024-01-15,2024-01-15 10:30:00\nORD002,CUST002,PROD002,299.99,2024-01-16,2024-01-16 14:20:00\nORD003,CUST001,PROD003,75.25,2024-01-17,2024-01-17 09:15:00\n```"
        },
        {
          "id": 5,
          "label": "Deploy and monitor",
          "content": "Now let's deploy your pipeline and monitor its execution.\n\n### Deploy the Pipeline\n\nRun the following commands to execute your pipeline:\n\n```bash\n# Extract data from source\nstarlake extract --config extract/gcp-sales.yml\n\n# Load data into GCP\nstarlake load --config load/sales.yml\n\n# Or run both in sequence\nstarlake run --extract-config extract/gcp-sales.yml --load-config load/sales.yml\n```\n\n### Monitor Execution\n\n#### 1. Check Starlake Logs\n\n```bash\n# View recent logs\ntail -f logs/starlake.log\n\n# Check execution status\nstarlake status --job-id <job_id>\n```\n\n#### 2. Monitor GCP Services\n\n- Check Cloud Storage for uploaded files\n- Monitor Cloud Logging for execution logs\n- Review BigQuery for data processing\n\n#### 3. Verify Data in GCP\n\n```bash\n# Check loaded data in Cloud Storage\ngsutil ls gs://your-data-bucket/sales_data/\n\n# Sample data from BigQuery\nbq query --use_legacy_sql=false \"\nSELECT COUNT(*) as total_rows \nFROM \\`your-project-id.starlake_dataset.sales_data\\`\n\"\n```\n\n### Next Steps\n\n- Set up scheduling with Cloud Scheduler\n- Configure alerts and notifications\n- Scale your pipelines as needed\n- Implement data quality checks"
        },
        {
          "id": 6,
          "label": "Next steps",
          "content": "Congratulations! You've successfully set up Starlake with GCP.\n\n### What's Next?\n\n#### Advanced Configuration\n\n- **Data Quality**: Implement data validation rules\n- **Incremental Loading**: Set up efficient incremental updates\n- **Partitioning**: Optimize performance with table partitioning\n- **Clustering**: Improve query performance with clustering keys\n\n#### Scheduling and Orchestration\n\n```bash\n# Set up Cloud Scheduler job\ngcloud scheduler jobs create http starlake-pipeline \\\n  --schedule=\"0 2 * * *\" \\\n  --uri=\"https://your-function-url.com/api/trigger\" \\\n  --http-method=POST \\\n  --message-body='{\"action\": \"run-pipeline\"}'\n```\n\n#### Monitoring and Alerting\n\n- Set up Cloud Monitoring alerts\n- Configure Starlake notifications\n- Monitor data quality metrics\n- Track pipeline performance\n\n#### Security Enhancements\n\n- Implement IAM roles and policies\n- Set up bucket-level permissions\n- Use customer-managed encryption keys\n- Configure VPC service controls\n\n### Resources\n\n- [Starlake Documentation](https://docs.starlake.ai)\n- [GCP Documentation](https://cloud.google.com/docs)\n- [GCP Best Practices](https://cloud.google.com/architecture/best-practices)\n- [Community Support](https://github.com/starlake-ai/starlake)\n\n### Need Help?\n\nIf you encounter any issues:\n1. Check the troubleshooting guide\n2. Review the logs for error messages\n3. Verify GCP permissions\n4. Reach out to the community\n\n### Example Advanced Configuration\n\n```yaml\n# Advanced GCP configuration\nconnections:\n  gcp:\n    type: \"gcp\"\n    project_id: \"${GCP_PROJECT_ID}\"\n    region: \"${GCP_REGION}\"\n    bucket: \"${GCP_BUCKET}\"\n    credentials_file: \"${GOOGLE_APPLICATION_CREDENTIALS}\"\n    options:\n      storage_class: \"STANDARD\"\n      location: \"US\"\n      encryption: \"AES256\"\n      lifecycle_policy: true\n      versioning: true\n```\n\n</div>"
        }
      ]
    },
    {
      "id": "snowflake",
      "title": "Quickstart for Starlake and Snowflake",
      "description": "Get started with Starlake and Snowflake in minutes",
      "categories": [
        "Snowflake"
      ],
      "icon": "/img/icons/snowflake.svg",
      "tags": [
        "Snowflake"
      ],
      "level": "Beginner",
      "content": "<div style={{maxWidth: '900px'}}>\n\n## Introduction\n\nIn this quickstart guide, you'll learn how to use Starlake with Snowflake. It will show you how to: \n\n- Set up Snowflake credentials and permissions\n- Configure Starlake for Snowflake\n- Create your first data pipeline\n- Deploy and monitor your pipelines\n- Set up scheduling and orchestration\n\n### Prerequisites\n\n- A Snowflake account (trial or paid)\n- Basic knowledge of SQL and data warehousing\n- Starlake CLI installed\n- Access to create databases and schemas\n\n### Why Snowflake + Starlake?\n\n- **Scalability**: Snowflake's elastic compute and storage\n- **Performance**: Optimized for large-scale data processing\n- **Security**: Built-in security features and compliance\n- **Cost-effective**: Pay-per-use pricing model\n\nLet's get started!\n\n## Set up Snowflake credentials\n\nBefore you can use Starlake with Snowflake, you need to configure your Snowflake credentials and permissions.\n\n### Step 1: Create a Snowflake User\n\n1. Log in to your Snowflake account\n2. Navigate to **Admin** > **Users**\n3. Click **\"Create User\"**\n4. Enter username (e.g., `starlake_user`)\n5. Set a secure password\n6. Assign appropriate roles (e.g., `ACCOUNTADMIN` for setup)\n\n### Step 2: Create Database and Schema\n\nExecute these SQL commands in your Snowflake worksheet:\n\n```sql\n-- Create database for Starlake\nCREATE DATABASE starlake_db;\n\n-- Create schema for your data\nCREATE SCHEMA starlake_db.starlake_schema;\n\n-- Create warehouse for compute resources\nCREATE WAREHOUSE starlake_wh\n  WAREHOUSE_SIZE = 'X-SMALL'\n  AUTO_SUSPEND = 60\n  AUTO_RESUME = TRUE;\n```\n\n### Step 3: Grant Permissions\n\n```sql\n-- Grant usage on database\nGRANT USAGE ON DATABASE starlake_db TO ROLE your_role;\n\n-- Grant usage on schema\nGRANT USAGE ON SCHEMA starlake_db.starlake_schema TO ROLE your_role;\n\n-- Grant table creation permissions\nGRANT CREATE TABLE ON SCHEMA starlake_db.starlake_schema TO ROLE your_role;\n\n-- Grant warehouse usage\nGRANT USAGE ON WAREHOUSE starlake_wh TO ROLE your_role;\n\n-- Grant data loading permissions\nGRANT INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA starlake_db.starlake_schema TO ROLE your_role;\n```\n\n## Configure Starlake\n\nNow let's configure Starlake to work with your Snowflake environment.\n\n### Environment Variables\n\nSet the following environment variables in your shell:\n\n```bash\nexport SNOWFLAKE_ACCOUNT=your_account_identifier\nexport SNOWFLAKE_USER=starlake_user\nexport SNOWFLAKE_PASSWORD=your_secure_password\nexport SNOWFLAKE_DATABASE=starlake_db\nexport SNOWFLAKE_SCHEMA=starlake_schema\nexport SNOWFLAKE_WAREHOUSE=starlake_wh\nexport SNOWFLAKE_ROLE=your_role\n```\n\n**Note**: Replace `your_account_identifier` with your Snowflake account identifier (e.g., `xy12345.us-east-1`).\n\n### Starlake Configuration\n\nCreate a `starlake.conf` file in your project root:\n\n```yaml\nconnections:\n  snowflake:\n    type: \"snowflake\"\n    account: \"${SNOWFLAKE_ACCOUNT}\"\n    user: \"${SNOWFLAKE_USER}\"\n    password: \"${SNOWFLAKE_PASSWORD}\"\n    database: \"${SNOWFLAKE_DATABASE}\"\n    schema: \"${SNOWFLAKE_SCHEMA}\"\n    warehouse: \"${SNOWFLAKE_WAREHOUSE}\"\n    role: \"${SNOWFLAKE_ROLE}\"\n    options:\n      client_session_keep_alive: true\n      timezone: \"UTC\"\n\n# Global settings\nsettings:\n  default-connection: \"snowflake\"\n  default-format: \"PARQUET\"\n  default-mode: \"FILE\"\n```\n\n### Test Connection\n\nTest your connection with:\n\n```bash\nstarlake test-connection --connection snowflake\n```\n\n## Create your first pipeline\n\nLet's create a simple data pipeline using Starlake and Snowflake.\n\n### Step 1: Define the Extract\n\nCreate `extract/snowflake-sales.yml`:\n\n```yaml\nextract:\n  connectionRef: \"snowflake\"\n  tables:\n    - name: \"sales_data\"\n      schema: \"public\"\n      incremental: true\n      timestamp: \"created_at\"\n      partitionColumn: \"sale_date\"\n      fetchSize: 1000\n      where: \"sale_date >= '2024-01-01'\"\n\n# Optional: Add transformations during extract\ntransform:\n  - name: \"clean_sales_data\"\n    sql: |\n      SELECT \n        order_id,\n        customer_id,\n        product_id,\n        amount,\n        sale_date,\n        created_at\n      FROM sales_data\n      WHERE amount > 0\n```\n\n### Step 2: Define the Load\n\nCreate `load/sales.yml`:\n\n```yaml\ntable:\n  pattern: \"sales_data.*.parquet\"\n  metadata:\n    mode: \"FILE\"\n    format: \"PARQUET\"\n    encoding: \"UTF-8\"\n    withHeader: true\n  writeStrategy:\n    type: \"UPSERT_BY_KEY_AND_TIMESTAMP\"\n    timestamp: \"created_at\"\n    key: [\"order_id\"]\n  attributes:\n    - name: \"order_id\"\n      type: \"string\"\n      required: true\n      privacy: \"NONE\"\n    - name: \"customer_id\"\n      type: \"string\"\n      required: true\n      privacy: \"HIDE\"\n    - name: \"product_id\"\n      type: \"string\"\n      required: true\n    - name: \"amount\"\n      type: \"float\"\n      required: true\n    - name: \"sale_date\"\n      type: \"timestamp\"\n      required: true\n    - name: \"created_at\"\n      type: \"timestamp\"\n      required: true\n```\n\n### Step 3: Create Sample Data\n\nCreate a sample CSV file `data/sales_sample.csv`:\n\n```csv\norder_id,customer_id,product_id,amount,sale_date,created_at\nORD001,CUST001,PROD001,150.50,2024-01-15,2024-01-15 10:30:00\nORD002,CUST002,PROD002,299.99,2024-01-16,2024-01-16 14:20:00\nORD003,CUST001,PROD003,75.25,2024-01-17,2024-01-17 09:15:00\n```\n\n## Deploy and monitor\n\nNow let's deploy your pipeline and monitor its execution.\n\n### Deploy the Pipeline\n\nRun the following commands to execute your pipeline:\n\n```bash\n# Extract data from source\nstarlake extract --config extract/snowflake-sales.yml\n\n# Load data into Snowflake\nstarlake load --config load/sales.yml\n\n# Or run both in sequence\nstarlake run --extract-config extract/snowflake-sales.yml --load-config load/sales.yml\n```\n\n### Monitor Execution\n\n#### 1. Check Starlake Logs\n\n```bash\n# View recent logs\ntail -f logs/starlake.log\n\n# Check execution status\nstarlake status --job-id <job_id>\n```\n\n#### 2. Monitor Snowflake Query History\n\nIn Snowflake worksheet:\n\n```sql\n-- View recent queries\nSELECT \n  query_id,\n  query_text,\n  start_time,\n  end_time,\n  total_elapsed_time,\n  status\nFROM table(information_schema.query_history())\nWHERE start_time >= dateadd(hour, -1, current_timestamp())\nORDER BY start_time DESC;\n\n-- Check warehouse usage\nSELECT \n  warehouse_name,\n  credits_used,\n  bytes_scanned,\n  percentage_scanned_from_cache\nFROM table(information_schema.warehouse_metering_history(\n  date_range_start=>dateadd(hour, -1, current_timestamp()),\n  date_range_end=>current_timestamp()\n));\n```\n\n#### 3. Verify Data in Snowflake\n\n```sql\n-- Check loaded data\nSELECT COUNT(*) as total_rows FROM starlake_db.starlake_schema.sales_data;\n\n-- Sample data\nSELECT * FROM starlake_db.starlake_schema.sales_data LIMIT 10;\n\n-- Data quality check\nSELECT \n  COUNT(*) as total_orders,\n  SUM(amount) as total_amount,\n  MIN(sale_date) as earliest_sale,\n  MAX(sale_date) as latest_sale\nFROM starlake_db.starlake_schema.sales_data;\n```\n\n### Next Steps\n\n- Set up scheduling with cron or Airflow\n- Configure alerts and notifications\n- Scale your pipelines as needed\n- Implement data quality checks\n\n## Next steps\n\nCongratulations! You've successfully set up Starlake with Snowflake.\n\n### What's Next?\n\n#### Advanced Configuration\n\n- **Data Quality**: Implement data validation rules\n- **Incremental Loading**: Set up efficient incremental updates\n- **Partitioning**: Optimize performance with table partitioning\n- **Clustering**: Improve query performance with clustering keys\n\n#### Scheduling and Orchestration\n\n```bash\n# Set up cron job for daily execution\ncrontab -e\n\n# Add this line for daily execution at 2 AM\n0 2 * * * /path/to/starlake run --extract-config extract/snowflake-sales.yml --load-config load/sales.yml\n```\n\n#### Monitoring and Alerting\n\n- Set up Snowflake alerts for warehouse usage\n- Configure Starlake notifications\n- Monitor data quality metrics\n- Track pipeline performance\n\n#### Security Enhancements\n\n- Implement row-level security (RLS)\n- Set up column-level security\n- Use Snowflake's data masking features\n- Configure network policies\n\n### Resources\n\n- [Starlake Documentation](https://docs.starlake.ai)\n- [Snowflake Documentation](https://docs.snowflake.com)\n- [Snowflake Best Practices](https://docs.snowflake.com/en/user-guide/best-practices-overview)\n- [Community Support](https://github.com/starlake-ai/starlake)\n\n### Need Help?\n\nIf you encounter any issues:\n1. Check the troubleshooting guide\n2. Review the logs for error messages\n3. Verify Snowflake permissions\n4. Reach out to the community\n\n### Example Advanced Configuration\n\n```yaml\n# Advanced Snowflake configuration\nconnections:\n  snowflake:\n    type: \"snowflake\"\n    account: \"${SNOWFLAKE_ACCOUNT}\"\n    user: \"${SNOWFLAKE_USER}\"\n    password: \"${SNOWFLAKE_PASSWORD}\"\n    database: \"${SNOWFLAKE_DATABASE}\"\n    schema: \"${SNOWFLAKE_SCHEMA}\"\n    warehouse: \"${SNOWFLAKE_WAREHOUSE}\"\n    role: \"${SNOWFLAKE_ROLE}\"\n    options:\n      client_session_keep_alive: true\n      timezone: \"UTC\"\n      session_parameters:\n        USE_CACHED_RESULT: true\n        STATEMENT_TIMEOUT_IN_SECONDS: 3600\n```\n\n</div>",
      "tabs": [
        {
          "id": 1,
          "label": "Introduction",
          "content": "In this quickstart guide, you'll learn how to use Starlake with Snowflake. It will show you how to: \n\n- Set up Snowflake credentials and permissions\n- Configure Starlake for Snowflake\n- Create your first data pipeline\n- Deploy and monitor your pipelines\n- Set up scheduling and orchestration\n\n### Prerequisites\n\n- A Snowflake account (trial or paid)\n- Basic knowledge of SQL and data warehousing\n- Starlake CLI installed\n- Access to create databases and schemas\n\n### Why Snowflake + Starlake?\n\n- **Scalability**: Snowflake's elastic compute and storage\n- **Performance**: Optimized for large-scale data processing\n- **Security**: Built-in security features and compliance\n- **Cost-effective**: Pay-per-use pricing model\n\nLet's get started!"
        },
        {
          "id": 2,
          "label": "Set up Snowflake credentials",
          "content": "Before you can use Starlake with Snowflake, you need to configure your Snowflake credentials and permissions.\n\n### Step 1: Create a Snowflake User\n\n1. Log in to your Snowflake account\n2. Navigate to **Admin** > **Users**\n3. Click **\"Create User\"**\n4. Enter username (e.g., `starlake_user`)\n5. Set a secure password\n6. Assign appropriate roles (e.g., `ACCOUNTADMIN` for setup)\n\n### Step 2: Create Database and Schema\n\nExecute these SQL commands in your Snowflake worksheet:\n\n```sql\n-- Create database for Starlake\nCREATE DATABASE starlake_db;\n\n-- Create schema for your data\nCREATE SCHEMA starlake_db.starlake_schema;\n\n-- Create warehouse for compute resources\nCREATE WAREHOUSE starlake_wh\n  WAREHOUSE_SIZE = 'X-SMALL'\n  AUTO_SUSPEND = 60\n  AUTO_RESUME = TRUE;\n```\n\n### Step 3: Grant Permissions\n\n```sql\n-- Grant usage on database\nGRANT USAGE ON DATABASE starlake_db TO ROLE your_role;\n\n-- Grant usage on schema\nGRANT USAGE ON SCHEMA starlake_db.starlake_schema TO ROLE your_role;\n\n-- Grant table creation permissions\nGRANT CREATE TABLE ON SCHEMA starlake_db.starlake_schema TO ROLE your_role;\n\n-- Grant warehouse usage\nGRANT USAGE ON WAREHOUSE starlake_wh TO ROLE your_role;\n\n-- Grant data loading permissions\nGRANT INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA starlake_db.starlake_schema TO ROLE your_role;\n```"
        },
        {
          "id": 3,
          "label": "Configure Starlake",
          "content": "Now let's configure Starlake to work with your Snowflake environment.\n\n### Environment Variables\n\nSet the following environment variables in your shell:\n\n```bash\nexport SNOWFLAKE_ACCOUNT=your_account_identifier\nexport SNOWFLAKE_USER=starlake_user\nexport SNOWFLAKE_PASSWORD=your_secure_password\nexport SNOWFLAKE_DATABASE=starlake_db\nexport SNOWFLAKE_SCHEMA=starlake_schema\nexport SNOWFLAKE_WAREHOUSE=starlake_wh\nexport SNOWFLAKE_ROLE=your_role\n```\n\n**Note**: Replace `your_account_identifier` with your Snowflake account identifier (e.g., `xy12345.us-east-1`).\n\n### Starlake Configuration\n\nCreate a `starlake.conf` file in your project root:\n\n```yaml\nconnections:\n  snowflake:\n    type: \"snowflake\"\n    account: \"${SNOWFLAKE_ACCOUNT}\"\n    user: \"${SNOWFLAKE_USER}\"\n    password: \"${SNOWFLAKE_PASSWORD}\"\n    database: \"${SNOWFLAKE_DATABASE}\"\n    schema: \"${SNOWFLAKE_SCHEMA}\"\n    warehouse: \"${SNOWFLAKE_WAREHOUSE}\"\n    role: \"${SNOWFLAKE_ROLE}\"\n    options:\n      client_session_keep_alive: true\n      timezone: \"UTC\"\n\n# Global settings\nsettings:\n  default-connection: \"snowflake\"\n  default-format: \"PARQUET\"\n  default-mode: \"FILE\"\n```\n\n### Test Connection\n\nTest your connection with:\n\n```bash\nstarlake test-connection --connection snowflake\n```"
        },
        {
          "id": 4,
          "label": "Create your first pipeline",
          "content": "Let's create a simple data pipeline using Starlake and Snowflake.\n\n### Step 1: Define the Extract\n\nCreate `extract/snowflake-sales.yml`:\n\n```yaml\nextract:\n  connectionRef: \"snowflake\"\n  tables:\n    - name: \"sales_data\"\n      schema: \"public\"\n      incremental: true\n      timestamp: \"created_at\"\n      partitionColumn: \"sale_date\"\n      fetchSize: 1000\n      where: \"sale_date >= '2024-01-01'\"\n\n# Optional: Add transformations during extract\ntransform:\n  - name: \"clean_sales_data\"\n    sql: |\n      SELECT \n        order_id,\n        customer_id,\n        product_id,\n        amount,\n        sale_date,\n        created_at\n      FROM sales_data\n      WHERE amount > 0\n```\n\n### Step 2: Define the Load\n\nCreate `load/sales.yml`:\n\n```yaml\ntable:\n  pattern: \"sales_data.*.parquet\"\n  metadata:\n    mode: \"FILE\"\n    format: \"PARQUET\"\n    encoding: \"UTF-8\"\n    withHeader: true\n  writeStrategy:\n    type: \"UPSERT_BY_KEY_AND_TIMESTAMP\"\n    timestamp: \"created_at\"\n    key: [\"order_id\"]\n  attributes:\n    - name: \"order_id\"\n      type: \"string\"\n      required: true\n      privacy: \"NONE\"\n    - name: \"customer_id\"\n      type: \"string\"\n      required: true\n      privacy: \"HIDE\"\n    - name: \"product_id\"\n      type: \"string\"\n      required: true\n    - name: \"amount\"\n      type: \"float\"\n      required: true\n    - name: \"sale_date\"\n      type: \"timestamp\"\n      required: true\n    - name: \"created_at\"\n      type: \"timestamp\"\n      required: true\n```\n\n### Step 3: Create Sample Data\n\nCreate a sample CSV file `data/sales_sample.csv`:\n\n```csv\norder_id,customer_id,product_id,amount,sale_date,created_at\nORD001,CUST001,PROD001,150.50,2024-01-15,2024-01-15 10:30:00\nORD002,CUST002,PROD002,299.99,2024-01-16,2024-01-16 14:20:00\nORD003,CUST001,PROD003,75.25,2024-01-17,2024-01-17 09:15:00\n```"
        },
        {
          "id": 5,
          "label": "Deploy and monitor",
          "content": "Now let's deploy your pipeline and monitor its execution.\n\n### Deploy the Pipeline\n\nRun the following commands to execute your pipeline:\n\n```bash\n# Extract data from source\nstarlake extract --config extract/snowflake-sales.yml\n\n# Load data into Snowflake\nstarlake load --config load/sales.yml\n\n# Or run both in sequence\nstarlake run --extract-config extract/snowflake-sales.yml --load-config load/sales.yml\n```\n\n### Monitor Execution\n\n#### 1. Check Starlake Logs\n\n```bash\n# View recent logs\ntail -f logs/starlake.log\n\n# Check execution status\nstarlake status --job-id <job_id>\n```\n\n#### 2. Monitor Snowflake Query History\n\nIn Snowflake worksheet:\n\n```sql\n-- View recent queries\nSELECT \n  query_id,\n  query_text,\n  start_time,\n  end_time,\n  total_elapsed_time,\n  status\nFROM table(information_schema.query_history())\nWHERE start_time >= dateadd(hour, -1, current_timestamp())\nORDER BY start_time DESC;\n\n-- Check warehouse usage\nSELECT \n  warehouse_name,\n  credits_used,\n  bytes_scanned,\n  percentage_scanned_from_cache\nFROM table(information_schema.warehouse_metering_history(\n  date_range_start=>dateadd(hour, -1, current_timestamp()),\n  date_range_end=>current_timestamp()\n));\n```\n\n#### 3. Verify Data in Snowflake\n\n```sql\n-- Check loaded data\nSELECT COUNT(*) as total_rows FROM starlake_db.starlake_schema.sales_data;\n\n-- Sample data\nSELECT * FROM starlake_db.starlake_schema.sales_data LIMIT 10;\n\n-- Data quality check\nSELECT \n  COUNT(*) as total_orders,\n  SUM(amount) as total_amount,\n  MIN(sale_date) as earliest_sale,\n  MAX(sale_date) as latest_sale\nFROM starlake_db.starlake_schema.sales_data;\n```\n\n### Next Steps\n\n- Set up scheduling with cron or Airflow\n- Configure alerts and notifications\n- Scale your pipelines as needed\n- Implement data quality checks"
        },
        {
          "id": 6,
          "label": "Next steps",
          "content": "Congratulations! You've successfully set up Starlake with Snowflake.\n\n### What's Next?\n\n#### Advanced Configuration\n\n- **Data Quality**: Implement data validation rules\n- **Incremental Loading**: Set up efficient incremental updates\n- **Partitioning**: Optimize performance with table partitioning\n- **Clustering**: Improve query performance with clustering keys\n\n#### Scheduling and Orchestration\n\n```bash\n# Set up cron job for daily execution\ncrontab -e\n\n# Add this line for daily execution at 2 AM\n0 2 * * * /path/to/starlake run --extract-config extract/snowflake-sales.yml --load-config load/sales.yml\n```\n\n#### Monitoring and Alerting\n\n- Set up Snowflake alerts for warehouse usage\n- Configure Starlake notifications\n- Monitor data quality metrics\n- Track pipeline performance\n\n#### Security Enhancements\n\n- Implement row-level security (RLS)\n- Set up column-level security\n- Use Snowflake's data masking features\n- Configure network policies\n\n### Resources\n\n- [Starlake Documentation](https://docs.starlake.ai)\n- [Snowflake Documentation](https://docs.snowflake.com)\n- [Snowflake Best Practices](https://docs.snowflake.com/en/user-guide/best-practices-overview)\n- [Community Support](https://github.com/starlake-ai/starlake)\n\n### Need Help?\n\nIf you encounter any issues:\n1. Check the troubleshooting guide\n2. Review the logs for error messages\n3. Verify Snowflake permissions\n4. Reach out to the community\n\n### Example Advanced Configuration\n\n```yaml\n# Advanced Snowflake configuration\nconnections:\n  snowflake:\n    type: \"snowflake\"\n    account: \"${SNOWFLAKE_ACCOUNT}\"\n    user: \"${SNOWFLAKE_USER}\"\n    password: \"${SNOWFLAKE_PASSWORD}\"\n    database: \"${SNOWFLAKE_DATABASE}\"\n    schema: \"${SNOWFLAKE_SCHEMA}\"\n    warehouse: \"${SNOWFLAKE_WAREHOUSE}\"\n    role: \"${SNOWFLAKE_ROLE}\"\n    options:\n      client_session_keep_alive: true\n      timezone: \"UTC\"\n      session_parameters:\n        USE_CACHED_RESULT: true\n        STATEMENT_TIMEOUT_IN_SECONDS: 3600\n```\n\n</div>"
        }
      ]
    }
  ],
  "categories": [
    "AWS",
    "Azure",
    "BigQuery",
    "Databricks",
    "GCP",
    "Snowflake"
  ]
}