---
title: "Clustering and Partitioning in Starlake â€” BigQuery, Databricks, Spark"
description: "Configure clustering, partitioning, materialized views and expiration in Starlake for BigQuery and Spark/Databricks. YAML-based sink configuration for query optimization."
keywords: [starlake, clustering, partitioning, BigQuery, Databricks, sink, query optimization, materialized view, Spark]
---

import Head from '@docusaurus/Head';

<Head>
  <script type="application/ld+json">
    {JSON.stringify({
      "@context": "https://schema.org",
      "@type": "FAQPage",
      "mainEntity": [
        {
          "@type": "Question",
          "name": "How do I configure clustering in Starlake for BigQuery?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "Add a sink.clustering section in the table YAML file with the list of clustering columns. BigQuery supports multiple clustering columns."
          }
        },
        {
          "@type": "Question",
          "name": "What is the difference between clustering and partitioning?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "Partitioning physically divides the table into segments based on a column value (e.g. date). Clustering sorts data within each partition by one or more columns, speeding up queries that filter on those columns."
          }
        },
        {
          "@type": "Question",
          "name": "How do I enable a materialized view in Starlake for BigQuery?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "Set sink.materializedView: true in the table configuration. Optionally enable automatic refresh with enableRefresh: true and set the interval via refreshIntervalMs."
          }
        },
        {
          "@type": "Question",
          "name": "Can I partition on multiple columns in BigQuery via Starlake?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "No. BigQuery supports only a single partition field. Starlake respects this constraint: sink.partition.field accepts only one column. However, with Spark (Databricks), multiple partition columns are supported."
          }
        },
        {
          "@type": "Question",
          "name": "What Spark options are available in the sink section?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "The sink.options section allows passing key-value pairs to the Spark writer, for example compression: snappy. You can also define the format (delta, parquet), coalesce, and clustering/partition columns."
          }
        }
      ]
    })}
  </script>
  <script type="application/ld+json">
    {JSON.stringify({
      "@context": "https://schema.org",
      "@type": "HowTo",
      "name": "How to configure partitioning and clustering in Starlake",
      "description": "Set up partitioning and clustering in the table YAML sink section for BigQuery or Spark/Databricks.",
      "step": [
        {
          "@type": "HowToStep",
          "name": "Open the table YAML file",
          "text": "Edit metadata/load/<domain>/<table>.sl.yml."
        },
        {
          "@type": "HowToStep",
          "name": "Add a sink section",
          "text": "Under the table key, add a sink section with clustering and partition properties."
        },
        {
          "@type": "HowToStep",
          "name": "Define clustering columns",
          "text": "List one or more columns under sink.clustering. Both BigQuery and Spark support multiple clustering columns."
        },
        {
          "@type": "HowToStep",
          "name": "Define a partition field",
          "text": "Set sink.partition.field (BigQuery, single column) or sink.partition (Spark, multiple columns)."
        },
        {
          "@type": "HowToStep",
          "name": "Run the load",
          "text": "Execute starlake load. The target table will be created or updated with the configured partitioning and clustering."
        }
      ]
    })}
  </script>
</Head>

# Clustering and Partitioning

Starlake lets you configure partitioning and clustering directly in the table YAML definition to optimize query performance on large datasets. Both BigQuery and Spark/Databricks are supported, with additional options for materialized views, table expiration and Spark writer settings. This page details every sink property available for each platform.

## How to configure partitioning and clustering

1. **Open the table YAML file** -- Edit `metadata/load/<domain>/<table>.sl.yml`.
2. **Add a `sink` section** -- Under the `table` key, add a `sink` section with clustering and partition properties.
3. **Define clustering columns** -- List one or more columns under `sink.clustering`. Both BigQuery and Spark support multiple clustering columns.
4. **Define a partition field** -- Set `sink.partition.field` (BigQuery, single column) or `sink.partition` (Spark, multiple columns).
5. **Run the load** -- Execute `starlake load`. The target table will be created or updated with the configured partitioning and clustering.

Some datawarehouses are designed to store data in a way that makes it easy to perform clustering and partitioning.
Clustering and partitioning are two techniques that can be used to improve the performance of queries on large datasets.
Clustering involves storing similar rows of data together, while partitioning involves splitting a table into smaller chunks of data.


`BigQuery`and `Databricks` are two datawarehouses that support user-defined clustering and partitioning.

To specify clustering and partitioning in BigQuery, you define the following properties in the `sink` section of the table configuration file.


import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';


<Tabs>

<TabItem value="bigquery" label="BigQuery">

```yaml title="metadata/load/<domain>/<table>.sl.yml"
table:
    ...
    sink:
      clustering:
        - <field1>
        - <field2>
      partition:
        field: <field> # only one field is allowed
      requirePartitionFilter: <true|false> # default is false
      days: <number> # expiration in days. default is "never expire"
      materializedView: <true|false> # Sink data as a materialized view. default is false
      enableRefresh: <true|false> # only if materializedView is true. Default is false
      refreshIntervalMs: <number> # only if enable refresh is true.
```

</TabItem>

<TabItem value="spark" label="Spark">

```yaml title="metadata/load/<domain>/<table>.sl.yml"
table:
    ...
    sink:
        format: <format> # For example, "delta" or "parquet".
        clustering:
            - <field1>
            - <field2>
        partition:
            - <field1>
            - <field2>
        coalesce: <true|false> # default is false
        options:
            <key>: <value> # Options to pass to the Spark writer. For example, "compression" -> "snappy"
```

</TabItem>
</Tabs>
