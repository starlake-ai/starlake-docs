---
title: "Load CSV and DSV Files"
description: "Load CSV and delimiter-separated files into your data warehouse with Starlake. Configure parsing options, infer schemas and validate data."
keywords: [starlake, CSV, DSV, delimiter, load, parsing, infer schema, data validation]
---

import Head from '@docusaurus/Head';

<Head>
  <script type="application/ld+json">
    {JSON.stringify({
      "@context": "https://schema.org",
      "@type": "FAQPage",
      "mainEntity": [
        {
          "@type": "Question",
          "name": "What delimiters does Starlake support for CSV files?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "Starlake supports any single-character delimiter through the 'separator' property in the metadata configuration. Common delimiters include semicolons (;), commas (,), pipes (|), and tabs. The default separator is semicolon (;)."
          }
        },
        {
          "@type": "Question",
          "name": "What encoding does Starlake use for CSV files?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "The default encoding is UTF-8. You can change it by setting the 'encoding' property in the metadata section of the table configuration file. Any encoding supported by Java can be used."
          }
        },
        {
          "@type": "Question",
          "name": "Does Starlake require CSV files to have headers?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "No, headers are optional. By default, Starlake expects headers (withHeader: true). If your file has no header, set 'withHeader' to false and the attribute names in the schema will refer to column indices instead."
          }
        },
        {
          "@type": "Question",
          "name": "How does Starlake infer the schema of a CSV file?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "Use the 'starlake infer-schema' command to automatically detect the schema from a sample data file. This generates a YAML configuration file with detected column names, types, and parsing options that you can then customize."
          }
        }
      ]
    })}
  </script>
</Head>

<Head>
  <script type="application/ld+json">
    {JSON.stringify({
      "@context": "https://schema.org",
      "@type": "HowTo",
      "name": "How to Load CSV and DSV Files with Starlake",
      "description": "Load CSV and delimiter-separated files into your data warehouse using Starlake by inferring the schema and configuring parsing options.",
      "step": [
        {
          "@type": "HowToStep",
          "position": 1,
          "name": "Infer the schema",
          "text": "Run 'starlake infer-schema --input-path incoming/<domain>' to auto-detect column names, types, and separator from a sample file."
        },
        {
          "@type": "HowToStep",
          "position": 2,
          "name": "Configure parsing options",
          "text": "Edit the generated <table>.sl.yml file to set format, separator, encoding, withHeader, quote, and escape properties."
        },
        {
          "@type": "HowToStep",
          "position": 3,
          "name": "Define attribute validation",
          "text": "Review the attributes section and set column types (int, double, date, etc.), required constraints, and trim strategies."
        },
        {
          "@type": "HowToStep",
          "position": 4,
          "name": "Run the load command",
          "text": "Execute 'starlake load' to parse, validate, and load CSV data into the target warehouse table."
        }
      ]
    })}
  </script>
</Head>

# Load DSV files

Starlake loads CSV and delimiter-separated value (DSV) files into your data warehouse with automatic schema inference and data validation. It supports configurable separators, encodings, and write strategies. Rejected records are stored separately in an audit table for review.


## File load configuration

Most of the time, we won't need to define the table configuration, as the `infer-schema` command will be able to infer the table configuration from the file itself using the `infer-schema command.

Sometimes, we still need to update some properties of the load configuration for the table or add new properties not present in the source file (ACL, comments  ...) or apply transformations.

<br/>

Describing the file you load involves defining :
- the file pattern that is mapped to this schema,
- the parsing parameters and the materialization strategy (APPEND, OVERWRITE, OVERWRITE_BY_PARTITION, UPSERT_BY_KEY, UPSERT_BY_KEY_AND_TIMESTAMP, SCD2 ...)
- the file format as a list of attributes.

## Infer schema

The very first step is to infer the schema of the file from a data file as described in the [autoload section](autoload#how-autoload-detects-the-format-of-the-files) before you start customizing your configuration. This is done using the `infer-schema` command. This will bootstrap the configuration file for the table.

```bash

starlake infer-schema --input-path incoming/starbake

```


## Parsing CSV

CSV Parsing options are defined in the `metadata` section of the configuration file.

<br/>

```yaml title="metadata/load/<domain>/<table>.sl.yml - parsing section"
table:
  pattern: "order_line.*.csv"
  metadata:
    format: "DSV"
    withHeader: true
    separator: ";"
    ...
  attributes:
    - ...
```

---

|Attribute| Description|
|---|---|
|encoding| the encoding of the file. Default is `UTF-8`|
|withHeader| if the file contains a header. Default is `true`|
|separator| the separator used in the file. Default is `;`|
|quote| the quote character used in the file. Default is `"`|
|escape| the escape character used in the file. Default is `\`|
|filter| a condition / regex to filter lines. All lines matching the condition will be loaded. All others will be discarded. Default is None|
|ack| load file only if a file with the same name and with this extension exist in the same directory.|
|options| a map of options to pass to the parser. Available options are defined in the [Apache Spark CSV documentation](https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option)|
|emptyIsNull| if empty column values should be considered as null. Default is `false`|
|fillWithDefaultValue| if the null value should be filled with the default value of the column. Default is `false`|

## Attributes validation

```yaml title="metadata/load/<domain>/<table>.sl.yml - validation section"
table:
  pattern: "order_line.*.csv"
  metadata:
    - ...
  attributes:
  - name: "order_id"
    type: "int"
  - name: "product_id"
    type: "int"
  - name: "quantity"
    type: "int"
  - name: "sale_price"
    type: "double"
```
<br/>
The attributes properties are defined in the `attributes` section of the configuration file.

|Attribute|Description|Default|
|---|---|---|
|name| the name of the column in the file header. If no header is present, this refers to the column index in the file. This is the only required property. this will also refer to the column name in the table except if the `rename` property is set.| |
|type| the type of the column. Default is `string`. Available types are `string`, `int`, `long`, `double`, `float`, `boolean`, any flavor of `date` or `timestamp` as defined in the `metadata/types/default.sl.yml` file. Also note that you can add your own types in the `metadata/types` directory.| |
|required| if the column is required to have a non null value (see `nullValue` in the `metadata.options` section).| false|
|privacy| a transformation to apply to the column.| |
|transform| a transformation to apply to the column.| |
|comment| a comment for the column.| |
|rename| the name of the column in the database.|`name`|
|default| the default value of the column.| |
|trim| the trim strategy for the column.  Available trim strategies are `None`, `Left`, `Right`, `Both`| |


To add / replace or ignore some attributes, check the [Transform on load](transform) section.

## Complete configuration

The name property of the column is the only required field. The type property is optional and will be set to `string` if not provided.
The configuration file below describes how the `order_line` files should be loaded.


<br/>

```yaml title="metadata/load/<domain>/<table>.sl.yml"
table:
  pattern: "order_line.*.csv"
  metadata:
    format: "DSV"
    withHeader: true
    separator: ";"
    writeStrategy:
      type: "APPEND"
  attributes:
  - name: "order_id"
    type: "int"
  - name: "product_id"
    type: "int"
  - name: "quantity"
    type: "int"
  - name: "sale_price"
    type: "double"
```

## How to Load CSV/DSV Files with Starlake

1. **Infer the schema** -- Run `starlake infer-schema --input-path incoming/<domain>` to auto-detect column names, types, and separator from a sample file.
2. **Configure parsing options** -- Edit the generated `<table>.sl.yml` file to set `format`, `separator`, `encoding`, `withHeader`, `quote`, and `escape` properties.
3. **Define attribute validation** -- Review the `attributes` section and set column types (`int`, `double`, `date`, etc.), required constraints, and trim strategies.
4. **Run the load command** -- Execute `starlake load` to parse, validate, and load CSV data into the target warehouse table.

## Frequently Asked Questions

### What delimiters does Starlake support for CSV files?
Starlake supports any single-character delimiter through the `separator` property in the metadata configuration. Common delimiters include semicolons (`;`), commas (`,`), pipes (`|`), and tabs. The default separator is semicolon (`;`).

### What encoding does Starlake use for CSV files?
The default encoding is UTF-8. You can change it by setting the `encoding` property in the metadata section of the table configuration file. Any encoding supported by Java can be used.

### Does Starlake require CSV files to have headers?
No, headers are optional. By default, Starlake expects headers (`withHeader: true`). If your file has no header, set `withHeader` to false and the attribute names in the schema will refer to column indices instead.

### How does Starlake infer the schema of a CSV file?
Use the `starlake infer-schema` command to automatically detect the schema from a sample data file. This generates a YAML configuration file with detected column names, types, and parsing options that you can then customize.
