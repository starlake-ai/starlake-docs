---
title: "Orchestrate Starlake Transforms: Automatic DAG Generation for Airflow, Dagster, Snowflake Tasks"
description: "Generate execution DAGs for Starlake SQL and Python transforms. Automatic dependency resolution from SQL analysis. Supports Airflow, Dagster, and Snowflake Tasks. Includes load and transform jobs in correct order."
keywords: [starlake transform orchestration, sql transform dag, airflow transform, dagster transform, dag-generate, dependency resolution, snowflake tasks]
---

import Head from '@docusaurus/Head';

<Head>
  <script type="application/ld+json">
    {JSON.stringify({
      "@context": "https://schema.org",
      "@type": "FAQPage",
      "mainEntity": [
        {
          "@type": "Question",
          "name": "How does Starlake resolve dependencies between SQL transforms?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "Starlake analyzes the FROM and JOIN clauses of each SQL file to build a dependency graph. Upstream tables are always executed before downstream ones."
          }
        },
        {
          "@type": "Question",
          "name": "Which orchestrators are supported by Starlake?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "Starlake generates DAGs for Airflow, Dagster, and Snowflake Tasks. Other orchestrators can be integrated via CLI commands."
          }
        },
        {
          "@type": "Question",
          "name": "What does the starlake dag-generate command do?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "It analyzes all SQL and Python transforms in the project and generates the corresponding DAG files for the configured orchestrator (Airflow, Dagster, or Snowflake Tasks)."
          }
        },
        {
          "@type": "Question",
          "name": "Do generated DAGs include load jobs as well?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "Yes. The generated DAGs include both load and transform jobs in the correct execution order."
          }
        },
        {
          "@type": "Question",
          "name": "Do you need to regenerate DAGs after every SQL change?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "Yes. After any change that modifies dependencies (adding a table, modifying a JOIN), re-run starlake dag-generate to update the DAGs."
          }
        },
        {
          "@type": "Question",
          "name": "Can you customize the generated DAGs?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "Yes. See the DAG customization page to configure scheduling, retries, parallelism, and other orchestrator-specific parameters."
          }
        }
      ]
    })}
  </script>
</Head>

<Head>
  <script type="application/ld+json">
    {JSON.stringify({
      "@context": "https://schema.org",
      "@type": "HowTo",
      "name": "How to Generate Transform DAGs in Starlake",
      "description": "Use Starlake to analyze SQL transform dependencies and generate execution DAGs for Airflow, Dagster, or Snowflake Tasks.",
      "step": [
        {
          "@type": "HowToStep",
          "name": "Write your SQL transforms",
          "text": "Place SQL files in metadata/transform/<domain>/. Starlake will parse FROM and JOIN clauses to detect dependencies automatically.",
          "position": 1
        },
        {
          "@type": "HowToStep",
          "name": "Configure your orchestrator",
          "text": "Set the target orchestrator (Airflow, Dagster, or Snowflake Tasks) and scheduling parameters in the DAG configuration YAML.",
          "position": 2
        },
        {
          "@type": "HowToStep",
          "name": "Run starlake dag-generate",
          "text": "Execute 'starlake dag-generate' to analyze all transforms and produce DAG files with the correct execution order for both load and transform jobs.",
          "position": 3
        },
        {
          "@type": "HowToStep",
          "name": "Deploy the generated DAGs",
          "text": "Copy the generated DAG files to your orchestrator's DAG directory (e.g., Airflow's dags/ folder) and trigger or schedule the workflow.",
          "position": 4
        },
        {
          "@type": "HowToStep",
          "name": "Regenerate after dependency changes",
          "text": "After adding or modifying SQL transforms that change table references, re-run starlake dag-generate to keep DAGs in sync.",
          "position": 5
        }
      ]
    })}
  </script>
</Head>

# Orchestrate Transform Jobs

Starlake analyzes the `FROM` and `JOIN` clauses in your SQL transforms to build a dependency graph and generates ready-to-use DAGs for Airflow, Dagster, or Snowflake Tasks. The generated DAGs include both load and transform jobs in the correct execution order. After any SQL change that modifies table references, re-run `starlake dag-generate` to keep the DAGs in sync.

## How to Generate Transform DAGs

1. **Write your SQL transforms** -- Place SQL files in `metadata/transform/<domain>/`. Starlake parses `FROM` and `JOIN` clauses to detect dependencies automatically.
2. **Configure your orchestrator** -- Set the target orchestrator (Airflow, Dagster, or Snowflake Tasks) and scheduling parameters in the DAG configuration YAML.
3. **Run `starlake dag-generate`** -- This analyzes all transforms and produces DAG files with the correct execution order for both load and transform jobs.
4. **Deploy the generated DAGs** -- Copy the generated DAG files to your orchestrator's DAG directory (e.g., Airflow's `dags/` folder).
5. **Regenerate after dependency changes** -- After adding or modifying SQL transforms that change table references, re-run `starlake dag-generate`.

---

Starlake automatically resolves dependencies between your SQL transforms and generates execution DAGs for your orchestrator.

## How Dependency Resolution Works

Starlake analyzes the `FROM` and `JOIN` clauses in your SQL transforms to build a dependency graph. This graph determines the execution order so that upstream tables are always computed before downstream ones.

## Generate DAGs for Transform

```bash
starlake dag-generate
```

The generated DAGs include both load and transform jobs in the correct execution order.

## Next Steps

- See the [Orchestration Tutorial](/docs/guides/orchestrate/tutorial) for a complete walkthrough.
- See [Customizing DAG Generation](/docs/guides/orchestrate/customization) for per-orchestrator configuration.

## Frequently Asked Questions

### How does Starlake resolve dependencies between SQL transforms?
Starlake analyzes the `FROM` and `JOIN` clauses of each SQL file to build a dependency graph. Upstream tables are always executed before downstream ones.

### Which orchestrators are supported by Starlake?
Starlake generates DAGs for Airflow, Dagster, and Snowflake Tasks. Other orchestrators can be integrated via CLI commands.

### What does the starlake dag-generate command do?
It analyzes all SQL and Python transforms in the project and generates the corresponding DAG files for the configured orchestrator (Airflow, Dagster, or Snowflake Tasks).

### Do generated DAGs include load jobs as well?
Yes. The generated DAGs include both load and transform jobs in the correct execution order.

### Do you need to regenerate DAGs after every SQL change?
Yes. After any change that modifies dependencies (adding a table, modifying a JOIN), re-run `starlake dag-generate` to update the DAGs.

### Can you customize the generated DAGs?
Yes. See the DAG customization page to configure scheduling, retries, parallelism, and other orchestrator-specific parameters.
