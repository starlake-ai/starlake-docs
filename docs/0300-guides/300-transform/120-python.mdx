---
title: "Python Transforms in Starlake: PySpark DataFrame Pipelines"
description: "Run Python transforms in Starlake with PySpark. Pass arguments via --options, return a DataFrame as SL_THIS temporary view, and materialize results to any table. Same YAML config as SQL transforms."
keywords: [starlake, python transforms, dataframe, python pipeline, data transformation, spark, pyspark, SL_THIS]
---

import Head from '@docusaurus/Head';

<Head>
  <script type="application/ld+json">
    {JSON.stringify({
      "@context": "https://schema.org",
      "@type": "FAQPage",
      "mainEntity": [
        {
          "@type": "Question",
          "name": "When should you use a Python transform instead of SQL in Starlake?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "Python transforms are useful for complex operations that are difficult to express in SQL, such as machine learning, text transformations, or API calls. They require a Spark or Databricks runtime."
          }
        },
        {
          "@type": "Question",
          "name": "How do you pass parameters to a Python script in Starlake?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "Use the --options key1=value1,key2=value2 flag in the starlake transform command. The parameters are passed as command-line arguments to the Python script."
          }
        },
        {
          "@type": "Question",
          "name": "What is SL_THIS in a Starlake Python transform?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "SL_THIS is the name of the temporary view that the Python script must create before returning. Starlake reads this view to materialize the result into the target table."
          }
        },
        {
          "@type": "Question",
          "name": "Do Starlake Python transforms work with DuckDB?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "No. Python transforms require a Spark runtime (standalone, Databricks, or EMR). For DuckDB, use SQL transforms."
          }
        },
        {
          "@type": "Question",
          "name": "Can you use the same YAML file for a Python and SQL transform?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "Yes. The YAML format is identical. Only the source file changes: .sql for SQL, .py for Python."
          }
        },
        {
          "@type": "Question",
          "name": "Does the returned DataFrame need a specific schema?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "The DataFrame must match the target table schema if it exists. Otherwise, Starlake infers the schema. The DataFrame must be registered as the SL_THIS temporary view."
          }
        }
      ]
    })}
  </script>
</Head>

<Head>
  <script type="application/ld+json">
    {JSON.stringify({
      "@context": "https://schema.org",
      "@type": "HowTo",
      "name": "How to Write a Python Transform in Starlake",
      "description": "Create a PySpark-based Python transform in Starlake that reads data, processes it, and registers the result as the SL_THIS temporary view for materialization.",
      "step": [
        {
          "@type": "HowToStep",
          "name": "Create a Python file in the transform directory",
          "text": "Add a .py file in the metadata/transform/<domain>/ directory. The file name determines the task name.",
          "position": 1
        },
        {
          "@type": "HowToStep",
          "name": "Write the PySpark logic",
          "text": "Use SparkSession to read source tables, apply transformations, and produce a result DataFrame.",
          "position": 2
        },
        {
          "@type": "HowToStep",
          "name": "Register the result as SL_THIS",
          "text": "Call df.createOrReplaceTempView('SL_THIS') on your result DataFrame so Starlake can materialize it to the target table.",
          "position": 3
        },
        {
          "@type": "HowToStep",
          "name": "Add an optional YAML configuration",
          "text": "Create a .sl.yml file with the same name to set write strategy, partitioning, or expectations -- the format is identical to SQL transforms.",
          "position": 4
        },
        {
          "@type": "HowToStep",
          "name": "Run the transform",
          "text": "Execute 'starlake transform --name <domain>.<task>' with optional '--options key1=value1,key2=value2' to pass parameters.",
          "position": 5
        }
      ]
    })}
  </script>
</Head>

# Python Transforms

Starlake supports Python transforms for operations that go beyond what SQL can express, such as machine learning inference, text processing, or external API calls. Python transforms require a Spark runtime (standalone, Databricks, or EMR) and follow the same YAML configuration format as SQL transforms. The script must register its output DataFrame as the `SL_THIS` temporary view so Starlake can materialize the result.

## How to Write a Python Transform

1. **Create a Python file** -- Add a `.py` file in the `metadata/transform/<domain>/` directory. The file name determines the task name.
2. **Write the PySpark logic** -- Use `SparkSession` to read source tables, apply transformations, and produce a result DataFrame.
3. **Register the result as SL_THIS** -- Call `df.createOrReplaceTempView("SL_THIS")` on your result DataFrame.
4. **Add an optional YAML configuration** -- Create a `.sl.yml` file with the same name to set write strategy, partitioning, or expectations.
5. **Run the transform** -- Execute `starlake transform --name <domain>.<task>` with optional `--options key1=value1,key2=value2` to pass parameters.

---

In addition to SQL transforms, you may run Python transforms in your pipeline.
Python transforms are defined by a Python function that takes a DataFrame as input and returns a DataFrame as output.
The function is then registered as a transform in the pipeline.


Exactly like the SQL transform, you can define a python transform by creating a python file and adding it
to the `metadata/transform/<domain>` directory.

You can also define a YAML configuration file for the python script using the exact same format as the SQL transform.

Arguments specified in the command line through the `--options` flag will be passed to the python function as keyword arguments:

```bash

$ starlake transform --name <domain>.<transform_name> --options key1=value1,key2=value2

```

will be passed as keyword arguments to the python function:

```bash

<transform-name>.py --key1 value1 --key2 value2

```

The dataframe returned by the python function will be saved as the output table of the transform.

:::note

Before returning the dataframe, the function must create a temporary view with the name `SL_THIS` so that the dataframe can be saved as a table.

:::

```python

import sys
from random import random
from operator import add

from pyspark.sql import SparkSession

if __name__ == "__main__":
    """
        Usage: pi [partitions]
    """
    spark = SparkSession \
        .builder \
        .getOrCreate()

    partitions = 2
    n = 100000 * partitions

    def f(_: int) -> float:
        x = random() * 2 - 1
        y = random() * 2 - 1
        return 1 if x ** 2 + y ** 2 <= 1 else 0

    count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)
    result = "Pi is roughly %f" % (4.0 * count / n)
    df = spark.createDataFrame([[result]])
    df.createOrReplaceTempView("SL_THIS")

```

## Frequently Asked Questions

### When should you use a Python transform instead of SQL in Starlake?
Python transforms are useful for complex operations that are difficult to express in SQL, such as machine learning, text transformations, or API calls. They require a Spark or Databricks runtime.

### How do you pass parameters to a Python script in Starlake?
Use the `--options key1=value1,key2=value2` flag in the `starlake transform` command. The parameters are passed as command-line arguments to the Python script.

### What is SL_THIS in a Starlake Python transform?
`SL_THIS` is the name of the temporary view that the Python script must create before returning. Starlake reads this view to materialize the result into the target table.

### Do Starlake Python transforms work with DuckDB?
No. Python transforms require a Spark runtime (standalone, Databricks, or EMR). For DuckDB, use SQL transforms.

### Can you use the same YAML file for a Python and SQL transform?
Yes. The YAML format is identical. Only the source file changes: `.sql` for SQL, `.py` for Python.

### Does the returned DataFrame need a specific schema?
The DataFrame must match the target table schema if it exists. Otherwise, Starlake infers the schema. The DataFrame must be registered as the `SL_THIS` temporary view.
