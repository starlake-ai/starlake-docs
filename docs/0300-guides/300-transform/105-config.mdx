---
title: "Starlake Transform YAML Configuration: Write Strategies, Partitioning, ACL"
description: "Complete YAML configuration reference for Starlake transforms. Set write strategies (overwrite, upsert, append), table partitioning, clustering, row-level security, expectations, and cross-database writes."
keywords: [starlake, transform configuration, write strategy, partitioning, yaml config, data pipeline, clustering, row-level security, expectations]
---

import Head from '@docusaurus/Head';

<Head>
  <script type="application/ld+json">
    {JSON.stringify({
      "@context": "https://schema.org",
      "@type": "FAQPage",
      "mainEntity": [
        {
          "@type": "Question",
          "name": "What is the default write strategy for a Starlake transform?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "By default, the result of the SELECT statement is appended to the target table. You can change this with the writeStrategy.type property in the YAML file (overwrite, upsert, etc.)."
          }
        },
        {
          "@type": "Question",
          "name": "How do you partition a table created by a Starlake transform?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "Add a sink.partition section in the transform YAML file listing the partition columns. Clustering is configured similarly with sink.clustering."
          }
        },
        {
          "@type": "Question",
          "name": "Can Starlake transform read from one database and write to another?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "Yes. Use connectionRef at the task level to change the source database, and sink.connectionRef to change the target database."
          }
        },
        {
          "@type": "Question",
          "name": "What are expectations in a Starlake transform?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "Expectations are data quality assertions evaluated after the transform runs. If failOnError is set to true, the job fails when the assertion is not satisfied."
          }
        },
        {
          "@type": "Question",
          "name": "What is the difference between domain, schema, catalog and dataset in Starlake?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "Starlake uses the term 'domain' as an abstraction. Depending on the data warehouse, it maps to a schema (PostgreSQL, Snowflake), a dataset (BigQuery), or a catalog (Databricks)."
          }
        }
      ]
    })}
  </script>
</Head>

<Head>
  <script type="application/ld+json">
    {JSON.stringify({
      "@context": "https://schema.org",
      "@type": "HowTo",
      "name": "How to Configure a SQL Transform in Starlake",
      "description": "Set up a YAML configuration file for a Starlake SQL transform with write strategy, partitioning, clustering, access control, and expectations.",
      "step": [
        {
          "@type": "HowToStep",
          "name": "Create the YAML configuration file",
          "text": "Create a file named <table>.sl.yml in the metadata/transform/<domain>/ directory next to your SQL file.",
          "position": 1
        },
        {
          "@type": "HowToStep",
          "name": "Set the write strategy",
          "text": "Add a writeStrategy.type property (overwrite, append, upsert, etc.) to control how results are materialized into the target table.",
          "position": 2
        },
        {
          "@type": "HowToStep",
          "name": "Configure partitioning and clustering",
          "text": "Add sink.partition and sink.clustering arrays listing the columns to use for table partitioning and clustering.",
          "position": 3
        },
        {
          "@type": "HowToStep",
          "name": "Define access control rules",
          "text": "Set acl for table-level grants, rls for row-level security predicates, and attributesDesc for column-level access policies.",
          "position": 4
        },
        {
          "@type": "HowToStep",
          "name": "Add data quality expectations",
          "text": "Use the expectations array to define assertions that are evaluated after the transform runs. Set failOnError: true to abort on failure.",
          "position": 5
        }
      ]
    })}
  </script>
</Head>

# Customize the Transform

Every Starlake SQL transform can be fine-tuned through a companion YAML configuration file placed next to the SQL file. This configuration controls how results are written (append, overwrite, upsert), how the target table is partitioned and clustered, and who can access the data. You can also add data quality expectations and route output to a different database.

## How to Configure a SQL Transform

1. **Create the YAML configuration file** -- Create a file named `<table>.sl.yml` in the `metadata/transform/<domain>/` directory next to your SQL file.
2. **Set the write strategy** -- Add a `writeStrategy.type` property (`overwrite`, `append`, `upsert`, etc.) to control how results are materialized into the target table.
3. **Configure partitioning and clustering** -- Add `sink.partition` and `sink.clustering` arrays listing the columns to use.
4. **Define access control rules** -- Set `acl` for table-level grants, `rls` for row-level security predicates, and `attributesDesc` for column-level access policies.
5. **Add data quality expectations** -- Use the `expectations` array to define assertions evaluated after the transform runs. Set `failOnError: true` to abort on failure.

---

Add write strategies, partitioning, access control, and more to your transform tasks.

:::note
Datawarehouses are organized around schemas where tables are grouped.
Depending on the database, a database `schema` can be called `schema` or `catalog` or `dataset`.
In starlake, we use the term `domain` to designate  a `schema`, `catalog` or `dataset`.
:::

Exactly like in the `load` section, the `transform` section is organized around the `domain` and `table` concepts.

Transforming data consists in applying a series of operations to the data to make it ready for analysis.
The operations can be as simple as renaming a column or as complex as joining multiple tables together.

The transform takes SQL queries or python scripts as input and outputs to an existing or new table using a write strategy.

The way the materialization is done is defined in a YAML file next to the SQL or Python file.
The YAML file and the SQL file are named, by default, after the table they are transforming .

## Conventions

The transform directory looks like this:
```plaintext title="transform directory structure"
metadata/transform
└── <domain>
    ├── _config.sl.yml
    ├── <table>.sl.yml
    └── <table>.sql
```
<br/>

The SQL file contains only the SELECT statement that contain the transformations to apply before storing on the target table.

And the YAML file contains the configuration of the transformation. The sections below describe the content of the YAML file.

By default, your transforms tasks will be named after the table they are transforming. The result of the SQL file will be written to the table with the same name as the SQL file (`<table>`in this case) in the schema with the same name as
the folder name (`<domain>` in this case).

You can change this by specifying a different name in the `name` field of the transform configuration file:

```yaml title="metadata/transform/<domain>/custom.sl.yml"
task:
  domain: mydomain # will write to the mydomain schema whatever the name of the <domain> folder
  table: mytable # will write to the mytable table whatever the name of the <table> in the filename
  name: custom # this task name. Will be used to name the task when running from the CLI
```


## Write strategies

If not specified, the result of the `SELECT`statement contained in the SQL file will be appended to the `<table>` located in `<domain>`.
You may use any of these strategies. Below is an example of using the `overwrite` strategy.

```yaml title="metadata/transform/<domain>/<table>.sl.yml"
task:
  writeStrategy:
    type: overwrite ## or any of the strategies defined [here](../load/write-strategies)
```

Transform strategies work exactly like the load [write strategies](../load/write-strategies).


## Clustering and Partitioning

You can specify the clustering and partitioning of the table in the transform configuration file.

```yaml title="metadata/transform/<domain>/<table>.sl.yml"
task:
  ...
  sink:
    partition:
      - column1
      - column2
    clustering:
      - column3
      - column4
```

Detailed explanation of the clustering and partitioning can be found [here](../load/sink).


##  Access control

You can specify the access control of the table in the transform configuration file similar to the load configuration file [access control section](../load/security).

```yaml title="metadata/transform/<domain>/<table>.sl.yml"

task:
    ...
    acl:
        - role: SELECT
          grants:
            - user@starlake.ai
            - group
    rls:
      - name: "USA only"
        predicate: "country = 'USA'"
        grants:
          - "group:mygroup"
    attributesDesc:
        - name: "code0"
          accessPolicy: <column_access_policy>

    ...
```


## Running a transform against a different database

By default, the transform runs against the database referenced in the connectionRef in the `metadata/application.sl.yml` file.
You can override this by specifying the `connectionRef` in the transform configuration file.

```yaml title="metadata/transform/<domain>/<table>.sl.yml"

task:
    ...
    connectionRef: myConnection
    ...

```

## Writing to a different database

You can even specify a different target database in the sink section of the transform configuration file.

This allows you to transform data from one database and write it to another database.

```yaml title="metadata/transform/<domain>/<table>.sl.yml"

task:
    ...
    sink:
      connectionRef: myOtherConnection
      ...
    ...

```

## Expectations

You can specify the expectations of the table in the transform configuration file.

```yaml title="metadata/transform/<domain>/<table>.sl.yml"
task:
  ...
  expectations:
    - expect: "is_col_value_not_unique('id') => result(0) == 1"
      failOnError: true
```


Detailed explanation of the expectations can be found [here](../load/expectations).
