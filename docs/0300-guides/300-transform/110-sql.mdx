---
title: "SQL Transform Syntax: SELECT, Incremental Models, and Custom SQL in Starlake"
description: "Write SQL transforms in Starlake using standard SELECT statements. Configure incremental models with sl_start_date/sl_end_date, document calculated columns, and use custom MERGE/INSERT with parseSQL: false."
keywords: [starlake, sql transforms, select statement, materialization, incremental model, data pipeline, parseSQL]
---

import Head from '@docusaurus/Head';

<Head>
  <script type="application/ld+json">
    {JSON.stringify({
      "@context": "https://schema.org",
      "@type": "FAQPage",
      "mainEntity": [
        {
          "@type": "Question",
          "name": "Does the SELECT result have to match the target table schema?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "If the target table already exists, the SELECT result must match its schema. Otherwise, Starlake automatically infers the schema from the result."
          }
        },
        {
          "@type": "Question",
          "name": "How do you document calculated columns in a SQL transform?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "Use the attributesDesc property in the transform YAML file to add a name and comment for each calculated column."
          }
        },
        {
          "@type": "Question",
          "name": "Can you write custom MERGE or INSERT instead of a SELECT?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "Yes. Set parseSQL: false in the YAML configuration. Starlake will then execute the SQL as-is without converting it into a write statement."
          }
        },
        {
          "@type": "Question",
          "name": "What are the sl_start_date and sl_end_date variables?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "They are reserved environment variables representing the current time interval. They are automatically passed by the orchestrator (Airflow, Dagster, or Snowflake Tasks) for incremental executions."
          }
        },
        {
          "@type": "Question",
          "name": "What happens if an incremental job fails for a day?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "Starlake integrates with Airflow's Catchup mechanism. Missed intervals are automatically replayed with the correct sl_start_date and sl_end_date values."
          }
        },
        {
          "@type": "Question",
          "name": "Does Starlake support Jinja templating in SQL files?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "No. Starlake SQL files use standard SQL. Environment variables are injected directly without a Jinja layer."
          }
        },
        {
          "@type": "Question",
          "name": "How does schema inference work when the target table does not exist?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "Starlake executes the SELECT and infers the schema (types, column names) from the result. Source column documentation is automatically propagated."
          }
        }
      ]
    })}
  </script>
</Head>

#  SQL Transforms

Starlake SQL transforms let you materialize any SELECT statement into a target table using a configurable write strategy. This page covers SELECT syntax, column documentation with `attributesDesc`, custom MERGE/INSERT via `parseSQL: false`, and incremental modelling with the `sl_start_date` / `sl_end_date` environment variables. All SQL is standard -- no Jinja templating is involved.

## SELECT statements
[SQL transforms](../transform/tutorial#the-select-statements) are SELECT statements materialized using a user defined strategy.
The result of the SELECT statement is then used to populate the target table by applying the [write strategy](../load/write-strategies) defined in the corresponding YAML configuration file.
The SELECT statement can be as simple or as complex as needed.

The only requirement is that the result of the SELECT statement must match the schema of the target table if it exists, otherwise starlake will infer the schema.



## Column description

Most of the time you won't need to document the columns in the SELECT statement as starlake will infer the schema from the result of the SELECT statement
and the column documentation from the source table column description.

You may need to document the calculated columns in the SELECT statement as they don't exist in the source tables.
You can do that by adding setting the `attributesDesc` key in the YAML configuration file.

```yaml title="metadata/transform/<domain>/<table>.sl.yml"

task:
  ...
  attributesDesc:
    name: <column_name>
    comment: <column_description>
  ...

```

## Custom write strategy

If none of the [predefined write strategies](../load/write-strategies) fits your need, you can write your own update/insert/merge statements directly in the SQL file
instead of letting starlake convert your select statements into update/insert/merge statements.

But in that case, you will need to set the `parseSQL` (`true` by default) to `false` in the YAML configuration file.

```yaml title="metadata/transform/<domain>/<table>.sl.yml"

task:
  ...
  parseSQL: false
  ...

```


## Incremental modelling

As data arrives periodically we have to build KPIs on that new data only. 
For example, the daily_sales KPI does not need the whole input dataset but just the data
for the day we are computing the KPI. The computed daily KPI are then appended 
to the existing dataset, greatly reducing the amount of data processed.

Starlake allows you to write your query as follows:


![alt_text](/img/blog/incremental-models/query.png "image_tooltip")


The `sl_start_date` and `sl_end_date` variables represent the start and end times for
which the query is being executed. These are reserved environment variables that are automatically 
passed to the transformation process by the orchestrator (e.g., Airflow, Dagster or Snowflake Tasks).

You might wonder what happens if the job fails to run for a specific day.
There's no need to worryâ€”Starlake handles this seamlessly by leveraging Airflow's
 **Catchup** mechanism. By configuration, Starlake requests the orchestrator 
 to catch up on any missed intervals. To enable this, simply add the `catchup` flag
 to your YAML DAG definition in Starlake, and as expected the orchestrator will run the missed intervals using 
 the `sl_start_date` and `sl_end_date` variables valued accordingly.

## Frequently Asked Questions

### Does the SELECT result have to match the target table schema?
If the target table already exists, the SELECT result must match its schema. Otherwise, Starlake automatically infers the schema from the result.

### How do you document calculated columns in a SQL transform?
Use the `attributesDesc` property in the transform YAML file to add a name and comment for each calculated column.

### Can you write custom MERGE or INSERT instead of a SELECT?
Yes. Set `parseSQL: false` in the YAML configuration. Starlake will then execute the SQL as-is without converting it into a write statement.

### What are the sl_start_date and sl_end_date variables?
They are reserved environment variables representing the current time interval. They are automatically passed by the orchestrator (Airflow, Dagster, or Snowflake Tasks) for incremental executions.

### What happens if an incremental job fails for a day?
Starlake integrates with Airflow's Catchup mechanism. Missed intervals are automatically replayed with the correct `sl_start_date` and `sl_end_date` values.

### Does Starlake support Jinja templating in SQL files?
No. Starlake SQL files use standard SQL. Environment variables are injected directly without a Jinja layer.

### How does schema inference work when the target table does not exist?
Starlake executes the SELECT and infers the schema (types, column names) from the result. Source column documentation is automatically propagated.

