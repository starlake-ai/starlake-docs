---
title: "Customizing DAG Generation"
description: "Complete guide to customizing Starlake DAG generation: YAML configuration, dagRef references, Jinja2 templates, pre-load strategies, and the IStarlakeJob Python factory interface for Airflow, Dagster, and Snowflake Tasks."
keywords: [starlake, dag customization, airflow dag, dagster, snowflake tasks, jinja2, orchestration, dagRef, pre-load strategy, IStarlakeJob]
# Display h2 to h5 headings
toc_min_heading_level: 2
toc_max_heading_level: 4
---

import Head from '@docusaurus/Head';

<Head>
  <script type="application/ld+json">
    {JSON.stringify({
      "@context": "https://schema.org",
      "@type": "FAQPage",
      "mainEntity": [
        {
          "@type": "Question",
          "name": "How does DAG generation work in Starlake?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "Starlake uses a framework composed of: the dag-generate command, YAML configuration files in metadata/dags, Jinja2 templates, and the starlake-orchestration framework that dynamically generates the tasks."
          }
        },
        {
          "@type": "Question",
          "name": "Where to define the dagRef reference for data loading?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "At the project level in application.sl.yml (application.dagRef.load property), at the domain level in _config.sl.yml (load.metadata.dagRef property), or at the table level in the table's .sl.yml file."
          }
        },
        {
          "@type": "Question",
          "name": "Where to define the dagRef reference for transformations?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "At the project level in application.sl.yml (application.dagRef.transform property) or at the transformation level in the .sl.yml file (task.dagRef property)."
          }
        },
        {
          "@type": "Question",
          "name": "What are the available pre-load strategies?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "Four strategies: NONE (no condition), IMPORTED (file present in the landing area), PENDING (file in the pending zone), ACK (acknowledgment file present)."
          }
        },
        {
          "@type": "Question",
          "name": "How to generate a DAG per domain or per table?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "The filename property in the YAML configuration controls granularity. Use {{domain}} for one DAG per domain, {{domain}}_{{table}} for one DAG per table."
          }
        },
        {
          "@type": "Question",
          "name": "How to pass Starlake environment variables to the DAG?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "Use the sl_env_var option in the YAML configuration. It accepts an encoded JSON string containing the environment variables."
          }
        },
        {
          "@type": "Question",
          "name": "What is the IStarlakeJob interface?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "It is the generic Python factory interface responsible for generating tasks for the stage, load, and transform commands. Each orchestrator has concrete classes that implement the sl_job method."
          }
        },
        {
          "@type": "Question",
          "name": "What options are available for the dag-generate command?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "--outputDir (output directory), --clean (remove old files), --domains (generate for schemas), --tasks (generate for tasks), --tags (filter by tags)."
          }
        }
      ]
    })}
  </script>
</Head>

# Customizing DAG generation

Starlake comes with out of the box DAG templates. These templates can be customized to fit your specific needs for any scheduler of your choice.

You just need to be comfortable with **Jinja2** templating language and **Python** programming language.

starlake is not an orchestration tool, but it can be used to generate your DAG based on templates and to run your transforms in the right
order on your tools of choice for scheduling and monitoring batch oriented workflows.<br/>
<br/>
Starlake DAG generation relies on:

- **starlake** command line tool
- **DAG** **configuration**(s) and their **references** within the loads and tasks
- **template**(s) that may be customized
- **starlake-orchestration** framework to dynamically generate the tasks that will be run
- **managing dependencies** between tasks to execute transforms in the **correct order**

## Prerequisites

Before using Starlake dag generation, ensure the following minimum versions are installed on your system:

- **starlake**: 1.0.1 or higher

Additional requirements depend on the orchestrator you are using. See the specific pages below.

## Orchestrator-specific guides

Choose your orchestrator for detailed customization instructions:

- [**Customize Airflow DAGs**](./210-airflow.mdx) -- Concrete factory classes, templates, data-aware scheduling, user-defined macros, and Terraform integration for Apache Airflow.
- [**Customize Dagster DAGs**](./220-dagster.mdx) -- Concrete factory classes, templates, and multi-asset sensors for Dagster.
- [**Customize Snowflake Task DAGs**](./230-snowflake-tasks.mdx) -- Task generation and configuration for Snowflake Tasks.

## Command

```bash
starlake dag-generate [options]
```

where options are:

| parameter               | cardinality | description                                                                                                                |
| ----------------------- | ----------- | -------------------------------------------------------------------------------------------------------------------------- |
| --outputDir `<value>` | optional    | Path for saving the resulting DAG file(s) (*$\{SL_ROOT\}/metadata/dags/generated* by default).                             |
| --clean                 | optional    | Should the existing DAG file(s) be removed first (*false* by default)                                             |
| --domains               | optional    | Wether to generate DAG file(s) to load schema(s) or not (*true* by default if *--tasks* option has not been specified) |
| --tasks                 | optional    | Whether to generate DAG file(s) for tasks or not  (*true* by default if *--domains* option has not been specified)     |
| --tags `<value>`      | optional    | Whether to generate DAG file(s) for the specified tags only (no tags by default)                                           |

## Configuration

All DAG configuration files are located in *$\{SL_ROOT\}/metadata/dags* directory. The root element is **dag**.

### References

We reference a DAG configuration by using the configuration file name without its extension

#### DAG configuration for loading data
:::note DAG configuration for loading data

The configuration files to use for *loading* data can be defined:
- at the **project** level, in the **application** file *$\{SL_ROOT\}/metadata/application.sl.yml* under the *application.dagRef.load* property.<br/>
In this case the same configuration file will be used as the default DAG configuration for all the tables in the project.

```yaml
application:
  dagRef:
    load: load_cloud_run_domain
#...
```

- at the **domain** level, in the **domain** configuration file *$\{SL_ROOT\}/metadata/load/\{domain\}/_config.sl.yml* under the *load.metadata.dagRef* property.<br/>
In this case the configuration file will be used as the default DAG configuration for all the tables in the domain.

```yaml
load:
  metadata:
    dagRef:load_bash_domain
#...
```

- at the **table** level, in the **table** configuration file *$\{SL_ROOT\}/metadata/load/\{domain\}/\{table\}.sl.yml* under the *table.metadata.dagRef* property.<br/>
In this case the configuration file will be used as the default DAG configuration for the table only.

```yaml
table:
  metadata:
    dagRef:load_bash_domain
#...
```

:::
#### DAG configuration for transforming data
:::note DAG configuration for transforming data

The configuration files to use for *transforming* data can be defined

- at the **project** level, in the **application** file *$\{SL_ROOT\}/metadata/application.sl.yml* under the *application.dagRef.transform* property.<br/>
In this case the same configuration file will be used as the default DAG configuration for all the transformations in the project.

```yaml
application:
  dagRef:
    transform: norm_cloud_run_domain
#...
```

- at the **transformation** level, in the **transformation** configuration file *$\{SL_ROOT\}/metadata/transform/\{domain\}/\{transformation\}.sl.yml* under the *task.dagRef* property.<br/>
In this case the configuration file will be used as the default DAG configuration for the transformation only.

```yaml
task:
  dagRef: agr_cloud_run_domain
#...
```
:::

### Properties

A DAG configuration defines four properties: [comment](#comment), [template](#template), [filename](#filename) and [options](#options).

```yaml
dag:
  comment: "dag for transforming tables for domain {\{domain\}} with cloud run" # will appear as a description of the dag
  template: "custom_scheduled_task_cloud_run.py.j2" # the dag template to use
  filename: "{\{domain\}}_norm_cloud_run.py" # the relative path to the outputDir specified as a parameter of the `dag-generate` command where the generated dag file will be copied
  options:
    sl_env_var: "{\"SL_ROOT\": \"${root_path}\", \"SL_DATASETS\": \"${root_path}/datasets\", \"SL_TIMEZONE\": \"Europe/Paris\"}"

 #...
```

#### Comment
:::note Comment

A short **description** to describe the generated DAG.

:::
#### Template
:::note Template

The **path** to the template that will generate the DAG(s), either:

- an **absolute** path
- a **relative** path name to the *$\{SL_ROOT\}metadata/dags/template* directory
- a **relative** path name to the *src/main/templates/dags* starlake resource directory

:::
#### Filename
:::note Filename

The filename defines the **relative path** to the DAG(s) that will be generated. The specified path is relative to the **outputDir** option that was specified on the command line (or its default value if not specified).

The value of this property may include **special variable**s that will have a direct **impact** on the **number of dags** that will be **generated**:

- **domain**: a single DAG for all tables within the domain affected by this configuration

```yaml
dag:
  filename: "{\{domain\}}_norm_cloud_run.py" # one DAG per domain
 #...
```

- **table** : as many dags as there are tables in the domain affected by this configuration

```yaml
dag:
  filename: "{\{domain\}}_{\{table\}}_norm_cloud_run.py" # one DAG per table
 #...
```

Otherwise, a single DAG will be generated for all tables affected by this configuration.

:::
#### Options
:::note Options

This property allows you to pass a certain number of options to the template in the form of a *dictionary*.

:::
Some of these **options** are **common** to all templates.

##### Starlake env vars

**sl_en_var** defines starlake environment variables passed as an encoded json string

```yaml
dag:
  options:
    sl_env_var: "{\"SL_ROOT\": \"${root_path}\", \"SL_DATASETS\": \"${root_path}/datasets\", \"SL_TIMEZONE\": \"Europe/Paris\"}"
 #...
```

##### Pre-load strategy

**pre_load_strategy** defines the strategy that can be used to conditionally load the tables of a domain within the DAG.

Four possible strategies:

###### NONE
:::note NONE

The load of the domain will not be conditionned and no pre-load tasks will be executed (the default strategy).

:::

###### IMPORTED
:::note IMPORTED

This strategy implies that at least one file is present in the **landing area** (*$\{SL_ROOT\}/incoming/\{domain\}* by default, if option **incoming_path** has not been specified). If there is one or more files to load, the method **sl_import** will be called to import the domain before loading it, otherwise the loading of the domain will be skipped.

```yaml
dag:
  options:
    pre_load_strategy: "imported"
 #...
```

:::

###### PENDING
:::note PENDING

This strategy implies that at least one file is present in the **pending** **datasets area** of the **domain** (*$\{SL_ROOT\}/datasets/pending/\{domain\}* by default if option **pending_path** has not been specified), otherwise the loading of the domain will be skipped.

```yaml
dag:
  options:
    pre_load_strategy: "pending"
 #...
```

:::

###### ACK
:::note ACK

This strategy implies that an **ack file** is present at the specified path (*$\{SL_ROOT\}/datasets/pending/\{domain\}/\{\{\{\{ds\}\}\}\}.ack* by default if option **global_ack_file_path** has not been specified), otherwise the loading of the domain will be skipped.

```yaml
dag:
  options:
    pre_load_strategy: "ack"
 #...
```

:::

##### Load dependencies

**run_dependencies_first** defines wether or not we want to **generate recursively** all the **dependencies** associated to each task for which the transformation DAG was generated (*False* by default).

```yaml
dag:
  options:
    run_dependencies_first: True
 #...
```

### Additional options

Depending on the **template** chosen, a specific **concrete** factory class extending `ai.starlake.job.IStarlakeJob` will be instantiated for which additional options may be required.


#### IStarlakeJob
`ai.starlake.job.IStarlakeJob` is the **generic factory interface** responsible for **generating** the **tasks** that will run the starlake's [stage](../../cli/stage), [load](../../category/load) and [transform](../../category/transform) commands:

- **sl_import** will generate the task that will run the starlake [stage](../../cli/stage) command.

```python
def sl_import(
    self,
    task_id: str,
    domain: str,
    **kwargs) -> BaseOperator:
    #...
```

| name    | type | description                                           |
| ------- | ---- | ----------------------------------------------------- |
| task_id | str  | the optional task id (`\{domain\}_import` by default) |
| domain  | str  | the required domain to import                         |

- **sl_load** will generate the task that will run the starlake [load](../../cli/load) command.

```python
def sl_load(
    self,
    task_id: str,
    domain: str,
    table: str,
    spark_config: StarlakeSparkConfig=None,
    **kwargs) -> BaseOperator:
    #...
```

| name         | type                | description                                                 |
| ------------ | ------------------- | ----------------------------------------------------------- |
| task_id      | str                 | the optional task id (`\{domain\}_\{table\}_load` by default) |
| domain       | str                 | the required domain of the table to load                    |
| table        | str                 | the required table to load                                  |
| spark_config | StarlakeSparkConfig | the optional `ai.starlake.job.StarlakeSparkConfig`        |

- **sl_transform** will generate the task that will run the starlake [transform](../../cli/transform) command.

```python
def sl_transform(
    self,
    task_id: str,
    transform_name: str,
    transform_options: str=None,
    spark_config: StarlakeSparkConfig=None, **kwargs) -> BaseOperator:
    #...
```

| name              | type                | description                                            |
| ----------------- | ------------------- | ------------------------------------------------------ |
| task_id           | str                 | the optional task id (`{transform_name}` by default) |
| transform_name    | str                 | the transform to run                                   |
| transform_options | str                 | the optional transform options                         |
| spark_config      | StarlakeSparkConfig | the optional `ai.starlake.job.StarlakeSparkConfig`   |

Ultimately, all of these methods will call the **sl_job** method that needs to be **implemented** in all **concrete** factory classes.

```python
def sl_job(
    self,
    task_id: str,
    arguments: list,
    spark_config: StarlakeSparkConfig=None,
    **kwargs) -> BaseOperator:
    #...
```

| name         | type                | description                                           |
| ------------ | ------------------- | ----------------------------------------------------- |
| task_id      | str                 | the required task id                                  |
| arguments    | list                | The required arguments of the starlake command to run |
| spark_config | StarlakeSparkConfig | the optional `ai.starlake.job.StarlakeSparkConfig`  |

For orchestrator-specific concrete factory classes, templates, dependencies handling, and advanced customization, see the dedicated pages:

- [Customize Airflow DAGs](./210-airflow.mdx)
- [Customize Dagster DAGs](./220-dagster.mdx)
- [Customize Snowflake Task DAGs](./230-snowflake-tasks.mdx)

## Frequently Asked Questions

### How does DAG generation work in Starlake?

Starlake uses a framework composed of: the `dag-generate` command, YAML configuration files in `metadata/dags`, Jinja2 templates, and the `starlake-orchestration` framework that dynamically generates the tasks.

### Where to define the dagRef reference for data loading?

At the **project** level in `application.sl.yml` (`application.dagRef.load` property), at the **domain** level in `_config.sl.yml` (`load.metadata.dagRef` property), or at the **table** level in the table's `.sl.yml` file.

### Where to define the dagRef reference for transformations?

At the **project** level in `application.sl.yml` (`application.dagRef.transform` property) or at the **transformation** level in the `.sl.yml` file (`task.dagRef` property).

### What are the available pre-load strategies?

Four strategies: `NONE` (no condition), `IMPORTED` (file present in the landing area), `PENDING` (file in the pending zone), `ACK` (acknowledgment file present).

### How to generate a DAG per domain or per table?

The `filename` property in the YAML configuration controls granularity. Use `{{domain}}` for one DAG per domain, `{{domain}}_{{table}}` for one DAG per table.

### How to pass Starlake environment variables to the DAG?

Use the `sl_env_var` option in the YAML configuration. It accepts an encoded JSON string containing the environment variables.

### What is the IStarlakeJob interface?

It is the generic Python factory interface responsible for generating tasks for the `stage`, `load`, and `transform` commands. Each orchestrator has concrete classes that implement the `sl_job` method.

### What options are available for the dag-generate command?

`--outputDir` (output directory), `--clean` (remove old files), `--domains` (generate for schemas), `--tasks` (generate for tasks), `--tags` (filter by tags).
